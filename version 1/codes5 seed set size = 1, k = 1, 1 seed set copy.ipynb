{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import confusion_matrix, pairwise_distances\n",
    "from sklearn.manifold import TSNE\n",
    "from itertools import chain\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_matrix(G,weighted=False,directed=False): # convert a graph object into a matrix\n",
    "    matrix = np.zeros((len(G.nodes()),len(G.nodes())))\n",
    "    for edge in G.edges(data=True):\n",
    "        if weighted:\n",
    "            matrix[edge[0],edge[1]] = edge[2]['weight']\n",
    "        else:\n",
    "            matrix[edge[0],edge[1]] = 1\n",
    "        if not directed:\n",
    "            matrix[edge[1],edge[0]] = matrix[edge[0],edge[1]] \n",
    "    return matrix\n",
    "    \n",
    "def matrix_to_graph(matrix): # convert an adjacency matrix into a graph object\n",
    "    G = nx.Graph().to_directed()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        G.add_node(i)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i,j] != 0:\n",
    "                G.add_edge(i, j, weight=matrix[i,j])\n",
    "    return G\n",
    "\n",
    "def ErdosRenyiGraph(n,p):\n",
    "    return nx.erdos_renyi_graph(n, p).to_directed(),False,False\n",
    "\n",
    "def dRegularGraph(n,d):\n",
    "    return nx.random_regular_graph(d, n).to_directed(),False,False\n",
    "\n",
    "def randomAdjacencyMatrix(n,p,directed=False,weighted=False): #ErdosRenyi is unweighted & undirected\n",
    "    # matrix is in dimension nxn (i.e. n is the number of nodes)\n",
    "    # p is probability for having non-zero entries\n",
    "    # values of non-zero entries are sampled in Unif[0,1) if graph is weighted\n",
    "    matrix = (np.random.rand(n, n) < p).astype(int)\n",
    "    np.fill_diagonal(matrix, 0)\n",
    "    if weighted:\n",
    "        matrix = matrix * np.random.rand(n, n)\n",
    "    if not directed:\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                matrix[j,i] = matrix[i,j]\n",
    "    return matrix,directed,weighted\n",
    "\n",
    "def geometricAdjacencyMatrix(n,d,r,metric='euclidean'):\n",
    "    '''\n",
    "    Valid values for metric are:\n",
    "    From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]. These metrics support sparse matrix inputs. [‘nan_euclidean’] but it does not yet support sparse matrices.\n",
    "    From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.\n",
    "    '''\n",
    "    points = np.random.uniform(0, 1, size=(n, d))\n",
    "    distance_matrix = (pairwise_distances(points, metric=metric) < r).astype(int)\n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    return distance_matrix,False,False\n",
    "\n",
    "def ChungLuGraph(n,max_degree):\n",
    "    degrees = np.random.choice(range(1,max_degree+1), n, replace=True)\n",
    "    G = nx.Graph()\n",
    "    for node, degree in enumerate(degrees):\n",
    "        G.add_node(node, degree=degree)\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if u < v and np.random.rand() < G.nodes[u]['degree'] * G.nodes[v]['degree'] / (2 * sum(degrees)):\n",
    "                    G.add_edge(u, v)\n",
    "    return G.to_directed(),False,False\n",
    "\n",
    "def gridGraph(nrows,ncols): # not random\n",
    "    G = nx.Graph()\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            G.add_node(i*ncols+j)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            if i < nrows - 1:\n",
    "                G.add_edge(i*ncols+j, (i+1)*ncols+j)\n",
    "            if j < ncols - 1:\n",
    "                G.add_edge(i*ncols+j+1, i*ncols+j)\n",
    "    return G.to_directed(),False,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closestSeed_networkx(G,u,S,return_path=False):\n",
    "    closest_point = None\n",
    "    smallest_distance = float('inf')\n",
    "    shortest_path = []\n",
    "    for s in S:\n",
    "        if nx.has_path(G, u, s):\n",
    "            path = nx.shortest_path(G, source=u, target=s, weight=\"weight\")\n",
    "            distance = sum(G[path[i]][path[i + 1]][\"weight\"] for i in range(len(path) - 1))\n",
    "            if distance < smallest_distance:\n",
    "                closest_point, smallest_distance, shortest_path = s, distance, path  \n",
    "    if not return_path:\n",
    "        return closest_point, smallest_distance\n",
    "    else:\n",
    "        return closest_point, smallest_distance, shortest_path\n",
    "\n",
    "#### Das Sarma & Bourgain ####\n",
    "\n",
    "def closestSeedFromStartPoint(graph, start, target_set, return_path=False):\n",
    "    # Works for weighted & unweighted, directed & undirected graphs \n",
    "    # Doesn't allow self-directed nodes\n",
    "\n",
    "    closest_point, smallest_weight, shortest_path = None, float('inf'), []\n",
    "    target_set = [x for x in target_set if x != start]\n",
    "    if len(target_set) > 0:\n",
    "        visited = set()  # To keep track of visited vertices (vertices in paths stored in queue)\n",
    "        queue = deque([(start, 0, [])])  # Initialize the queue with the starting vertex, its path, and the total weight\n",
    "        storage = deque() # To store nodes removed from queue\n",
    "\n",
    "        while queue:\n",
    "            vertex, weight, path = queue.popleft()  # Dequeue a vertex, its path, and the total weight\n",
    "            storage.append((vertex, weight))\n",
    "            if vertex not in target_set:\n",
    "                for neighbor, edge_weight in enumerate(graph[vertex]):\n",
    "                    if neighbor not in path and neighbor != vertex and edge_weight != 0:\n",
    "                        new_weight = weight + edge_weight\n",
    "                        if neighbor not in visited:\n",
    "                            queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            visited.add(neighbor)\n",
    "                        else: # Replace the path from 'start' to 'neighbor' in queue or storage with a shorter one if found\n",
    "                            weight_info = [(index,item[1]) for index,item in enumerate(list(queue)) if item[0] == neighbor]\n",
    "                            if len(weight_info) != 0:\n",
    "                                for index, w in weight_info:\n",
    "                                    if new_weight < w:\n",
    "                                        del queue[index]\n",
    "                                        queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            else:\n",
    "                                weights = [item[1] for item in storage if item[0] == neighbor]\n",
    "                                if len(weights) > 0 and new_weight < np.min(weights):\n",
    "                                    queue.append((neighbor, new_weight, path + [vertex]))\n",
    "\n",
    "            elif vertex in target_set:\n",
    "                if closest_point == None:\n",
    "                    closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                else:\n",
    "                    if weight < smallest_weight:\n",
    "                        closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                k = len(queue)\n",
    "                for i in range(k):\n",
    "                    if smallest_weight <= queue[k-1-i][1]:\n",
    "                        queue.remove(queue[k-1-i]) # Remove all paths in queue which are longer than the current shortest from 'start' to a seed\n",
    "    \n",
    "    if return_path:\n",
    "        return closest_point, smallest_weight, shortest_path\n",
    "    else:\n",
    "        return closest_point, smallest_weight\n",
    "\n",
    "def closestSeedToEndPoint(graph, target, start_set, return_path=False): \n",
    "    # Works for weighted & unweighted, directed & undirected graphs \n",
    "    # Doesn't allow self-directed nodes\n",
    "\n",
    "    closest_point, smallest_weight, shortest_path = None, float('inf'), []\n",
    "    start_set = [x for x in start_set if x != target]\n",
    "    if len(start_set) > 0:\n",
    "        visited = set()  # To keep track of visited vertices (vertices in paths stored in queue)\n",
    "        queue = deque()  # Initialize the queue with the starting vertex, its path, and the total weight\n",
    "        for i in range(len(start_set)):\n",
    "            queue.append((start_set[i], 0, []))\n",
    "        storage = deque()\n",
    "\n",
    "        while queue:\n",
    "            vertex, weight, path = queue.popleft()  # Dequeue a vertex, its path, and the total weight\n",
    "            storage.append((vertex, weight))\n",
    "            if vertex != target:\n",
    "                for neighbor, edge_weight in enumerate(graph[vertex]):\n",
    "                    if neighbor not in path and neighbor not in start_set and neighbor != vertex and edge_weight != 0:\n",
    "                        new_weight = weight + edge_weight\n",
    "                        if neighbor not in visited:\n",
    "                            queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            visited.add(neighbor)\n",
    "                        else: # Replace the path from a start node to 'neighbor' in queue with a shorter one if found\n",
    "                            weight_info = [(index,item[1]) for index,item in enumerate(list(queue)) if item[0] == neighbor]\n",
    "                            if len(weight_info) != 0:\n",
    "                                for index, w in weight_info:\n",
    "                                    if new_weight < w:\n",
    "                                        del queue[index]\n",
    "                                        queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            else:\n",
    "                                weights = [item[1] for item in storage if item[0] == neighbor]\n",
    "                                if len(weights) > 0 and new_weight < np.min(weights):\n",
    "                                    queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                                \n",
    "            else:\n",
    "                if closest_point == None:\n",
    "                    closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                else:\n",
    "                    if weight < smallest_weight:\n",
    "                        closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                k = len(queue)\n",
    "                for i in range(k):\n",
    "                    if smallest_weight <= queue[k-1-i][1]:\n",
    "                        queue.remove(queue[k-1-i]) # Remove all paths in queue which are longer than the current shortest from 'start' to a seed\n",
    "\n",
    "    if return_path:\n",
    "        return closest_point, smallest_weight, shortest_path\n",
    "    else:\n",
    "        return closest_point, smallest_weight\n",
    "\n",
    "def offlineSample(G,u,node_to_sets=True):\n",
    "    support = [n for n in range(G.shape[0]) if np.count_nonzero(G[n]) >= 2 and n != u] # u is removed from the support for sampling seed sets\n",
    "    if len(support) == 0:\n",
    "        return set()\n",
    "    r = math.floor(np.log(len(support)))\n",
    "    sample_sets = [np.random.choice(support,size=2**i,replace=False) for i in range(r+1)]\n",
    "    if node_to_sets:\n",
    "        closest_points = set([closestSeedFromStartPoint(G,u,S) for S in sample_sets])\n",
    "    else:\n",
    "        closest_points = set([closestSeedToEndPoint(G,u,S) for S in sample_sets])\n",
    "    if (None,float('inf')) in closest_points:\n",
    "        closest_points.remove((None,float('inf')))\n",
    "    return closest_points,set(np.concatenate(sample_sets))\n",
    "\n",
    "def offlineSketch(G,u,k,node_to_sets=True):\n",
    "    closest_points,sample_sets = offlineSample(G,u,node_to_sets)\n",
    "    for i in range(k):\n",
    "        closest_points_new,sample_sets_new = offlineSample(G,u,node_to_sets)\n",
    "        closest_points = closest_points.union(closest_points_new)\n",
    "        sample_sets = sample_sets.union(sample_sets_new)\n",
    "    return np.array(list(closest_points)),np.array(list(sample_sets))\n",
    "\n",
    "def onlineShortestPath_Sarma(G,u,v,k,directed=False): ## upper bound\n",
    "    if not directed:\n",
    "        sketch_u,_ = offlineSketch(G,u,k)\n",
    "        sketch_v,_ = offlineSketch(G,v,k)\n",
    "    else:\n",
    "        sketch_u,_ = offlineSketch(G,u,k)\n",
    "    if sketch_u.shape[0] != 0 and sketch_v.shape[0] != 0:\n",
    "        common_nodes = [w for w in sketch_u[:,0] if w in sketch_v[:,0]]\n",
    "        while None in common_nodes:\n",
    "            common_nodes.remove(None)\n",
    "        min_dist = float('inf')\n",
    "        for w in common_nodes:\n",
    "            dist = sketch_u[sketch_u[:, 0] == w][0,1] + sketch_v[sketch_v[:, 0] == w][0,1]\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        return min_dist\n",
    "    else:\n",
    "        return float('inf')\n",
    "\n",
    "def onlineShortestPath_Bourgain(G,u,v,directed=False): ## lower bound\n",
    "    support = [n for n in range(G.shape[0]) if np.count_nonzero(G[n]) >= 2 and n != u] # u is removed from the support for sampling seed sets\n",
    "    r = math.floor(np.log(len(support)))\n",
    "    sample_sets = [np.random.choice(support,size=2**i,replace=False) for i in range(r+1)]\n",
    "    if directed:\n",
    "        d_u_S = [closestSeedFromStartPoint(G,u,S)[1] for S in sample_sets]\n",
    "        d_v_S = [closestSeedFromStartPoint(G,v,S)[1] for S in sample_sets]\n",
    "        d_S_u = [closestSeedToEndPoint(G,u,S)[1] for S in sample_sets]\n",
    "        d_S_v = [closestSeedToEndPoint(G,v,S)[1] for S in sample_sets]\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_u_S,d_v_S))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_u_S = np.array([value for index, value in enumerate(d_u_S) if index not in to_remove])\n",
    "        d_v_S = np.array([value for index, value in enumerate(d_v_S) if index not in to_remove])\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_S_u,d_S_v))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_S_u = np.array([value for index, value in enumerate(d_S_u) if index not in to_remove])\n",
    "        d_S_v = np.array([value for index, value in enumerate(d_S_v) if index not in to_remove])\n",
    "        return max([0,np.max(d_S_v-d_S_u),np.max(d_u_S-d_v_S)])\n",
    "    else:\n",
    "        d_u_S = [closestSeedFromStartPoint(G,u,S)[1] for S in sample_sets]\n",
    "        d_v_S = [closestSeedFromStartPoint(G,v,S)[1] for S in sample_sets]\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_u_S,d_v_S))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_u_S = np.array([value for index, value in enumerate(d_u_S) if index not in to_remove])\n",
    "        d_v_S = np.array([value for index, value in enumerate(d_v_S) if index not in to_remove])\n",
    "        return np.max(np.abs(d_u_S-d_v_S))\n",
    "\n",
    "def shortestDistance_allNodes_Sarma(G,u,k,directed=False):\n",
    "    distances = np.zeros(G.shape[0])\n",
    "    for v in range(G.shape[0]):\n",
    "        if u != v:\n",
    "            distances[v] = onlineShortestPath_Sarma(G,u,v,k,directed)\n",
    "    return distances\n",
    "\n",
    "def shortestDistance_allNodes_Bourgain(G,u,directed=False):\n",
    "    distances = np.zeros(G.shape[0])\n",
    "    for v in range(G.shape[0]):\n",
    "        if u != v:\n",
    "            distances[v] = onlineShortestPath_Bourgain(G,u,v,directed)\n",
    "    return distances\n",
    "\n",
    "def shortestDistance_allNodes_networkx(G,u):\n",
    "    if isinstance(G, np.ndarray):\n",
    "        G = matrix_to_graph(G)\n",
    "    n_nodes = len(G.nodes())\n",
    "    distances = np.zeros(n_nodes)\n",
    "    for v in range(n_nodes):\n",
    "        if u != v:\n",
    "            if nx.has_path(G, u, v):\n",
    "                distances[v] = nx.shortest_path_length(G, u, v)\n",
    "            else:\n",
    "                distances[v] = float('inf')       \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels1, hidden_channels2, hidden_channels3, hidden_channels4, hidden_channels5, out_channels, sigmoid = False, reLU = False):\n",
    "        super(MLP, self).__init__()\n",
    "        #torch.manual_seed(12345)\n",
    "        self.name = 'mlp'\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.first_hidden_channels = hidden_channels1\n",
    "        self.n_hidden_layers = 5\n",
    "        self.lin1 = Linear(in_channels, hidden_channels1)\n",
    "        self.lin2 = Linear(hidden_channels1, hidden_channels2)\n",
    "        self.lin3 = Linear(hidden_channels2, hidden_channels3)\n",
    "        self.lin4 = Linear(hidden_channels3, hidden_channels4)\n",
    "        self.lin5 = Linear(hidden_channels4, hidden_channels5)\n",
    "        self.lin6 = Linear(hidden_channels5, out_channels)\n",
    "        if sigmoid:\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        else:\n",
    "            self.sigmoid = None\n",
    "        if reLU:\n",
    "            self.reLU = torch.nn.ReLU()\n",
    "        else:\n",
    "            self.reLU = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin4(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin5(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin6(x)\n",
    "        if self.sigmoid != None:\n",
    "            x = self.sigmoid(x)\n",
    "        if self.reLU != None:\n",
    "            x = self.reLU(x)\n",
    "        return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels1, hidden_channels2, hidden_channels3, hidden_channels4, hidden_channels5, out_channels, sigmoid = False, reLU = False):\n",
    "        super(GCN, self).__init__()\n",
    "        #torch.manual_seed(1234567)\n",
    "        self.name = 'gcn'\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.first_hidden_channels = hidden_channels1\n",
    "        self.n_hidden_layers = 5\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels1)\n",
    "        self.conv2 = GCNConv(hidden_channels1, hidden_channels2)\n",
    "        self.conv3 = GCNConv(hidden_channels2, hidden_channels3)\n",
    "        self.conv4 = GCNConv(hidden_channels3, hidden_channels4)\n",
    "        self.conv5 = GCNConv(hidden_channels4, hidden_channels5)\n",
    "        self.conv6 = GCNConv(hidden_channels5, out_channels)\n",
    "        if sigmoid:\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        else:\n",
    "            self.sigmoid = None\n",
    "        if reLU:\n",
    "            self.reLU = torch.nn.ReLU()\n",
    "        else:\n",
    "            self.reLU = None\n",
    "\n",
    "    def forward(self, x, edge_index, weights=None):\n",
    "        if weights == None:\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv3(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv4(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv5(x, edge_index)\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv2(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv3(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv4(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv5(x, edge_index, edge_weight=weights)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv6(x, edge_index)\n",
    "        if self.sigmoid != None:\n",
    "            x = self.sigmoid(x)\n",
    "        if self.reLU != None:\n",
    "            x = self.reLU(x)\n",
    "        return x\n",
    "\n",
    "def build(in_channels,out_channels,model,criterion_type,optimizer_type,scheduler_type = None):\n",
    "    \n",
    "    if isinstance(model, str):\n",
    "        hidden_channels1 = 256\n",
    "        hidden_channels2 = 128\n",
    "        hidden_channels3 = 64\n",
    "        hidden_channels4 = 32\n",
    "        hidden_channels5 = 8\n",
    "        if model == 'mlp':\n",
    "            if criterion_type in ['ce','bce','multimargin']:\n",
    "                model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,sigmoid = True)\n",
    "            elif criterion_type in ['mse','l2','l1']: \n",
    "                model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,reLU = True)\n",
    "            else:\n",
    "                model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels)\n",
    "        elif model == 'gcn':\n",
    "            if criterion_type in ['ce','bce','multimargin']:\n",
    "                model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,sigmoid = True)\n",
    "            elif criterion_type in ['mse','l2','l1']: \n",
    "                model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,reLU = True)\n",
    "            else:\n",
    "                model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Model type not yet defined.\"\n",
    "            )\n",
    "    \n",
    "    if criterion_type == 'ce':\n",
    "        criterion = [torch.nn.CrossEntropyLoss()]\n",
    "    elif criterion_type == 'bce':\n",
    "        criterion = [torch.nn.BCELoss()]\n",
    "    elif criterion_type == 'bcelogits':\n",
    "        criterion = [torch.nn.BCEWithLogitsLoss()]\n",
    "    elif criterion_type in ['mse','l2']:\n",
    "        criterion = [torch.nn.MSELoss()]\n",
    "    elif criterion_type == 'l1':\n",
    "        criterion = [torch.nn.L1Loss()]\n",
    "    elif criterion_type == 'multimargin': # cuda crashed (similar to focal loss)\n",
    "        criterion = [torch.nn.MultiMarginLoss()]\n",
    "    elif criterion_type == 'mse-mse':\n",
    "        criterion1 = torch.nn.MSELoss()\n",
    "        criterion2 = torch.nn.MSELoss()\n",
    "        criterion = [criterion1,criterion2]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Criterion type not yet defined.\"\n",
    "        )\n",
    "    \n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, weight_decay=0.0001, alpha=0.99, eps=1e-8, momentum=0.9)\n",
    "    elif optimizer_type == 'adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=0.0001, lr_decay=0.0001)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Optimizer type not yet defined.\"\n",
    "        )\n",
    "    \n",
    "    if scheduler_type == None:\n",
    "        scheduler = None\n",
    "    else:\n",
    "        if scheduler_type == 'step':\n",
    "            scheduler = [torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)]\n",
    "        elif scheduler_type == 'reduce_on_plateau':\n",
    "            scheduler = [torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=False)]\n",
    "        elif scheduler_type == 'exponential':\n",
    "            scheduler = [torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)]\n",
    "        elif scheduler_type == 'cosine':\n",
    "            scheduler = [torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)]\n",
    "        elif scheduler_type == 'cyclic':\n",
    "            if optimizer_type in ['sgd','rmsprop']:\n",
    "                scheduler = [torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, cycle_momentum=True)]\n",
    "            else:\n",
    "                scheduler = [torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, cycle_momentum=False)]\n",
    "        elif scheduler_type == 'cyclic-cosine':\n",
    "            cycle_epochs = 5\n",
    "            if optimizer_type in ['sgd','rmsprop']:\n",
    "                scheduler_cyclic = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=cycle_epochs, cycle_momentum=True)\n",
    "            else:\n",
    "                scheduler_cyclic = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=cycle_epochs, cycle_momentum=False)\n",
    "            scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cycle_epochs * 2)\n",
    "            scheduler = [scheduler_cyclic,scheduler_cosine]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Scheduler type not yet defined.\"\n",
    "            )\n",
    "    \n",
    "    return model,criterion,optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gpu_bool,model,criterion,criterion_type,samples_x,samples_y,samples_edge_index = None,samples_weights = None):\n",
    "\n",
    "    t_loss = 0\n",
    "    total_samples = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if samples_edge_index == None:\n",
    "            for x,y in list(zip(samples_x,samples_y)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                out = model(x)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        elif samples_weights == None:\n",
    "            for x,y,edge_index in list(zip(samples_x,samples_y,samples_edge_index)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        else:\n",
    "            for x,y,edge_index,weights in list(zip(samples_x,samples_y,samples_edge_index,samples_weights)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                    weights = weights.to('cuda:1')\n",
    "                out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "\n",
    "    t_loss = t_loss/total_samples\n",
    "    t_loss = t_loss.cpu()\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        return t_loss, None, None, None\n",
    "    else:\n",
    "        y_true = np.array([y.cpu() for y in y_true])\n",
    "        y_pred = np.array([y.cpu() for y in y_pred])\n",
    "        #print(np.array(y_true).shape)\n",
    "        #print(np.array(y_pred).shape)\n",
    "        t_accuracy = sum(y_true == y_pred)/len(y_true)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        #print(cm.ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        t_sensitivity = tp / (tp + fn)\n",
    "        t_specificity = tn / (tn + fp)\n",
    "        return t_loss, t_accuracy, t_sensitivity, t_specificity\n",
    "\n",
    "def train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val,edge_index_train=None,edge_index_val=None,weights_train=None,weights_val=None):\n",
    "    model.train()\n",
    "    if edge_index_train == None:\n",
    "        for x,y in list(zip(x_train,y_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "            out = model(x)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    elif weights_train == None:\n",
    "        for x,y,edge_index in list(zip(x_train,y_train,edge_index_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "            out = model(x,edge_index)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    else:\n",
    "        for x,y,edge_index,weights in list(zip(x_train,y_train,edge_index_train,weights_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "                weights = weights.to('cuda:1')\n",
    "            out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    train_loss, train_accuracy, train_sensitivity, train_specificity = test(gpu_bool,model,criterion,criterion_type,x_train,y_train,edge_index_train,weights_train)\n",
    "    v_loss, v_accuracy, v_sensitivity, v_specificity = test(gpu_bool,model,criterion,criterion_type,x_val,y_val,edge_index_val,weights_val)\n",
    "\n",
    "    if scheduler_type in ['step','exponential','cyclic','cosine']:\n",
    "        scheduler[0].step()\n",
    "    elif scheduler_type == 'reduce_on_plateau': \n",
    "        scheduler[0].step(v_loss)\n",
    "    elif scheduler_type == 'cyclic-cosine':\n",
    "        scheduler[0].step()\n",
    "        scheduler[1].step()\n",
    "    return train_loss,train_accuracy,train_sensitivity,train_specificity,v_loss,v_accuracy,v_sensitivity,v_specificity\n",
    "\n",
    "def visualizeTNSE(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().numpy())\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"\")\n",
    "    plt.show()\n",
    "\n",
    "def run(samples,model,criterion_type,optimizer_type,scheduler_type,num_epochs=100,early_stopping_patience=None,visualize = False,save_model = False):\n",
    "\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "\n",
    "    x_train = samples[0]\n",
    "    x_val = samples[1]\n",
    "    x_test = samples[2]\n",
    "    y_train = samples[3]\n",
    "    y_val = samples[4]\n",
    "    y_test = samples[5]\n",
    "    edge_index_train = samples[6]\n",
    "    edge_index_val = samples[7]\n",
    "    edge_index_test = samples[8]\n",
    "    weights_train = samples[9]\n",
    "    weights_val = samples[10]\n",
    "    weights_test = samples[11]\n",
    "\n",
    "    if criterion_type in ['ce','bce','multimargin']:\n",
    "        out_channels = 2\n",
    "    elif criterion_type == 'mse-mse':\n",
    "        out_channels = y_train[0].shape[1]\n",
    "    else:\n",
    "        out_channels = 1\n",
    "\n",
    "    if isinstance(model, str):\n",
    "        model_type = model\n",
    "        model,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model_type,criterion_type,optimizer_type,scheduler_type)\n",
    "    else:\n",
    "        model_type = model.name\n",
    "        _,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model, criterion_type,optimizer_type,scheduler_type)\n",
    "    print(model)\n",
    "\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "        \n",
    "    if visualize:\n",
    "        model.eval()\n",
    "        if model_type == 'mlp':\n",
    "            out = model(x_test[0])\n",
    "        else:\n",
    "            out = model(x_test[0],edge_index_test[0])\n",
    "        visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_sen = []\n",
    "    train_spec = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_sen = []\n",
    "    val_spec = []\n",
    "\n",
    "    if early_stopping_patience != None:\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if model_type == 'mlp':\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val)\n",
    "        else:\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,optimizer_type,x_train,x_val,y_train,y_val,edge_index_train,edge_index_val,weights_train,weights_val)\n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(t_acc)\n",
    "        train_sen.append(t_sen)\n",
    "        train_spec.append(t_spec)\n",
    "        val_loss.append(v_loss)\n",
    "        val_acc.append(v_acc)\n",
    "        val_sen.append(v_sen)\n",
    "        val_spec.append(v_spec)\n",
    "        #print(t_loss)\n",
    "        #print(t_acc_or_error)\n",
    "        if epoch % 10 == 0:\n",
    "            if criterion_type in ['mse','l2','mse-mse']:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MSE): {t_loss:.4f}, Validation Loss (MSE): {v_loss:.4f}')\n",
    "            elif criterion_type == 'l1':\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MAE): {t_loss:.4f}, Validation Loss (MAE): {v_loss:.4f}')\n",
    "            else:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss: {t_loss:.4f}, Training Accuracy: {t_acc:.4f}, Training Sensitivity: {t_sen:.4f}, Training Specificity: {t_spec:.4f}, Validation Loss: {v_loss:.4f}, Validation Accuracy: {v_acc:.4f}, Validation Sensitivity: {v_sen:.4f}, Validation Specificity: {v_spec:.4f}')\n",
    "\n",
    "        if early_stopping_patience != None:\n",
    "            if v_loss < best_val_loss:\n",
    "                best_val_loss = v_loss\n",
    "                best_epoch = epoch\n",
    "                no_improvement_count = 0\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            if no_improvement_count >= early_stopping_patience:\n",
    "                model.load_state_dict(torch.load('best_model.pth'))\n",
    "                if gpu_bool:\n",
    "                    model = model.to('cuda:1')\n",
    "                break\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test)\n",
    "    else:\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test, edge_index_test, weights_test)\n",
    "    \n",
    "    if early_stopping_patience == None:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "    else:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "\n",
    "    x = range(1, len(train_loss)+1)\n",
    "    plt.plot(x, train_loss, color = '#1f77b4', label = 'training loss', alpha = 0.6)\n",
    "    plt.plot(x, val_loss, color = '#ff7f0e', label = 'validation loss', alpha = 0.6)\n",
    "    if criterion_type in ['ce','bce','bcelogits','multimargin']:\n",
    "        plt.plot(x, train_acc, color = '#2ca02c', label = 'training accuracy', alpha = 0.6)\n",
    "        plt.plot(x, val_acc, color = '#d62728', label = 'validation accuracy', alpha = 0.6)\n",
    "        plt.plot(x, train_sen, color = '#9467bd', label = 'training sensitivity', alpha = 0.6)\n",
    "        plt.plot(x, val_sen, color = '#8c564b', label = 'validation sensitivity', alpha = 0.6)\n",
    "        plt.plot(x, train_spec, color = '#e377c2', label = 'training specificity', alpha = 0.6)\n",
    "        plt.plot(x, val_spec, color = '#7f7f7f', label = 'validation specificity', alpha = 0.6)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"training results\")\n",
    "    plt.legend(loc = 'right',bbox_to_anchor=(1.45, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "    if visualize:\n",
    "        model.eval()\n",
    "        if model_type == 'mlp':\n",
    "            out = model(x_test[0])\n",
    "        else:\n",
    "            out = model(x_test[0],edge_index_test[0])\n",
    "        visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), 'trained_model_'+str(model.in_channels)+'_'+str(model.first_hidden_channels)+'_'+str(model.out_channels)+'_'+str(model.n_hidden_layers)+'.pth')\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        if early_stopping_patience == None:\n",
    "            return [model, train_loss[-1], val_loss[-1], test_loss]\n",
    "        else:\n",
    "            return [model, train_loss[best_epoch-1], val_loss[best_epoch-1], test_loss]\n",
    "    else:\n",
    "        if early_stopping_patience == None:\n",
    "            return [model, train_acc[-1], train_sen[-1], train_spec[-1], val_acc[-1], val_sen[-1], val_spec[-1], test_acc, test_sen, test_spec]\n",
    "        else:\n",
    "            return [model, train_acc[best_epoch-1], train_sen[best_epoch-1], train_spec[best_epoch-1], val_acc[best_epoch-1], val_sen[best_epoch-1], val_spec[best_epoch-1], test_acc, test_sen, test_spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(gpu_bool,model,criterion_type,samples_x,samples_edge_index=None,samples_weights=None):\n",
    "    y_pred = []\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if model.name == 'mlp':\n",
    "            for i in range(len(samples_x)):\n",
    "                if gpu_bool:\n",
    "                    x = samples_x[i].to('cuda:1')\n",
    "                else:\n",
    "                    x = samples_x[i]\n",
    "                out = model(x)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.append(pred.cpu())\n",
    "        elif samples_weights == None:\n",
    "            for i in range(len(samples_x)):\n",
    "                if gpu_bool:\n",
    "                    x = samples_x[i].to('cuda:1')\n",
    "                    edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                else:\n",
    "                    x = samples_x[i]\n",
    "                    edge_index = samples_edge_index[i]\n",
    "                out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.append(pred.cpu())\n",
    "        else:\n",
    "            for i in range(len(samples_x)):\n",
    "                if gpu_bool:\n",
    "                    x = samples_x[i].to('cuda:1')\n",
    "                    edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                    weights = samples_weights[i].to('cuda:1')\n",
    "                else:\n",
    "                    x = samples_x[i]\n",
    "                    edge_index = samples_edge_index[i]\n",
    "                    weights = samples_weights[i]\n",
    "                out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.append(pred.cpu())\n",
    "    model = model.to('cpu')\n",
    "    return np.array(y_pred)\n",
    "\n",
    "def predict_allBatches(model,criterion_type,samples):\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "    y_pred_train = predict(gpu_bool, model, criterion_type, samples[0], samples[6], samples[9])\n",
    "    y_pred_val = predict(gpu_bool, model, criterion_type, samples[1], samples[7], samples[10])\n",
    "    y_pred_test = predict(gpu_bool, model, criterion_type, samples[2], samples[8], samples[11])\n",
    "    return y_pred_train,y_pred_val,y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inner(title,y_actual,y_pred,true_distances,x_samples = None,classification=True): ## target is nx1\n",
    "\n",
    "    if classification:\n",
    "\n",
    "        graph_size = true_distances.shape[1]\n",
    "\n",
    "        ## Calculate the proportion of points selected from the sample.\n",
    "        ratio1 = np.mean(y_pred,axis=1)\n",
    "\n",
    "        ## Check if any disconnected node is in actual sketch.\n",
    "        true_distances[true_distances == np.inf] = -1 ## convert inf into a large number for the sake of calculations\n",
    "        M = np.max(true_distances) * 10\n",
    "        true_distances[true_distances == -1] = M\n",
    "        y_actual = torch.stack(y_actual).detach().numpy()\n",
    "        dump_actual = y_actual*true_distances\n",
    "        n_dump_actual_equal_M = np.sum((dump_actual == M).astype(int),axis=1)\n",
    "        ratio2 = n_dump_actual_equal_M/np.sum(y_actual,axis=1) ## nan means empty sketch\n",
    "        #ratio2[np.isnan(ratio2)] = -1\n",
    "\n",
    "        ## Check if any disconnected node is chosen.\n",
    "        dump = y_pred*true_distances\n",
    "        n_true_distances_equal_M = np.sum((true_distances == M).astype(int),axis=1)\n",
    "        n_dump_equal_M = np.sum((dump == M).astype(int),axis=1)\n",
    "        n_selected = np.sum(y_pred,axis=1)\n",
    "        n_selected[n_selected == 0] = 0.0001\n",
    "        ratio3 = n_dump_equal_M/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio3[np.isnan(ratio3)] = -1\n",
    "\n",
    "        if x_samples != None:\n",
    "\n",
    "            ## Seed node and neighbors -> 1. Others -> 0\n",
    "            seed_or_neighbors = np.zeros_like(true_distances)\n",
    "            _,seeds = np.where(true_distances == 0) ## number of seeds = number of samples because we selected one seed from each sample\n",
    "            seed_disconnected = np.zeros(seed_or_neighbors.shape[0])\n",
    "            for i in range(len(seeds)):\n",
    "                seed = seeds[i]\n",
    "                seed_or_neighbors[i] = x_samples[i].detach().numpy()[seed] ## x_samples[i] has to be adjacency matrix\n",
    "                seed_disconnected[i] = sum((seed_or_neighbors[i]>0).astype(int))>0\n",
    "                seed_or_neighbors[i,seed] = 1\n",
    "            seed_or_neighbors_selected = seed_or_neighbors*y_pred\n",
    "\n",
    "            ## Is seed node not disconnected from the remaining?\n",
    "            seed_disconnected = seed_disconnected.astype(int)\n",
    "\n",
    "            ## Have seed node and its neighbors been chosen?\n",
    "            ratio4 = np.sum(seed_or_neighbors_selected,axis=1)/np.sum(seed_or_neighbors,axis=1)\n",
    "\n",
    "            ## How many of the chosen nodes are seed node and its neighbors?\n",
    "            ratio5 = np.sum(seed_or_neighbors_selected,axis=1)/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "            #ratio5[np.isnan(ratio5)] = -1\n",
    "\n",
    "        ## Check if the chosen nodes are closer than random selection.\n",
    "        sum_distance_selected = np.sum(dump,axis=1)\n",
    "        term0 = np.mean(true_distances,axis=1)\n",
    "        term0[term0 == 0] = 0.0001\n",
    "        ratio6 = sum_distance_selected/n_selected/term0 ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio6[np.isnan(ratio6)] = -1\n",
    "\n",
    "        ## Same as ratio6 but excludes disconnected nodes with inf distance to the seed node.\n",
    "        n_selected[n_selected == 0.0001] = 0\n",
    "        term1 = n_selected-n_dump_equal_M\n",
    "        term1[term1 == 0] = 0.0001\n",
    "        term2 = np.sum(true_distances,axis=1)-n_true_distances_equal_M*M\n",
    "        term2[term2 == 0] = 0.0001\n",
    "        ratio7 = (sum_distance_selected-n_dump_equal_M*M)/term1/term2*(graph_size-n_true_distances_equal_M)\n",
    "        #ratio7[np.isnan(ratio7)] = -1\n",
    "        ## nan means no nodes have been selected (ratio1 = 0) or selected nodes are all disconnected from seed node (inf distance)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(ratio1)), ratio1, color = '#1f77b4', label = 'ratio 1: size of selected community to size of sample ratio',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio6, color = '#8c564b', label = 'ratio 2: mean distance of chosen nodes to seed node divided by mean distance of all nodes to seed node',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio7, color = '#e377c2', label = 'ratio 3: same as ratio 6 but excludes disconnected nodes',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio2, color = '#ff7f0e', label = 'ratio 4: proportion of disconnected nodes in actual sketch',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio3, color = '#2ca02c', label = 'ratio 5: proportion of disconnected nodes in the chosen community',alpha = 0.6)\n",
    "        if x_samples != None:\n",
    "            plt.plot(range(len(ratio1)), ratio4, color = '#d62728', label = 'ratio 6: proportion of seed node and its neighbors that are selected',alpha = 0.6)\n",
    "            plt.plot(range(len(ratio1)), ratio5, color = '#9467bd', label = 'ratio 7: proportion of selected nodes that are seed node or its neighbors',alpha = 0.6)\n",
    "            plt.scatter(range(len(ratio1)), seed_disconnected, color = 'k', marker='x', label = 'is seed node not disconnected from the remaining nodes in graph? 1 = True, 0 = False',alpha = 0.6)\n",
    "        plt.xlabel(\"sample index\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(title)\n",
    "        plt.legend(loc = 'lower center',bbox_to_anchor=(0.48, -0.6))\n",
    "        plt.show()\n",
    "\n",
    "        true_distances[true_distances == M] = np.inf\n",
    "        if x_samples == None:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3)]\n",
    "        else:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3),np.mean(ratio4),np.mean(ratio5)]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        ## Calculate prediction to true distance ratio.\n",
    "        true_distances[true_distances == np.inf] = -1 ## convert inf into a large number for the sake of calculations\n",
    "        y_pred[y_pred == np.inf] = -1\n",
    "        M = max(np.max(true_distances),np.max(y_pred))*10\n",
    "        true_distances[true_distances == -1] = M\n",
    "        true_distances[true_distances == 0] = 0.0001 ## to avoid ratio = x/0\n",
    "        y_pred[y_pred == -1] = M\n",
    "        ratio = (y_pred/true_distances).flatten()\n",
    "        \n",
    "        if len(np.unique(ratio)) == 1:\n",
    "            print('Prediction to true distance ratio is '+str(ratio[0])+' for all samples in '+title+'.')\n",
    "        else:\n",
    "            plt.hist(ratio,density=False)\n",
    "            plt.xlabel(\"prediction to true distance ratio\")\n",
    "            plt.ylabel(\"frequency\")\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "            print('mean ratio = ',np.mean(ratio))\n",
    "\n",
    "        true_distances[true_distances == M] = np.inf\n",
    "        true_distances[true_distances == 0.0001] = 0\n",
    "        y_pred[y_pred == M] = np.inf\n",
    "\n",
    "        return [np.mean(ratio)]\n",
    "    \n",
    "def evaluate(model,criterion_type,samples,adjacency_matrix_available = False):\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        classification = False\n",
    "    else:\n",
    "        classification = True\n",
    "    y_pred_train,y_pred_val,y_pred_test = predict_allBatches(model,criterion_type,samples)\n",
    "    stats = []\n",
    "    if adjacency_matrix_available:\n",
    "        stats.append(evaluate_inner('training data',samples[3],y_pred_train,samples[12],samples[0],classification))\n",
    "        stats.append(evaluate_inner('validation data',samples[4],y_pred_val,samples[13],samples[1],classification))\n",
    "        stats.append(evaluate_inner('test data',samples[5],y_pred_test,samples[14],samples[2],classification))\n",
    "    else:\n",
    "        stats.append(evaluate_inner('training data',samples[3],y_pred_train,samples[12],classification=classification))\n",
    "        stats.append(evaluate_inner('validation data',samples[4],y_pred_val,samples[13],classification=classification))\n",
    "        stats.append(evaluate_inner('test data',samples[5],y_pred_test,samples[14],classification=classification))\n",
    "    return np.array(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples_shortestPath_inner(n_samples,max_n_samples_per_graph,function,*args,**kwargs): ## max_n_samples_per_graph <= n\n",
    "    samples_x = []\n",
    "    samples_y = []\n",
    "    samples_edge_index = []\n",
    "    samples_weights = None\n",
    "    object,directed,weighted = function(*args,**kwargs)\n",
    "    if weighted:\n",
    "        samples_weights = []\n",
    "    if isinstance(object, np.ndarray):\n",
    "        #matrix_type = object\n",
    "        object = matrix_to_graph(object)\n",
    "    #else:\n",
    "        #matrix_type = graph_to_matrix(object)\n",
    "    k = len(object.nodes())\n",
    "    i = 0\n",
    "    while i<n_samples:\n",
    "        components = list(nx.strongly_connected_components(object))\n",
    "        connected_components = [c for c in components if len(c) > 1] ## only consider components with >= 2 nodes\n",
    "        components_len = np.array([len(c) for c in connected_components])\n",
    "        components_weights = components_len/np.sum(components_len)\n",
    "        if len(connected_components) > 0:\n",
    "            j = 0\n",
    "            selected_pairs = []\n",
    "            while i<n_samples and j<max_n_samples_per_graph: \n",
    "                selected_component = np.random.choice(connected_components, p=components_weights) ## select components with probability ~ its size\n",
    "                nodes = list(np.random.choice(list(selected_component), size=2, replace=False))\n",
    "                if nodes not in selected_pairs and [nodes[1],nodes[0]] not in selected_pairs: ## avoid the same nodes to be selected again\n",
    "                    path = nx.shortest_path(object, nodes[0], nodes[1], weight=\"weight\")\n",
    "                    #print('Ground truth shortest path:',path)\n",
    "                    #samples_x.append(torch.tensor(matrix_type.astype(np.float32), requires_grad=True))\n",
    "                    samples_x.append(torch.tensor(np.array([int(node in nodes) for node in range(k)]).reshape((k,1)).astype(np.float32), requires_grad=True))\n",
    "                    samples_y.append(torch.tensor([int(node in path) for node in range(k)]).to(torch.float32))\n",
    "                    samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                    if weighted:\n",
    "                        print(nx.get_edge_attributes(object,'weight').values())\n",
    "                        samples_weights.append(torch.tensor(list(nx.get_edge_attributes(object,'weight').values())).to(torch.int64))\n",
    "                        print(samples_weights[-1])\n",
    "                    i += 1\n",
    "                j += 1\n",
    "        else:\n",
    "            object,directed,weighted = function(*args,**kwargs)\n",
    "            if isinstance(object, np.ndarray):\n",
    "                #matrix_type = object\n",
    "                object = matrix_to_graph(object)\n",
    "            #else:\n",
    "                #matrix_type = graph_to_matrix(object)        \n",
    "    return samples_x, samples_y, samples_edge_index,samples_weights\n",
    "\n",
    "def generateSamples_shortestPath(n_train,n_val,n_test,max_n_samples_per_graph,function,*args,**kwargs):\n",
    "    x_train, y_train, edge_index_train,weights_train = generateSamples_shortestPath_inner(n_train,max_n_samples_per_graph,function,*args,**kwargs)\n",
    "    x_val, y_val, edge_index_val,weights_val = generateSamples_shortestPath_inner(n_val,max_n_samples_per_graph,function,*args,**kwargs)\n",
    "    x_test, y_test, edge_index_test,weights_test = generateSamples_shortestPath_inner(n_test,max_n_samples_per_graph,function,*args,**kwargs)\n",
    "    return [x_train, x_val, x_test, y_train, y_val, y_test, edge_index_train,edge_index_val,edge_index_test,weights_train,weights_val,weights_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples_sketch_classification_inner(n_samples,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    samples_x = []\n",
    "    samples_y = []\n",
    "    samples_edge_index = []\n",
    "    samples_weights = None\n",
    "    object,directed,weighted = function(*args,**kwargs)\n",
    "    if weighted:\n",
    "        samples_weights = []\n",
    "    if isinstance(object, np.ndarray):\n",
    "        matrix_type = object\n",
    "        object = matrix_to_graph(object)\n",
    "    else:\n",
    "        matrix_type = graph_to_matrix(object)\n",
    "    n = len(object.nodes())\n",
    "    true_distances = np.zeros((n_samples,n))\n",
    "    i = 0\n",
    "    n_err1 = 0\n",
    "    while i<n_samples:\n",
    "        if n_err1 > 100:\n",
    "            raise ValueError(\n",
    "                \"Took too long to generate the full set of samples.\"\n",
    "            )\n",
    "        n_err1 += 1\n",
    "        components = list(nx.strongly_connected_components(object))\n",
    "        connected_components = [c for c in components if len(c) > 1] ## only consider components with >= 2 nodes\n",
    "        components_len = np.array([len(c) for c in connected_components])\n",
    "        components_weights = components_len/np.sum(components_len)\n",
    "        if len(connected_components) > 0:\n",
    "            j = 0\n",
    "            while i<n_samples and j<max_n_samples_per_graph:\n",
    "                n_err2 = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        selected_component = np.random.choice(connected_components, p=components_weights) ## select components with probability ~ its size\n",
    "                        node = np.random.choice(list(selected_component))\n",
    "                        sketch,sample_sets = offlineSketch(matrix_type,node,k)\n",
    "                        if sketch.shape[0] != 0:\n",
    "                            true_distances[i] = shortestDistance_allNodes_networkx(object,node)\n",
    "                            #samples_x.append(torch.tensor(matrix_type.astype(np.float32), requires_grad=True))\n",
    "                            samples_x.append(torch.tensor(np.array([int(node in sample_sets) for node in range(n)]).reshape((n,1)).astype(np.float32), requires_grad=True))\n",
    "                            sketch = np.append(sketch[:,0],node)\n",
    "                            samples_y.append(torch.tensor([int(node in sketch) for node in range(n)]).to(torch.int64))\n",
    "                            samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                            if weighted:\n",
    "                                samples_weights.append(torch.tensor(np.array(list(nx.get_edge_attributes(object,'weight').values()))).to(torch.float32))\n",
    "                            n_err1 = 0\n",
    "                            i += 1\n",
    "                        j += 1\n",
    "                        break\n",
    "                    except:\n",
    "                        n_err2 += 1\n",
    "                        if n_err2 == 100:\n",
    "                            j = max_n_samples_per_graph\n",
    "                            break\n",
    "                            # if i == 0:\n",
    "                            #     raise ValueError(\n",
    "                            #         \"Did not succesfully generate any sample.\"\n",
    "                            #     )\n",
    "                            # else:\n",
    "                            #     raise ValueError(\n",
    "                            #         \"Took too long to generate the full set of samples.\"\n",
    "                            #     )\n",
    "                        continue\n",
    "        else:\n",
    "            object,directed,weighted = function(*args,**kwargs)\n",
    "            if isinstance(object, np.ndarray):\n",
    "                matrix_type = object\n",
    "                object = matrix_to_graph(object)\n",
    "            else:\n",
    "                matrix_type = graph_to_matrix(object)\n",
    "    return samples_x, samples_y, samples_edge_index, samples_weights, true_distances\n",
    "\n",
    "def generateSamples_sketch_classification(n_train,n_val,n_test,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    x_train, y_train, edge_index_train, weights_train, true_distances_train = generateSamples_sketch_classification_inner(n_train,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_val, y_val, edge_index_val, weights_val, true_distances_val = generateSamples_sketch_classification_inner(n_val,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_test, y_test, edge_index_test, weights_test, true_distances_test = generateSamples_sketch_classification_inner(n_test,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    return [x_train, x_val, x_test, y_train, y_val, y_test, edge_index_train,edge_index_val,edge_index_test,weights_train,weights_val,weights_test,true_distances_train,true_distances_val,true_distances_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples_sketch_regression_inner(n_samples,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    samples_x = []\n",
    "    samples_y = []\n",
    "    samples_edge_index = []\n",
    "    samples_weights = None\n",
    "    object,directed,weighted = function(*args,**kwargs)\n",
    "    if weighted:\n",
    "        samples_weights = []\n",
    "    if isinstance(object, np.ndarray):\n",
    "        matrix_type = object\n",
    "        object = matrix_to_graph(object)\n",
    "    else:\n",
    "        matrix_type = graph_to_matrix(object)\n",
    "    n = len(object.nodes())\n",
    "    true_distances = np.zeros((n_samples,n))\n",
    "    i = 0\n",
    "    n_err1 = 0\n",
    "    while i<n_samples:\n",
    "        if n_err1 > 100:\n",
    "            raise ValueError(\n",
    "                \"Took too long to generate the full set of samples.\"\n",
    "            )\n",
    "        n_err1 += 1\n",
    "        #print(i)\n",
    "        components = list(nx.strongly_connected_components(object))\n",
    "        connected_components = [c for c in components if len(c) > 1] ## only consider components with >= 2 nodes\n",
    "        components_len = np.array([len(c) for c in connected_components])\n",
    "        components_weights = components_len/np.sum(components_len)\n",
    "        if len(connected_components) > 0:\n",
    "            j = 0\n",
    "            while i<n_samples and j<max_n_samples_per_graph:\n",
    "                n_err2 = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        #print(2)\n",
    "                        selected_component = np.random.choice(connected_components, p=components_weights) ## select components with probability ~ its size\n",
    "                        node = np.random.choice(list(selected_component))\n",
    "                        sketch,sample_sets = offlineSketch(matrix_type,node,k)\n",
    "                        if sketch.shape[0] != 0:\n",
    "                            #print(3)\n",
    "                            true_distances[i] = shortestDistance_allNodes_networkx(object,node)\n",
    "                            #samples_x.append(torch.tensor(matrix_type.astype(np.float32), requires_grad=True))\n",
    "                            samples_x.append(torch.tensor(np.array([int(node in sample_sets) for node in range(n)]).reshape((n,1)).astype(np.float32), requires_grad=True))\n",
    "                            y = np.zeros(n)\n",
    "                            for t in range(sketch.shape[0]):\n",
    "                                y[int(sketch[t,0])] = sketch[t,1]\n",
    "                            samples_y.append(torch.tensor(y).to(torch.float32))\n",
    "                            samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                            if weighted:\n",
    "                                samples_weights.append(torch.tensor(list(nx.get_edge_attributes(object,'weight').values())).to(torch.float32))\n",
    "                            n_err1 = 0\n",
    "                            i += 1\n",
    "                            #print(i)\n",
    "                        j += 1\n",
    "                        break\n",
    "                    except:\n",
    "                        n_err2 += 1\n",
    "                        if n_err2 == 100:\n",
    "                            j = max_n_samples_per_graph\n",
    "                            break\n",
    "                            # if i == 0:\n",
    "                            #     raise ValueError(\n",
    "                            #         \"Did not succesfully generate any sample.\"\n",
    "                            #     )\n",
    "                            # else:\n",
    "                            #     raise ValueError(\n",
    "                            #         \"Took too long to generate the full set of samples.\"\n",
    "                            #     )\n",
    "                        continue  \n",
    "        else:\n",
    "            object,directed,weighted = function(*args,**kwargs)\n",
    "            if isinstance(object, np.ndarray):\n",
    "                matrix_type = object\n",
    "                object = matrix_to_graph(object)\n",
    "            else:\n",
    "                matrix_type = graph_to_matrix(object)\n",
    "    return samples_x, samples_y, samples_edge_index, samples_weights, true_distances\n",
    "\n",
    "def generateSamples_sketch_regression(n_train,n_val,n_test,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    x_train, y_train, edge_index_train, weights_train, true_distances_train= generateSamples_sketch_regression_inner(n_train,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_val, y_val, edge_index_val, weights_val, true_distances_val = generateSamples_sketch_regression_inner(n_val,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_test, y_test, edge_index_test, weights_test, true_distances_test = generateSamples_sketch_regression_inner(n_test,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    return [x_train, x_val, x_test, y_train, y_val, y_test, edge_index_train,edge_index_val,edge_index_test,weights_train,weights_val,weights_test,true_distances_train,true_distances_val,true_distances_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples_closestPoints_inner(n_samples,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    samples_x = []\n",
    "    samples_y = []\n",
    "    samples_edge_index = []\n",
    "    samples_weights = None\n",
    "    not_disconnected_nodes = []\n",
    "    object,directed,weighted = function(*args,**kwargs)\n",
    "    if weighted:\n",
    "        samples_weights = []\n",
    "    if isinstance(object, np.ndarray):\n",
    "        #matrix_type = object\n",
    "        object = matrix_to_graph(object)\n",
    "    #else:\n",
    "        #matrix_type = graph_to_matrix(object)\n",
    "    n = len(object.nodes())\n",
    "    i = 0\n",
    "    while i<n_samples:\n",
    "        #print(i)\n",
    "        components = list(nx.strongly_connected_components(object))\n",
    "        to_remove = [c.pop() for c in components if len(c) == 1]\n",
    "        if len(to_remove) < len(object.nodes()):\n",
    "            object.remove_nodes_from(to_remove)\n",
    "            remaining = object.nodes()\n",
    "            object = nx.relabel_nodes(object, {node: index for index, node in enumerate(object.nodes())})\n",
    "            if weighted:\n",
    "                weights = list(nx.get_edge_attributes(object,'weight').values())\n",
    "            n_new = len(object.nodes())\n",
    "            r = 0\n",
    "            #print(1)\n",
    "            r_new = r\n",
    "            while 2**r_new > n_new:\n",
    "                r_new -= 1\n",
    "                #print(r_new)\n",
    "            j = 0\n",
    "            #print('a')\n",
    "            while i<n_samples and j<max_n_samples_per_graph:\n",
    "                x_final = np.zeros((n_new,0))\n",
    "                y_final = np.zeros((n_new,0))\n",
    "                for h in range(k):\n",
    "                    sample_sets = [np.random.choice(object.nodes(),size=seed_set_size,replace=False) for t in range(r_new+1)]\n",
    "                    x = np.zeros((n_new,r+1))\n",
    "                    y = np.zeros((n_new,(r+1)*2))\n",
    "                    for t in range(r_new+1):\n",
    "                        x[:,t] = np.array([int(node in sample_sets[t]) for node in range(n_new)])\n",
    "                    for u in range(n_new):\n",
    "                        distances = shortestDistance_allNodes_networkx(object,u)\n",
    "                        if weighted:\n",
    "                            M = n*max(weights)\n",
    "                            distances = np.where(distances == float('inf'), M, distances)\n",
    "                        else:\n",
    "                            M = n\n",
    "                            distances = np.where(distances == float('inf'), M, distances)\n",
    "                        for t in range(r_new+1):\n",
    "                            selected_distances = distances[sample_sets[t]]\n",
    "                            y[u,t*2+1] = np.min(selected_distances)\n",
    "                            if y[u,t*2+1] == M:\n",
    "                                y[u,t*2] = -1\n",
    "                            else:\n",
    "                                y[u,t*2] = sample_sets[t][np.argmin(selected_distances)]\n",
    "                    for t in range(r_new+1,r+1):\n",
    "                        x[:,t] = x[:,t-1]\n",
    "                        y[:t*2] = y[:t*2-2]\n",
    "                        y[:t*2+1] = y[:t*2-1]\n",
    "                    x_final = np.concatenate((x_final, x), axis=1)\n",
    "                    y_final = np.concatenate((y_final, y), axis=1)\n",
    "                samples_x.append(torch.tensor(x_final.astype(np.float32), requires_grad=True))\n",
    "                samples_y.append(torch.tensor(y_final).to(torch.float32))\n",
    "                samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                if weighted:\n",
    "                    samples_weights.append(torch.tensor(weights).to(torch.float32))\n",
    "                not_disconnected_nodes.append(remaining)\n",
    "                i += 1\n",
    "                j += 1\n",
    "        else:\n",
    "            object,directed,weighted = function(*args,**kwargs)\n",
    "            if isinstance(object, np.ndarray):\n",
    "                #matrix_type = object\n",
    "                object = matrix_to_graph(object)\n",
    "            #else:\n",
    "                #matrix_type = graph_to_matrix(object)\n",
    "    return samples_x, samples_y, samples_edge_index, samples_weights,not_disconnected_nodes\n",
    "\n",
    "def generateSamples_closestPoints(n_train,n_val,n_test,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    x_train, y_train, edge_index_train, weights_train,not_disconnected_train = generateSamples_closestPoints_inner(n_train,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_val, y_val, edge_index_val, weights_val,not_disconnected_val = generateSamples_closestPoints_inner(n_val,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_test, y_test, edge_index_test, weights_test,not_disconnected_test = generateSamples_closestPoints_inner(n_test,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    return [x_train, x_val, x_test, y_train, y_val, y_test, edge_index_train,edge_index_val,edge_index_test,weights_train,weights_val,weights_test,not_disconnected_train,not_disconnected_val,not_disconnected_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gcn'\n",
    "criterion_type = 'bce'\n",
    "in_channels = 1\n",
    "out_channels = 2\n",
    "hidden_channels1 = 256\n",
    "hidden_channels2 = 128\n",
    "hidden_channels3 = 64\n",
    "hidden_channels4 = 32\n",
    "hidden_channels5 = 8\n",
    "if model == 'mlp':\n",
    "    if criterion_type in ['ce','bce','multimargin']:\n",
    "        model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,sigmoid = True)\n",
    "    elif criterion_type in ['mse','l2','l1']: \n",
    "        model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,reLU = True)\n",
    "    else:\n",
    "        model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels)\n",
    "elif model == 'gcn':\n",
    "    if criterion_type in ['ce','bce','multimargin']:\n",
    "        model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,sigmoid = True)\n",
    "    elif criterion_type in ['mse','l2','l1']: \n",
    "        model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,reLU = True)\n",
    "    else:\n",
    "        model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Model type not yet defined.\"\n",
    "    )\n",
    "model.load_state_dict(torch.load('trained_model_1_256_2_5.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inner(title,y_actual,y_pred,true_distances,x_samples = None,classification=True): ## target is nx1\n",
    "\n",
    "    if classification:\n",
    "\n",
    "        graph_size = true_distances.shape[1]\n",
    "\n",
    "        ## Calculate the proportion of points selected from the sample.\n",
    "        ratio1 = np.mean(y_pred,axis=1)\n",
    "\n",
    "        ## Check if any disconnected node is in actual sketch.\n",
    "        true_distances[true_distances == np.inf] = -1 ## convert inf into a large number for the sake of calculations\n",
    "        M = np.max(true_distances) * 10\n",
    "        true_distances[true_distances == -1] = M\n",
    "        y_actual = torch.stack(y_actual).detach().numpy()\n",
    "        dump_actual = y_actual*true_distances\n",
    "        n_dump_actual_equal_M = np.sum((dump_actual == M).astype(int),axis=1)\n",
    "        ratio2 = n_dump_actual_equal_M/np.sum(y_actual,axis=1) ## nan means empty sketch\n",
    "        #ratio2[np.isnan(ratio2)] = -1\n",
    "\n",
    "        ## Check if any disconnected node is chosen.\n",
    "        dump = y_pred*true_distances\n",
    "        n_true_distances_equal_M = np.sum((true_distances == M).astype(int),axis=1)\n",
    "        n_dump_equal_M = np.sum((dump == M).astype(int),axis=1)\n",
    "        n_selected = np.sum(y_pred,axis=1)\n",
    "        n_selected[n_selected == 0] = 0.0001\n",
    "        ratio3 = n_dump_equal_M/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio3[np.isnan(ratio3)] = -1\n",
    "\n",
    "        if x_samples != None:\n",
    "\n",
    "            ## Seed node and neighbors -> 1. Others -> 0\n",
    "            seed_or_neighbors = np.zeros_like(true_distances)\n",
    "            _,seeds = np.where(true_distances == 0) ## number of seeds = number of samples because we selected one seed from each sample\n",
    "            seed_disconnected = np.zeros(seed_or_neighbors.shape[0])\n",
    "            for i in range(len(seeds)):\n",
    "                seed = seeds[i]\n",
    "                seed_or_neighbors[i] = x_samples[i].detach().numpy()[seed] ## x_samples[i] has to be adjacency matrix\n",
    "                seed_disconnected[i] = sum((seed_or_neighbors[i]>0).astype(int))>0\n",
    "                seed_or_neighbors[i,seed] = 1\n",
    "            seed_or_neighbors_selected = seed_or_neighbors*y_pred\n",
    "\n",
    "            ## Is seed node not disconnected from the remaining?\n",
    "            seed_disconnected = seed_disconnected.astype(int)\n",
    "\n",
    "            ## Have seed node and its neighbors been chosen?\n",
    "            ratio4 = np.sum(seed_or_neighbors_selected,axis=1)/np.sum(seed_or_neighbors,axis=1)\n",
    "\n",
    "            ## How many of the chosen nodes are seed node and its neighbors?\n",
    "            ratio5 = np.sum(seed_or_neighbors_selected,axis=1)/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "            #ratio5[np.isnan(ratio5)] = -1\n",
    "\n",
    "        ## Check if the chosen nodes are closer than random selection.\n",
    "        sum_distance_selected = np.sum(dump,axis=1)\n",
    "        term0 = np.mean(true_distances,axis=1)\n",
    "        term0[term0 == 0] = 0.0001\n",
    "        ratio6 = sum_distance_selected/n_selected/term0 ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio6[np.isnan(ratio6)] = -1\n",
    "\n",
    "        ## Same as ratio6 but excludes disconnected nodes with inf distance to the seed node.\n",
    "        n_selected[n_selected == 0.0001] = 0\n",
    "        term1 = n_selected-n_dump_equal_M\n",
    "        term1[term1 == 0] = 0.0001\n",
    "        term2 = np.sum(true_distances,axis=1)-n_true_distances_equal_M*M\n",
    "        term2[term2 == 0] = 0.0001\n",
    "        ratio7 = (sum_distance_selected-n_dump_equal_M*M)/term1/term2*(graph_size-n_true_distances_equal_M)\n",
    "        #ratio7[np.isnan(ratio7)] = -1\n",
    "        ## nan means no nodes have been selected (ratio1 = 0) or selected nodes are all disconnected from seed node (inf distance)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(ratio1)), ratio1, color = '#1f77b4', label = 'ratio 1: size of selected community to size of sample ratio',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio6, color = '#8c564b', label = 'ratio 2: mean distance of chosen nodes to seed node divided by mean distance of all nodes to seed node',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio7, color = '#e377c2', label = 'ratio 3: same as ratio 6 but excludes disconnected nodes',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio2, color = '#ff7f0e', label = 'ratio 4: proportion of disconnected nodes in actual sketch',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio3, color = '#2ca02c', label = 'ratio 5: proportion of disconnected nodes in the chosen community',alpha = 0.6)\n",
    "        if x_samples != None:\n",
    "            plt.plot(range(len(ratio1)), ratio4, color = '#d62728', label = 'ratio 6: proportion of seed node and its neighbors that are selected',alpha = 0.6)\n",
    "            plt.plot(range(len(ratio1)), ratio5, color = '#9467bd', label = 'ratio 7: proportion of selected nodes that are seed node or its neighbors',alpha = 0.6)\n",
    "            plt.scatter(range(len(ratio1)), seed_disconnected, color = 'k', marker='x', label = 'is seed node not disconnected from the remaining nodes in graph? 1 = True, 0 = False',alpha = 0.6)\n",
    "        plt.xlabel(\"sample index\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(title)\n",
    "        plt.legend(loc = 'lower center',bbox_to_anchor=(0.48, -0.6))\n",
    "        plt.show()\n",
    "\n",
    "        true_distances[true_distances == M] = np.inf\n",
    "        if x_samples == None:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3)]\n",
    "        else:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3),np.mean(ratio4),np.mean(ratio5)]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        ## Calculate prediction to true distance.\n",
    "        #y_actual[y_actual == np.inf] = -1 ## convert inf into a large number for the sake of calculations\n",
    "        #y_pred[y_pred == np.inf] = -1\n",
    "        #M = max(np.max(y_actual),np.max(y_pred))*10\n",
    "        #y_actual[y_actual == -1] = M\n",
    "        y_actual = np.array(y_actual)\n",
    "        #y_actual[y_actual == 0] = 0.0001 ## to avoid ratio = x/0\n",
    "        #y_pred[y_pred == -1] = M\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_actual.shape)\n",
    "        diff_all = y_actual - y_pred\n",
    "\n",
    "        for s in [0,1]:\n",
    "            if s == 0:\n",
    "                plot_title = 'closest node index regression: '\n",
    "            else:\n",
    "                plot_title = 'shortest distance regression: '\n",
    "            diff = diff_all[:, :, s::2]\n",
    "            diff = diff.reshape((diff.shape[0]*diff.shape[1],diff.shape[2]))\n",
    "\n",
    "            if len(np.unique(diff)) == 1:\n",
    "                print('y_actual - y_actual is '+str(diff[0])+' for all nodes in all seed sets in '+title+'.')\n",
    "            else:\n",
    "                for r in range(diff.shape[1]):\n",
    "                    values, base = np.histogram(diff[:,r], bins=100)\n",
    "                    cumulative = np.cumsum(values)\n",
    "                    plt.plot(base[:-1], cumulative, label='r = '+str(r))\n",
    "                    #plt.scatter([r]*ratio.shape[0],ratio[:,r])\n",
    "                plt.xlabel(\"y_actual - y_pred\")\n",
    "                plt.ylabel(\"cummulative frequency\")\n",
    "                plt.title(plot_title + title)\n",
    "                plt.legend()\n",
    "\n",
    "                if plot_title == 'shortest distance regression: ':\n",
    "                    filenames = os.listdir(dir)\n",
    "                    if '0.png' not in filenames:\n",
    "                        plt.savefig(dir+'/0.png')\n",
    "                    else:\n",
    "                        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "                        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')                \n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "                m0 = np.mean(diff,axis=0)\n",
    "                m1 = np.mean(diff,axis=1)\n",
    "                print('mean difference per seed set = ',m0)\n",
    "                print('mean difference per seed node = ',m1)\n",
    "\n",
    "        #y_actual[y_actual == M] = np.inf\n",
    "        #y_actual[y_actual == 0.0001] = 0\n",
    "        #y_pred[y_pred == M] = np.inf\n",
    "\n",
    "        #return [m0,m1]\n",
    "    \n",
    "def evaluate(model,criterion_type,samples,adjacency_matrix_available = False):\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        classification = False\n",
    "    else:\n",
    "        classification = True\n",
    "    y_pred_train,y_pred_val,y_pred_test = predict_allBatches(model,criterion_type,samples)\n",
    "    if adjacency_matrix_available:\n",
    "        evaluate_inner('training data',samples[3],y_pred_train,samples[12],samples[0],classification)\n",
    "        evaluate_inner('validation data',samples[4],y_pred_val,samples[13],samples[1],classification)\n",
    "        evaluate_inner('test data',samples[5],y_pred_test,samples[14],samples[2],classification)\n",
    "    else:\n",
    "        evaluate_inner('training data',samples[3],y_pred_train,samples[12],classification=classification)\n",
    "        evaluate_inner('validation data',samples[4],y_pred_val,samples[13],classification=classification)\n",
    "        evaluate_inner('test data',samples[5],y_pred_test,samples[14],classification=classification)\n",
    "    #return np.array(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "\n",
    "def test(gpu_bool,model,criterion,criterion_type,samples_x,samples_y,samples_edge_index = None,samples_weights = None):\n",
    "\n",
    "    t_loss = 0\n",
    "    total_samples = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if samples_edge_index == None:\n",
    "            for x,y in list(zip(samples_x,samples_y)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                out = model(x)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += scale*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        elif samples_weights == None:\n",
    "            for x,y,edge_index in list(zip(samples_x,samples_y,samples_edge_index)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += scale*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        else:\n",
    "            for x,y,edge_index,weights in list(zip(samples_x,samples_y,samples_edge_index,samples_weights)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                    weights = weights.to('cuda:1')\n",
    "                out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += scale*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "\n",
    "    t_loss = t_loss/total_samples\n",
    "    t_loss = t_loss.cpu()\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        return t_loss, None, None, None\n",
    "    else:\n",
    "        y_true = np.array([y.cpu() for y in y_true])\n",
    "        y_pred = np.array([y.cpu() for y in y_pred])\n",
    "        #print(np.array(y_true).shape)\n",
    "        #print(np.array(y_pred).shape)\n",
    "        t_accuracy = sum(y_true == y_pred)/len(y_true)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        #print(cm.ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        t_sensitivity = tp / (tp + fn)\n",
    "        t_specificity = tn / (tn + fp)\n",
    "        return t_loss, t_accuracy, t_sensitivity, t_specificity\n",
    "\n",
    "def train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val,edge_index_train=None,edge_index_val=None,weights_train=None,weights_val=None):\n",
    "    model.train()\n",
    "    if edge_index_train == None:\n",
    "        for x,y in list(zip(x_train,y_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "            out = model(x)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = scale*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    elif weights_train == None:\n",
    "        for x,y,edge_index in list(zip(x_train,y_train,edge_index_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "            out = model(x,edge_index)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = scale*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    else:\n",
    "        for x,y,edge_index,weights in list(zip(x_train,y_train,edge_index_train,weights_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "                weights = weights.to('cuda:1')\n",
    "            out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = scale*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    train_loss, train_accuracy, train_sensitivity, train_specificity = test(gpu_bool,model,criterion,criterion_type,x_train,y_train,edge_index_train,weights_train)\n",
    "    v_loss, v_accuracy, v_sensitivity, v_specificity = test(gpu_bool,model,criterion,criterion_type,x_val,y_val,edge_index_val,weights_val)\n",
    "\n",
    "    if scheduler_type in ['step','exponential','cyclic','cosine']:\n",
    "        scheduler[0].step()\n",
    "    elif scheduler_type == 'reduce_on_plateau': \n",
    "        scheduler[0].step(v_loss)\n",
    "    elif scheduler_type == 'cyclic-cosine':\n",
    "        scheduler[0].step()\n",
    "        scheduler[1].step()\n",
    "    return train_loss,train_accuracy,train_sensitivity,train_specificity,v_loss,v_accuracy,v_sensitivity,v_specificity\n",
    "\n",
    "def visualizeTNSE(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().numpy())\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"\")\n",
    "    plt.show()\n",
    "\n",
    "def run(samples,model,criterion_type,optimizer_type,scheduler_type,num_epochs=100,early_stopping_patience=None,visualize = False,save_model = False):\n",
    "\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "\n",
    "    x_train = samples[0]\n",
    "    x_val = samples[1]\n",
    "    x_test = samples[2]\n",
    "    y_train = samples[3]\n",
    "    y_val = samples[4]\n",
    "    y_test = samples[5]\n",
    "    edge_index_train = samples[6]\n",
    "    edge_index_val = samples[7]\n",
    "    edge_index_test = samples[8]\n",
    "    weights_train = samples[9]\n",
    "    weights_val = samples[10]\n",
    "    weights_test = samples[11]\n",
    "\n",
    "    if criterion_type in ['ce','bce','multimargin']:\n",
    "        out_channels = 2\n",
    "    elif criterion_type == 'mse-mse':\n",
    "        out_channels = y_train[0].shape[1]\n",
    "    else:\n",
    "        out_channels = 1\n",
    "\n",
    "    if isinstance(model, str):\n",
    "        model_type = model\n",
    "        model,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model_type,criterion_type,optimizer_type,scheduler_type)\n",
    "    else:\n",
    "        model_type = model.name\n",
    "        _,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model, criterion_type,optimizer_type,scheduler_type)\n",
    "    print(model)\n",
    "\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "        \n",
    "    if visualize:\n",
    "        model.eval()\n",
    "        if model_type == 'mlp':\n",
    "            out = model(x_test[0])\n",
    "        else:\n",
    "            out = model(x_test[0],edge_index_test[0])\n",
    "        visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_sen = []\n",
    "    train_spec = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_sen = []\n",
    "    val_spec = []\n",
    "\n",
    "    if early_stopping_patience != None:\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if model_type == 'mlp':\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val)\n",
    "        else:\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,optimizer_type,x_train,x_val,y_train,y_val,edge_index_train,edge_index_val,weights_train,weights_val)\n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(t_acc)\n",
    "        train_sen.append(t_sen)\n",
    "        train_spec.append(t_spec)\n",
    "        val_loss.append(v_loss)\n",
    "        val_acc.append(v_acc)\n",
    "        val_sen.append(v_sen)\n",
    "        val_spec.append(v_spec)\n",
    "        #print(t_loss)\n",
    "        #print(t_acc_or_error)\n",
    "        if epoch % 10 == 0:\n",
    "            if criterion_type in ['mse','l2','mse-mse']:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MSE): {t_loss:.4f}, Validation Loss (MSE): {v_loss:.4f}')\n",
    "            elif criterion_type == 'l1':\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MAE): {t_loss:.4f}, Validation Loss (MAE): {v_loss:.4f}')\n",
    "            else:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss: {t_loss:.4f}, Training Accuracy: {t_acc:.4f}, Training Sensitivity: {t_sen:.4f}, Training Specificity: {t_spec:.4f}, Validation Loss: {v_loss:.4f}, Validation Accuracy: {v_acc:.4f}, Validation Sensitivity: {v_sen:.4f}, Validation Specificity: {v_spec:.4f}')\n",
    "\n",
    "        if early_stopping_patience != None:\n",
    "            if v_loss < best_val_loss:\n",
    "                best_val_loss = v_loss\n",
    "                best_epoch = epoch\n",
    "                no_improvement_count = 0\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            if no_improvement_count >= early_stopping_patience:\n",
    "                model.load_state_dict(torch.load('best_model.pth'))\n",
    "                if gpu_bool:\n",
    "                    model = model.to('cuda:1')\n",
    "                break\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test)\n",
    "    else:\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test, edge_index_test, weights_test)\n",
    "    \n",
    "    if early_stopping_patience == None:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "    else:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "\n",
    "    x = range(1, len(train_loss)+1)\n",
    "    plt.plot(x, train_loss, color = '#1f77b4', label = 'training loss', alpha = 0.6)\n",
    "    plt.plot(x, val_loss, color = '#ff7f0e', label = 'validation loss', alpha = 0.6)\n",
    "    if criterion_type in ['ce','bce','bcelogits','multimargin']:\n",
    "        plt.plot(x, train_acc, color = '#2ca02c', label = 'training accuracy', alpha = 0.6)\n",
    "        plt.plot(x, val_acc, color = '#d62728', label = 'validation accuracy', alpha = 0.6)\n",
    "        plt.plot(x, train_sen, color = '#9467bd', label = 'training sensitivity', alpha = 0.6)\n",
    "        plt.plot(x, val_sen, color = '#8c564b', label = 'validation sensitivity', alpha = 0.6)\n",
    "        plt.plot(x, train_spec, color = '#e377c2', label = 'training specificity', alpha = 0.6)\n",
    "        plt.plot(x, val_spec, color = '#7f7f7f', label = 'validation specificity', alpha = 0.6)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"training results\")\n",
    "    plt.legend(loc = 'right',bbox_to_anchor=(1.45, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "    if visualize:\n",
    "        model.eval()\n",
    "        if model_type == 'mlp':\n",
    "            out = model(x_test[0])\n",
    "        else:\n",
    "            out = model(x_test[0],edge_index_test[0])\n",
    "        visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), 'trained_model_'+str(model.in_channels)+'_'+str(model.first_hidden_channels)+'_'+str(model.out_channels)+'_'+str(model.n_hidden_layers)+'.pth')\n",
    "\n",
    "    evaluate(model,criterion_type,samples,adjacency_matrix_available = False)\n",
    "\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        if early_stopping_patience == None:\n",
    "            return [model, train_loss[-1], val_loss[-1], test_loss]\n",
    "        else:\n",
    "            return [model, train_loss[best_epoch-1], val_loss[best_epoch-1], test_loss]\n",
    "    else:\n",
    "        if early_stopping_patience == None:\n",
    "            return [model, train_acc[-1], train_sen[-1], train_spec[-1], val_acc[-1], val_sen[-1], val_spec[-1], test_acc, test_sen, test_spec]\n",
    "        else:\n",
    "            return [model, train_acc[best_epoch-1], train_sen[best_epoch-1], train_spec[best_epoch-1], val_acc[best_epoch-1], val_sen[best_epoch-1], val_spec[best_epoch-1], test_acc, test_sen, test_spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_set_size = 1\n",
    "k = 1\n",
    "dir = '1 seed set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,5000,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('er5000_015.pkl', 'wb') as file:\n",
    "    #pickle.dump(samples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('er5000_015.pkl', 'rb') as file:\n",
    "    #samples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "#results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 100\n",
    "#results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 10\n",
    "#results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1\n",
    "#results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "#results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "#results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "#results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 100*MSE1 + MSE2\n",
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10*MSE1 + MSE2\n",
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE1 + MSE2\n",
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/10\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/100\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0\n",
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "results = run(samples,'gcn','mse-mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples_closestPoints_inner(n_samples,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    samples_x = []\n",
    "    samples_y1 = []\n",
    "    samples_y2 = []\n",
    "    samples_y3 = []\n",
    "    samples_edge_index = []\n",
    "    samples_weights = None\n",
    "    not_disconnected_nodes = []\n",
    "    object,directed,weighted = function(*args,**kwargs)\n",
    "    if weighted:\n",
    "        samples_weights = []\n",
    "    if isinstance(object, np.ndarray):\n",
    "        #matrix_type = object\n",
    "        object = matrix_to_graph(object)\n",
    "    #else:\n",
    "        #matrix_type = graph_to_matrix(object)\n",
    "    n = len(object.nodes())\n",
    "    i = 0\n",
    "    while i<n_samples:\n",
    "        #print(i)\n",
    "        components = list(nx.strongly_connected_components(object))\n",
    "        to_remove = [c.pop() for c in components if len(c) == 1]\n",
    "        if len(to_remove) < len(object.nodes()):\n",
    "            object.remove_nodes_from(to_remove)\n",
    "            remaining = object.nodes()\n",
    "            object = nx.relabel_nodes(object, {node: index for index, node in enumerate(object.nodes())})\n",
    "            if weighted:\n",
    "                weights = list(nx.get_edge_attributes(object,'weight').values())\n",
    "            n_new = len(object.nodes())\n",
    "            r = 0\n",
    "            #print(1)\n",
    "            r_new = r\n",
    "            while 2**r_new > n_new:\n",
    "                r_new -= 1\n",
    "                #print(r_new)\n",
    "            j = 0\n",
    "            #print('a')\n",
    "            while i<n_samples and j<max_n_samples_per_graph:\n",
    "                x_final = np.zeros((n_new,0))\n",
    "                y1_final = np.zeros((n_new,0))\n",
    "                y2_final = np.zeros((n_new,0))\n",
    "                y3_final = np.zeros((n_new,0))\n",
    "                for h in range(k):\n",
    "                    sample_sets = [np.random.choice(object.nodes(),size=seed_set_size,replace=False) for t in range(r_new+1)]\n",
    "                    x = np.zeros((n_new,r+1))\n",
    "                    y1 = np.zeros((n_new,(r+1)*2))\n",
    "                    y2 = np.zeros((n_new,r+1))\n",
    "                    y3 = np.zeros((n_new,r+1))\n",
    "                    for t in range(r_new+1):\n",
    "                        x[:,t] = np.array([int(node in sample_sets[t]) for node in range(n_new)])\n",
    "                    for u in range(n_new):\n",
    "                        distances = shortestDistance_allNodes_networkx(object,u)\n",
    "                        if weighted:\n",
    "                            M = n*max(weights)\n",
    "                            distances = np.where(distances == float('inf'), M, distances)\n",
    "                        else:\n",
    "                            M = n\n",
    "                            distances = np.where(distances == float('inf'), M, distances)\n",
    "                        for t in range(r_new+1):\n",
    "                            selected_distances = distances[sample_sets[t]]\n",
    "                            y1[u,t*2+1] = np.min(selected_distances)\n",
    "                            y3[u,t] = y1[u,t*2+1]\n",
    "                            if y1[u,t*2+1] == M:\n",
    "                                y1[u,t*2] = -1\n",
    "                            else:\n",
    "                                y1[u,t*2] = sample_sets[t][np.argmin(selected_distances)]\n",
    "                            y2[u,t] = y1[u,t*2]\n",
    "                    for t in range(r_new+1,r+1):\n",
    "                        x[:,t] = x[:,t-1]\n",
    "                        y1[:t*2] = y1[:t*2-2]\n",
    "                        y2[:,t] = y1[:,t*2]\n",
    "                        y1[:t*2+1] = y1[:t*2-1]\n",
    "                        y3[:,t] = y1[:,t*2+1]\n",
    "                    x_final = np.concatenate((x_final, x), axis=1)\n",
    "                    y1_final = np.concatenate((y1_final, y1), axis=1)\n",
    "                    y2_final = np.concatenate((y2_final, y2), axis=1)\n",
    "                    y3_final = np.concatenate((y3_final, y3), axis=1)\n",
    "                    #print(y1_final.shape)\n",
    "                samples_x.append(torch.tensor(x_final.astype(np.float32), requires_grad=True))\n",
    "                samples_y1.append(torch.tensor(y1_final).to(torch.float32))\n",
    "                samples_y2.append(torch.tensor(y2_final).to(torch.float32))\n",
    "                samples_y3.append(torch.tensor(y3_final).to(torch.float32))\n",
    "                samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                if weighted:\n",
    "                    samples_weights.append(torch.tensor(weights).to(torch.float32))\n",
    "                not_disconnected_nodes.append(remaining)\n",
    "                i += 1\n",
    "                j += 1\n",
    "        else:\n",
    "            object,directed,weighted = function(*args,**kwargs)\n",
    "            if isinstance(object, np.ndarray):\n",
    "                #matrix_type = object\n",
    "                object = matrix_to_graph(object)\n",
    "            #else:\n",
    "                #matrix_type = graph_to_matrix(object)\n",
    "    return samples_x, samples_y1, samples_y2, samples_y3, samples_edge_index, samples_weights,not_disconnected_nodes\n",
    "\n",
    "def generateSamples_closestPoints(n_train,n_val,n_test,max_n_samples_per_graph,k,function,*args,**kwargs):\n",
    "    x_train, y1_train, y2_train, y3_train, edge_index_train, weights_train,not_disconnected_train = generateSamples_closestPoints_inner(n_train,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_val, y1_val, y2_val, y3_val, edge_index_val, weights_val,not_disconnected_val = generateSamples_closestPoints_inner(n_val,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    x_test, y1_test, y2_test, y3_test, edge_index_test, weights_test,not_disconnected_test = generateSamples_closestPoints_inner(n_test,max_n_samples_per_graph,k,function,*args,**kwargs)\n",
    "    return [x_train, x_val, x_test, [y1_train, y2_train, y3_train], [y1_val, y2_val, y3_val], [y1_test, y2_test, y3_test], edge_index_train,edge_index_val,edge_index_test,weights_train,weights_val,weights_test,not_disconnected_train,not_disconnected_val,not_disconnected_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inner(plot_title,title,y_actual,y_pred,true_distances,x_samples = None,classification=True): ## target is nx1\n",
    "\n",
    "    if classification:\n",
    "\n",
    "        graph_size = true_distances.shape[1]\n",
    "\n",
    "        ## Calculate the proportion of points selected from the sample.\n",
    "        ratio1 = np.mean(y_pred,axis=1)\n",
    "\n",
    "        ## Check if any disconnected node is in actual sketch.\n",
    "        true_distances[true_distances == np.inf] = -1 ## convert inf into a large number for the sake of calculations\n",
    "        M = np.max(true_distances) * 10\n",
    "        true_distances[true_distances == -1] = M\n",
    "        y_actual = torch.stack(y_actual).detach().numpy()\n",
    "        dump_actual = y_actual*true_distances\n",
    "        n_dump_actual_equal_M = np.sum((dump_actual == M).astype(int),axis=1)\n",
    "        ratio2 = n_dump_actual_equal_M/np.sum(y_actual,axis=1) ## nan means empty sketch\n",
    "        #ratio2[np.isnan(ratio2)] = -1\n",
    "\n",
    "        ## Check if any disconnected node is chosen.\n",
    "        dump = y_pred*true_distances\n",
    "        n_true_distances_equal_M = np.sum((true_distances == M).astype(int),axis=1)\n",
    "        n_dump_equal_M = np.sum((dump == M).astype(int),axis=1)\n",
    "        n_selected = np.sum(y_pred,axis=1)\n",
    "        n_selected[n_selected == 0] = 0.0001\n",
    "        ratio3 = n_dump_equal_M/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio3[np.isnan(ratio3)] = -1\n",
    "\n",
    "        if x_samples != None:\n",
    "\n",
    "            ## Seed node and neighbors -> 1. Others -> 0\n",
    "            seed_or_neighbors = np.zeros_like(true_distances)\n",
    "            _,seeds = np.where(true_distances == 0) ## number of seeds = number of samples because we selected one seed from each sample\n",
    "            seed_disconnected = np.zeros(seed_or_neighbors.shape[0])\n",
    "            for i in range(len(seeds)):\n",
    "                seed = seeds[i]\n",
    "                seed_or_neighbors[i] = x_samples[i].detach().numpy()[seed] ## x_samples[i] has to be adjacency matrix\n",
    "                seed_disconnected[i] = sum((seed_or_neighbors[i]>0).astype(int))>0\n",
    "                seed_or_neighbors[i,seed] = 1\n",
    "            seed_or_neighbors_selected = seed_or_neighbors*y_pred\n",
    "\n",
    "            ## Is seed node not disconnected from the remaining?\n",
    "            seed_disconnected = seed_disconnected.astype(int)\n",
    "\n",
    "            ## Have seed node and its neighbors been chosen?\n",
    "            ratio4 = np.sum(seed_or_neighbors_selected,axis=1)/np.sum(seed_or_neighbors,axis=1)\n",
    "\n",
    "            ## How many of the chosen nodes are seed node and its neighbors?\n",
    "            ratio5 = np.sum(seed_or_neighbors_selected,axis=1)/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "            #ratio5[np.isnan(ratio5)] = -1\n",
    "\n",
    "        ## Check if the chosen nodes are closer than random selection.\n",
    "        sum_distance_selected = np.sum(dump,axis=1)\n",
    "        term0 = np.mean(true_distances,axis=1)\n",
    "        term0[term0 == 0] = 0.0001\n",
    "        ratio6 = sum_distance_selected/n_selected/term0 ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio6[np.isnan(ratio6)] = -1\n",
    "\n",
    "        ## Same as ratio6 but excludes disconnected nodes with inf distance to the seed node.\n",
    "        n_selected[n_selected == 0.0001] = 0\n",
    "        term1 = n_selected-n_dump_equal_M\n",
    "        term1[term1 == 0] = 0.0001\n",
    "        term2 = np.sum(true_distances,axis=1)-n_true_distances_equal_M*M\n",
    "        term2[term2 == 0] = 0.0001\n",
    "        ratio7 = (sum_distance_selected-n_dump_equal_M*M)/term1/term2*(graph_size-n_true_distances_equal_M)\n",
    "        #ratio7[np.isnan(ratio7)] = -1\n",
    "        ## nan means no nodes have been selected (ratio1 = 0) or selected nodes are all disconnected from seed node (inf distance)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(ratio1)), ratio1, color = '#1f77b4', label = 'ratio 1: size of selected community to size of sample ratio',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio6, color = '#8c564b', label = 'ratio 2: mean distance of chosen nodes to seed node divided by mean distance of all nodes to seed node',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio7, color = '#e377c2', label = 'ratio 3: same as ratio 6 but excludes disconnected nodes',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio2, color = '#ff7f0e', label = 'ratio 4: proportion of disconnected nodes in actual sketch',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio3, color = '#2ca02c', label = 'ratio 5: proportion of disconnected nodes in the chosen community',alpha = 0.6)\n",
    "        if x_samples != None:\n",
    "            plt.plot(range(len(ratio1)), ratio4, color = '#d62728', label = 'ratio 6: proportion of seed node and its neighbors that are selected',alpha = 0.6)\n",
    "            plt.plot(range(len(ratio1)), ratio5, color = '#9467bd', label = 'ratio 7: proportion of selected nodes that are seed node or its neighbors',alpha = 0.6)\n",
    "            plt.scatter(range(len(ratio1)), seed_disconnected, color = 'k', marker='x', label = 'is seed node not disconnected from the remaining nodes in graph? 1 = True, 0 = False',alpha = 0.6)\n",
    "        plt.xlabel(\"sample index\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(title)\n",
    "        plt.legend(loc = 'lower center',bbox_to_anchor=(0.48, -0.6))\n",
    "        plt.show()\n",
    "\n",
    "        true_distances[true_distances == M] = np.inf\n",
    "        if x_samples == None:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3)]\n",
    "        else:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3),np.mean(ratio4),np.mean(ratio5)]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        ## Calculate prediction to true distance.\n",
    "        #y_actual[y_actual == np.inf] = -1 ## convert inf into a large number for the sake of calculations\n",
    "        #y_pred[y_pred == np.inf] = -1\n",
    "        #M = max(np.max(y_actual),np.max(y_pred))*10\n",
    "        #y_actual[y_actual == -1] = M\n",
    "        y_actual = np.array(y_actual)\n",
    "        #y_actual[y_actual == 0] = 0.0001 ## to avoid ratio = x/0\n",
    "        #y_pred[y_pred == -1] = M\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_actual.shape)\n",
    "        diff = y_actual - y_pred.reshape((y_actual.shape[0],y_actual.shape[1],y_actual.shape[2]))\n",
    "        diff = diff.reshape((diff.shape[0]*diff.shape[1],diff.shape[2]))\n",
    "        \n",
    "        if len(np.unique(diff)) == 1:\n",
    "            print('y_actual - y_actual is '+str(diff[0])+' for all nodes in all seed sets in '+title+'.')\n",
    "        else:\n",
    "            for r in range(diff.shape[1]):\n",
    "                values, base = np.histogram(diff[:,r], bins=100)\n",
    "                cumulative = np.cumsum(values)\n",
    "                plt.plot(base[:-1], cumulative, label='r = '+str(r))\n",
    "                #plt.scatter([r]*ratio.shape[0],ratio[:,r])\n",
    "            plt.xlabel(\"y_actual - y_pred\")\n",
    "            plt.ylabel(\"cummulative frequency\")\n",
    "            plt.title(plot_title+title)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            m0 = np.mean(diff,axis=0)\n",
    "            m1 = np.mean(diff,axis=1)\n",
    "            print('mean difference per seed set = ',m0)\n",
    "            print('mean difference per seed node = ',m1)\n",
    "\n",
    "        #y_actual[y_actual == M] = np.inf\n",
    "        #y_actual[y_actual == 0.0001] = 0\n",
    "        #y_pred[y_pred == M] = np.inf\n",
    "\n",
    "        #return [m0,m1]\n",
    "    \n",
    "def evaluate(plot_title,model,criterion_type,samples,adjacency_matrix_available = False):\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        classification = False\n",
    "    else:\n",
    "        classification = True\n",
    "    y_pred_train,y_pred_val,y_pred_test = predict_allBatches(model,criterion_type,samples)\n",
    "    if adjacency_matrix_available:\n",
    "        evaluate_inner(plot_title,'training data',samples[3],y_pred_train,samples[12],samples[0],classification)\n",
    "        evaluate_inner(plot_title,'validation data',samples[4],y_pred_val,samples[13],samples[1],classification)\n",
    "        evaluate_inner(plot_title,'test data',samples[5],y_pred_test,samples[14],samples[2],classification)\n",
    "    else:\n",
    "        evaluate_inner(plot_title,'training data',samples[3],y_pred_train,samples[12],classification=classification)\n",
    "        evaluate_inner(plot_title,'validation data',samples[4],y_pred_val,samples[13],classification=classification)\n",
    "        evaluate_inner(plot_title,'test data',samples[5],y_pred_test,samples[14],classification=classification)\n",
    "    #return np.array(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gpu_bool,model,criterion,criterion_type,samples_x,samples_y,samples_edge_index = None,samples_weights = None):\n",
    "    t_loss = 0\n",
    "    total_samples = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if samples_edge_index == None:\n",
    "            for x,y in list(zip(samples_x,samples_y)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                out = model(x)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        elif samples_weights == None:\n",
    "            for x,y,edge_index in list(zip(samples_x,samples_y,samples_edge_index)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        else:\n",
    "            for x,y,edge_index,weights in list(zip(samples_x,samples_y,samples_edge_index,samples_weights)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                    weights = weights.to('cuda:1')\n",
    "                out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "\n",
    "    t_loss = t_loss/total_samples\n",
    "    t_loss = t_loss.cpu()\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        return t_loss, None, None, None\n",
    "    else:\n",
    "        y_true = np.array([y.cpu() for y in y_true])\n",
    "        y_pred = np.array([y.cpu() for y in y_pred])\n",
    "        #print(np.array(y_true).shape)\n",
    "        #print(np.array(y_pred).shape)\n",
    "        t_accuracy = sum(y_true == y_pred)/len(y_true)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        #print(cm.ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        t_sensitivity = tp / (tp + fn)\n",
    "        t_specificity = tn / (tn + fp)\n",
    "        return t_loss, t_accuracy, t_sensitivity, t_specificity\n",
    "\n",
    "def train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val,edge_index_train=None,edge_index_val=None,weights_train=None,weights_val=None):\n",
    "    model.train()\n",
    "    if edge_index_train == None:\n",
    "        for x,y in list(zip(x_train,y_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "            out = model(x)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    elif weights_train == None:\n",
    "        for x,y,edge_index in list(zip(x_train,y_train,edge_index_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "            out = model(x,edge_index)  # Perform a single forward pass\n",
    "            #print(out.shape)\n",
    "            #print(y.shape)\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    else:\n",
    "        for x,y,edge_index,weights in list(zip(x_train,y_train,edge_index_train,weights_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "                weights = weights.to('cuda:1')\n",
    "            out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    train_loss, train_accuracy, train_sensitivity, train_specificity = test(gpu_bool,model,criterion,criterion_type,x_train,y_train,edge_index_train,weights_train)\n",
    "    v_loss, v_accuracy, v_sensitivity, v_specificity = test(gpu_bool,model,criterion,criterion_type,x_val,y_val,edge_index_val,weights_val)\n",
    "\n",
    "    if scheduler_type in ['step','exponential','cyclic','cosine']:\n",
    "        scheduler[0].step()\n",
    "    elif scheduler_type == 'reduce_on_plateau': \n",
    "        scheduler[0].step(v_loss)\n",
    "    elif scheduler_type == 'cyclic-cosine':\n",
    "        scheduler[0].step()\n",
    "        scheduler[1].step()\n",
    "    return train_loss,train_accuracy,train_sensitivity,train_specificity,v_loss,v_accuracy,v_sensitivity,v_specificity\n",
    "\n",
    "def run(samples,model,criterion_type,optimizer_type,scheduler_type,num_epochs=100,early_stopping_patience=None,visualize = False,save_model = False):\n",
    "\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "\n",
    "    x_train = samples[0]\n",
    "    x_val = samples[1]\n",
    "    x_test = samples[2]\n",
    "    edge_index_train = samples[6]\n",
    "    edge_index_val = samples[7]\n",
    "    edge_index_test = samples[8]\n",
    "    weights_train = samples[9]\n",
    "    weights_val = samples[10]\n",
    "    weights_test = samples[11]\n",
    "\n",
    "    new_samples = samples\n",
    "\n",
    "    #print(samples[3][0][0].shape)\n",
    "    #print(samples[3][1][0].shape)\n",
    "    #print(samples[3][2][0].shape)\n",
    "\n",
    "    out_channels = None\n",
    "\n",
    "    for index in range(2,3):\n",
    "\n",
    "        if index == 1:\n",
    "            plot_title = 'closest node index regression: '\n",
    "        else:\n",
    "            plot_title = 'shortest distance regression: '\n",
    "\n",
    "        y_train = samples[3][index]\n",
    "        y_val = samples[4][index]\n",
    "        y_test = samples[5][index]\n",
    "\n",
    "        #print(y_train[0].shape)\n",
    "        \n",
    "        # if criterion_type in ['ce','bce','multimargin']:\n",
    "        #     out_channels = 2\n",
    "        # elif criterion_type == 'mse-mse':\n",
    "        #     out_channels = y_train[0].shape[1]\n",
    "        # else:\n",
    "        #     out_channels = 1\n",
    "\n",
    "        if out_channels == None:\n",
    "            out_channels = y_train[0].shape[1]\n",
    "\n",
    "        if isinstance(model, str):\n",
    "            model_type = model\n",
    "            model,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model_type,criterion_type,optimizer_type,scheduler_type)\n",
    "        else:\n",
    "            model_type = model.name\n",
    "            _,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model, criterion_type,optimizer_type,scheduler_type)\n",
    "        print(model)\n",
    "\n",
    "        if gpu_bool:\n",
    "            model = model.to('cuda:1')\n",
    "            \n",
    "        if visualize:\n",
    "            model.eval()\n",
    "            if model_type == 'mlp':\n",
    "                out = model(x_test[0])\n",
    "            else:\n",
    "                out = model(x_test[0],edge_index_test[0])\n",
    "            visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        train_sen = []\n",
    "        train_spec = []\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        val_sen = []\n",
    "        val_spec = []\n",
    "\n",
    "        if early_stopping_patience != None:\n",
    "            best_val_loss = float('inf')\n",
    "            best_epoch = 0\n",
    "            no_improvement_count = 0\n",
    "\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            if model_type == 'mlp':\n",
    "                #print(y_train[0].shape)\n",
    "                t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val)\n",
    "            else:\n",
    "                #print(y_train[0].shape)\n",
    "                t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,optimizer_type,x_train,x_val,y_train,y_val,edge_index_train,edge_index_val,weights_train,weights_val)\n",
    "            train_loss.append(t_loss)\n",
    "            train_acc.append(t_acc)\n",
    "            train_sen.append(t_sen)\n",
    "            train_spec.append(t_spec)\n",
    "            val_loss.append(v_loss)\n",
    "            val_acc.append(v_acc)\n",
    "            val_sen.append(v_sen)\n",
    "            val_spec.append(v_spec)\n",
    "            #print(t_loss)\n",
    "            #print(t_acc_or_error)\n",
    "            if epoch % 10 == 0:\n",
    "                if criterion_type in ['mse','l2','mse-mse']:\n",
    "                    print(f'Epoch: {epoch:03d}, Training Loss (MSE): {t_loss:.4f}, Validation Loss (MSE): {v_loss:.4f}')\n",
    "                elif criterion_type == 'l1':\n",
    "                    print(f'Epoch: {epoch:03d}, Training Loss (MAE): {t_loss:.4f}, Validation Loss (MAE): {v_loss:.4f}')\n",
    "                else:\n",
    "                    print(f'Epoch: {epoch:03d}, Training Loss: {t_loss:.4f}, Training Accuracy: {t_acc:.4f}, Training Sensitivity: {t_sen:.4f}, Training Specificity: {t_spec:.4f}, Validation Loss: {v_loss:.4f}, Validation Accuracy: {v_acc:.4f}, Validation Sensitivity: {v_sen:.4f}, Validation Specificity: {v_spec:.4f}')\n",
    "\n",
    "            if early_stopping_patience != None:\n",
    "                if v_loss < best_val_loss:\n",
    "                    best_val_loss = v_loss\n",
    "                    best_epoch = epoch\n",
    "                    no_improvement_count = 0\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                if no_improvement_count >= early_stopping_patience:\n",
    "                    model.load_state_dict(torch.load('best_model.pth'))\n",
    "                    if gpu_bool:\n",
    "                        model = model.to('cuda:1')\n",
    "                    break\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test)\n",
    "        else:\n",
    "            test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test, edge_index_test, weights_test)\n",
    "        \n",
    "        if early_stopping_patience == None:\n",
    "            if criterion_type in ['mse','l2','mse-mse']:\n",
    "                print(f'Test Loss (MSE): {test_loss:03f}')\n",
    "            elif criterion_type == 'l1':\n",
    "                print(f'Test Loss (MAE): {test_loss:03f}')\n",
    "            else:\n",
    "                print(f'Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "        else:\n",
    "            if criterion_type in ['mse','l2','mse-mse']:\n",
    "                print(f'Best Epoch: {best_epoch:03d}, Test Loss (MSE): {test_loss:03f}')\n",
    "            elif criterion_type == 'l1':\n",
    "                print(f'Best Epoch: {best_epoch:03d}, Test Loss (MAE): {test_loss:03f}')\n",
    "            else:\n",
    "                print(f'Best Epoch: {best_epoch:03d}, Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "\n",
    "        x = range(1, len(train_loss)+1)\n",
    "        plt.plot(x, train_loss, color = '#1f77b4', label = 'training loss', alpha = 0.6)\n",
    "        plt.plot(x, val_loss, color = '#ff7f0e', label = 'validation loss', alpha = 0.6)\n",
    "        if criterion_type in ['ce','bce','bcelogits','multimargin']:\n",
    "            plt.plot(x, train_acc, color = '#2ca02c', label = 'training accuracy', alpha = 0.6)\n",
    "            plt.plot(x, val_acc, color = '#d62728', label = 'validation accuracy', alpha = 0.6)\n",
    "            plt.plot(x, train_sen, color = '#9467bd', label = 'training sensitivity', alpha = 0.6)\n",
    "            plt.plot(x, val_sen, color = '#8c564b', label = 'validation sensitivity', alpha = 0.6)\n",
    "            plt.plot(x, train_spec, color = '#e377c2', label = 'training specificity', alpha = 0.6)\n",
    "            plt.plot(x, val_spec, color = '#7f7f7f', label = 'validation specificity', alpha = 0.6)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(\"training results\")\n",
    "        plt.legend(loc = 'right',bbox_to_anchor=(1.45, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "        if visualize:\n",
    "            model.eval()\n",
    "            if model_type == 'mlp':\n",
    "                out = model(x_test[0])\n",
    "            else:\n",
    "                out = model(x_test[0],edge_index_test[0])\n",
    "            visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "        model = model.to('cpu')\n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), 'trained_model_'+str(model.in_channels)+'_'+str(model.first_hidden_channels)+'_'+str(model.out_channels)+'_'+str(model.n_hidden_layers)+'.pth')\n",
    "        \n",
    "        new_samples[3] = y_train\n",
    "        new_samples[4] = y_val\n",
    "        new_samples[5] = y_test\n",
    "        #print(y_train)\n",
    "        evaluate(plot_title,model,criterion_type,new_samples,adjacency_matrix_available = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.05)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.1)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.2)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.4)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,30,0.8)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,10,0.15)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,20,0.15)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,40,0.15)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,80,0.15)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,2,k,ErdosRenyiGraph,160,0.15)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shortestpath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
