{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import confusion_matrix, pairwise_distances\n",
    "from sklearn.manifold import TSNE\n",
    "from itertools import chain\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_matrix(G,weighted=False,directed=False): # convert a graph object into a matrix\n",
    "    matrix = np.zeros((len(G.nodes()),len(G.nodes())))\n",
    "    for edge in G.edges(data=True):\n",
    "        if weighted:\n",
    "            matrix[edge[0],edge[1]] = edge[2]['weight']\n",
    "        else:\n",
    "            matrix[edge[0],edge[1]] = 1\n",
    "        if not directed:\n",
    "            matrix[edge[1],edge[0]] = matrix[edge[0],edge[1]] \n",
    "    return matrix\n",
    "    \n",
    "def matrix_to_graph(matrix): # convert an adjacency matrix into a graph object\n",
    "    G = nx.Graph().to_directed()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        G.add_node(i)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i,j] != 0:\n",
    "                G.add_edge(i, j, weight=matrix[i,j])\n",
    "    return G\n",
    "\n",
    "def ErdosRenyiGraph(n,p):\n",
    "    return nx.erdos_renyi_graph(n, p).to_directed(),False,False\n",
    "\n",
    "def dRegularGraph(n,d):\n",
    "    return nx.random_regular_graph(d, n).to_directed(),False,False\n",
    "\n",
    "def randomAdjacencyMatrix(n,p,directed=False,weighted=False): #ErdosRenyi is unweighted & undirected\n",
    "    # matrix is in dimension nxn (i.e. n is the number of nodes)\n",
    "    # p is probability for having non-zero entries\n",
    "    # values of non-zero entries are sampled in Unif[0,1) if graph is weighted\n",
    "    matrix = (np.random.rand(n, n) < p).astype(int)\n",
    "    np.fill_diagonal(matrix, 0)\n",
    "    if weighted:\n",
    "        matrix = matrix * np.random.rand(n, n)\n",
    "    if not directed:\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                matrix[j,i] = matrix[i,j]\n",
    "    return matrix,directed,weighted\n",
    "\n",
    "def geometricAdjacencyMatrix(n,d,r,metric='euclidean'):\n",
    "    '''\n",
    "    Valid values for metric are:\n",
    "    From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]. These metrics support sparse matrix inputs. [‘nan_euclidean’] but it does not yet support sparse matrices.\n",
    "    From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.\n",
    "    '''\n",
    "    points = np.random.uniform(0, 1, size=(n, d))\n",
    "    distance_matrix = (pairwise_distances(points, metric=metric) < r).astype(int)\n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    return distance_matrix,False,False\n",
    "\n",
    "def ChungLuGraph(n,max_degree):\n",
    "    degrees = np.random.choice(range(1,max_degree+1), n, replace=True)\n",
    "    G = nx.Graph()\n",
    "    for node, degree in enumerate(degrees):\n",
    "        G.add_node(node, degree=degree)\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if u < v and np.random.rand() < G.nodes[u]['degree'] * G.nodes[v]['degree'] / (2 * sum(degrees)):\n",
    "                    G.add_edge(u, v)\n",
    "    return G.to_directed(),False,False\n",
    "\n",
    "def gridGraph(nrows,ncols): # not random\n",
    "    G = nx.Graph()\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            G.add_node(i*ncols+j)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            if i < nrows - 1:\n",
    "                G.add_edge(i*ncols+j, (i+1)*ncols+j)\n",
    "            if j < ncols - 1:\n",
    "                G.add_edge(i*ncols+j+1, i*ncols+j)\n",
    "    return G.to_directed(),False,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closestSeed_networkx(G,u,S,return_path=False):\n",
    "    closest_point = None\n",
    "    smallest_distance = float('inf')\n",
    "    shortest_path = []\n",
    "    for s in S:\n",
    "        if nx.has_path(G, u, s):\n",
    "            path = nx.shortest_path(G, source=u, target=s, weight=\"weight\")\n",
    "            distance = sum(G[path[i]][path[i + 1]][\"weight\"] for i in range(len(path) - 1))\n",
    "            if distance < smallest_distance:\n",
    "                closest_point, smallest_distance, shortest_path = s, distance, path  \n",
    "    if not return_path:\n",
    "        return closest_point, smallest_distance\n",
    "    else:\n",
    "        return closest_point, smallest_distance, shortest_path\n",
    "\n",
    "#### Das Sarma & Bourgain ####\n",
    "\n",
    "def closestSeedFromStartPoint(graph, start, target_set, return_path=False):\n",
    "    # Works for weighted & unweighted, directed & undirected graphs \n",
    "    # Doesn't allow self-directed nodes\n",
    "\n",
    "    closest_point, smallest_weight, shortest_path = None, float('inf'), []\n",
    "    target_set = [x for x in target_set if x != start]\n",
    "    if len(target_set) > 0:\n",
    "        visited = set()  # To keep track of visited vertices (vertices in paths stored in queue)\n",
    "        queue = deque([(start, 0, [])])  # Initialize the queue with the starting vertex, its path, and the total weight\n",
    "        storage = deque() # To store nodes removed from queue\n",
    "\n",
    "        while queue:\n",
    "            vertex, weight, path = queue.popleft()  # Dequeue a vertex, its path, and the total weight\n",
    "            storage.append((vertex, weight))\n",
    "            if vertex not in target_set:\n",
    "                for neighbor, edge_weight in enumerate(graph[vertex]):\n",
    "                    if neighbor not in path and neighbor != vertex and edge_weight != 0:\n",
    "                        new_weight = weight + edge_weight\n",
    "                        if neighbor not in visited:\n",
    "                            queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            visited.add(neighbor)\n",
    "                        else: # Replace the path from 'start' to 'neighbor' in queue or storage with a shorter one if found\n",
    "                            weight_info = [(index,item[1]) for index,item in enumerate(list(queue)) if item[0] == neighbor]\n",
    "                            if len(weight_info) != 0:\n",
    "                                for index, w in weight_info:\n",
    "                                    if new_weight < w:\n",
    "                                        del queue[index]\n",
    "                                        queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            else:\n",
    "                                weights = [item[1] for item in storage if item[0] == neighbor]\n",
    "                                if len(weights) > 0 and new_weight < np.min(weights):\n",
    "                                    queue.append((neighbor, new_weight, path + [vertex]))\n",
    "\n",
    "            elif vertex in target_set:\n",
    "                if closest_point == None:\n",
    "                    closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                else:\n",
    "                    if weight < smallest_weight:\n",
    "                        closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                k = len(queue)\n",
    "                for i in range(k):\n",
    "                    if smallest_weight <= queue[k-1-i][1]:\n",
    "                        queue.remove(queue[k-1-i]) # Remove all paths in queue which are longer than the current shortest from 'start' to a seed\n",
    "    \n",
    "    if return_path:\n",
    "        return closest_point, smallest_weight, shortest_path\n",
    "    else:\n",
    "        return closest_point, smallest_weight\n",
    "\n",
    "def closestSeedToEndPoint(graph, target, start_set, return_path=False): \n",
    "    # Works for weighted & unweighted, directed & undirected graphs \n",
    "    # Doesn't allow self-directed nodes\n",
    "\n",
    "    closest_point, smallest_weight, shortest_path = None, float('inf'), []\n",
    "    start_set = [x for x in start_set if x != target]\n",
    "    if len(start_set) > 0:\n",
    "        visited = set()  # To keep track of visited vertices (vertices in paths stored in queue)\n",
    "        queue = deque()  # Initialize the queue with the starting vertex, its path, and the total weight\n",
    "        for i in range(len(start_set)):\n",
    "            queue.append((start_set[i], 0, []))\n",
    "        storage = deque()\n",
    "\n",
    "        while queue:\n",
    "            vertex, weight, path = queue.popleft()  # Dequeue a vertex, its path, and the total weight\n",
    "            storage.append((vertex, weight))\n",
    "            if vertex != target:\n",
    "                for neighbor, edge_weight in enumerate(graph[vertex]):\n",
    "                    if neighbor not in path and neighbor not in start_set and neighbor != vertex and edge_weight != 0:\n",
    "                        new_weight = weight + edge_weight\n",
    "                        if neighbor not in visited:\n",
    "                            queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            visited.add(neighbor)\n",
    "                        else: # Replace the path from a start node to 'neighbor' in queue with a shorter one if found\n",
    "                            weight_info = [(index,item[1]) for index,item in enumerate(list(queue)) if item[0] == neighbor]\n",
    "                            if len(weight_info) != 0:\n",
    "                                for index, w in weight_info:\n",
    "                                    if new_weight < w:\n",
    "                                        del queue[index]\n",
    "                                        queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                            else:\n",
    "                                weights = [item[1] for item in storage if item[0] == neighbor]\n",
    "                                if len(weights) > 0 and new_weight < np.min(weights):\n",
    "                                    queue.append((neighbor, new_weight, path + [vertex]))\n",
    "                                \n",
    "            else:\n",
    "                if closest_point == None:\n",
    "                    closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                else:\n",
    "                    if weight < smallest_weight:\n",
    "                        closest_point, smallest_weight, shortest_path = vertex, weight, path + [vertex]\n",
    "                k = len(queue)\n",
    "                for i in range(k):\n",
    "                    if smallest_weight <= queue[k-1-i][1]:\n",
    "                        queue.remove(queue[k-1-i]) # Remove all paths in queue which are longer than the current shortest from 'start' to a seed\n",
    "\n",
    "    if return_path:\n",
    "        return closest_point, smallest_weight, shortest_path\n",
    "    else:\n",
    "        return closest_point, smallest_weight\n",
    "\n",
    "def offlineSample(G,u,node_to_sets=True):\n",
    "    support = [n for n in range(G.shape[0]) if np.count_nonzero(G[n]) >= 2 and n != u] # u is removed from the support for sampling seed sets\n",
    "    if len(support) == 0:\n",
    "        return set()\n",
    "    r = math.floor(np.log(len(support)))\n",
    "    sample_sets = [np.random.choice(support,size=2**i,replace=False) for i in range(r+1)]\n",
    "    if node_to_sets:\n",
    "        closest_points = set([closestSeedFromStartPoint(G,u,S) for S in sample_sets])\n",
    "    else:\n",
    "        closest_points = set([closestSeedToEndPoint(G,u,S) for S in sample_sets])\n",
    "    if (None,float('inf')) in closest_points:\n",
    "        closest_points.remove((None,float('inf')))\n",
    "    return closest_points,set(np.concatenate(sample_sets))\n",
    "\n",
    "def offlineSketch(G,u,k,node_to_sets=True):\n",
    "    closest_points,sample_sets = offlineSample(G,u,node_to_sets)\n",
    "    for i in range(k):\n",
    "        closest_points_new,sample_sets_new = offlineSample(G,u,node_to_sets)\n",
    "        closest_points = closest_points.union(closest_points_new)\n",
    "        sample_sets = sample_sets.union(sample_sets_new)\n",
    "    return np.array(list(closest_points)),np.array(list(sample_sets))\n",
    "\n",
    "def onlineShortestPath_Sarma(G,u,v,k,directed=False): ## upper bound\n",
    "    if not directed:\n",
    "        sketch_u,_ = offlineSketch(G,u,k)\n",
    "        sketch_v,_ = offlineSketch(G,v,k)\n",
    "    else:\n",
    "        sketch_u,_ = offlineSketch(G,u,k)\n",
    "    if sketch_u.shape[0] != 0 and sketch_v.shape[0] != 0:\n",
    "        common_nodes = [w for w in sketch_u[:,0] if w in sketch_v[:,0]]\n",
    "        while None in common_nodes:\n",
    "            common_nodes.remove(None)\n",
    "        min_dist = float('inf')\n",
    "        for w in common_nodes:\n",
    "            dist = sketch_u[sketch_u[:, 0] == w][0,1] + sketch_v[sketch_v[:, 0] == w][0,1]\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        return min_dist\n",
    "    else:\n",
    "        return float('inf')\n",
    "\n",
    "def onlineShortestPath_Bourgain(G,u,v,directed=False): ## lower bound\n",
    "    support = [n for n in range(G.shape[0]) if np.count_nonzero(G[n]) >= 2 and n != u] # u is removed from the support for sampling seed sets\n",
    "    r = math.floor(np.log(len(support)))\n",
    "    sample_sets = [np.random.choice(support,size=2**i,replace=False) for i in range(r+1)]\n",
    "    if directed:\n",
    "        d_u_S = [closestSeedFromStartPoint(G,u,S)[1] for S in sample_sets]\n",
    "        d_v_S = [closestSeedFromStartPoint(G,v,S)[1] for S in sample_sets]\n",
    "        d_S_u = [closestSeedToEndPoint(G,u,S)[1] for S in sample_sets]\n",
    "        d_S_v = [closestSeedToEndPoint(G,v,S)[1] for S in sample_sets]\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_u_S,d_v_S))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_u_S = np.array([value for index, value in enumerate(d_u_S) if index not in to_remove])\n",
    "        d_v_S = np.array([value for index, value in enumerate(d_v_S) if index not in to_remove])\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_S_u,d_S_v))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_S_u = np.array([value for index, value in enumerate(d_S_u) if index not in to_remove])\n",
    "        d_S_v = np.array([value for index, value in enumerate(d_S_v) if index not in to_remove])\n",
    "        return max([0,np.max(d_S_v-d_S_u),np.max(d_u_S-d_v_S)])\n",
    "    else:\n",
    "        d_u_S = [closestSeedFromStartPoint(G,u,S)[1] for S in sample_sets]\n",
    "        d_v_S = [closestSeedFromStartPoint(G,v,S)[1] for S in sample_sets]\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_u_S,d_v_S))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_u_S = np.array([value for index, value in enumerate(d_u_S) if index not in to_remove])\n",
    "        d_v_S = np.array([value for index, value in enumerate(d_v_S) if index not in to_remove])\n",
    "        return np.max(np.abs(d_u_S-d_v_S))\n",
    "\n",
    "def shortestDistance_allNodes_Sarma(G,u,k,directed=False):\n",
    "    distances = np.zeros(G.shape[0])\n",
    "    for v in range(G.shape[0]):\n",
    "        if u != v:\n",
    "            distances[v] = onlineShortestPath_Sarma(G,u,v,k,directed)\n",
    "    return distances\n",
    "\n",
    "def shortestDistance_allNodes_Bourgain(G,u,directed=False):\n",
    "    distances = np.zeros(G.shape[0])\n",
    "    for v in range(G.shape[0]):\n",
    "        if u != v:\n",
    "            distances[v] = onlineShortestPath_Bourgain(G,u,v,directed)\n",
    "    return distances\n",
    "\n",
    "def shortestDistance_allNodes_networkx(G,u):\n",
    "    if isinstance(G, np.ndarray):\n",
    "        G = matrix_to_graph(G)\n",
    "    n_nodes = len(G.nodes())\n",
    "    distances = np.zeros(n_nodes)\n",
    "    for v in range(n_nodes):\n",
    "        if u != v:\n",
    "            if nx.has_path(G, u, v):\n",
    "                distances[v] = nx.shortest_path_length(G, u, v)\n",
    "            else:\n",
    "                distances[v] = float('inf')       \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels1, hidden_channels2, hidden_channels3, hidden_channels4, hidden_channels5, out_channels, sigmoid = False, reLU = False):\n",
    "        super(MLP, self).__init__()\n",
    "        #torch.manual_seed(12345)\n",
    "        self.name = 'mlp'\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.width = hidden_channels1\n",
    "        self.n_hidden_layers = 5\n",
    "        self.lin1 = Linear(in_channels, hidden_channels1)\n",
    "        self.lin2 = Linear(hidden_channels1, hidden_channels2)\n",
    "        self.lin3 = Linear(hidden_channels2, hidden_channels3)\n",
    "        self.lin4 = Linear(hidden_channels3, hidden_channels4)\n",
    "        self.lin5 = Linear(hidden_channels4, hidden_channels5)\n",
    "        self.lin6 = Linear(hidden_channels5, out_channels)\n",
    "        if sigmoid:\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        else:\n",
    "            self.sigmoid = None\n",
    "        if reLU:\n",
    "            self.reLU = torch.nn.ReLU()\n",
    "        else:\n",
    "            self.reLU = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin4(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin5(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin6(x)\n",
    "        if self.sigmoid != None:\n",
    "            x = self.sigmoid(x)\n",
    "        if self.reLU != None:\n",
    "            x = self.reLU(x)\n",
    "        return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels1, hidden_channels2, hidden_channels3, hidden_channels4, hidden_channels5, out_channels, sigmoid = False, reLU = False):\n",
    "        super(GCN, self).__init__()\n",
    "        #torch.manual_seed(1234567)\n",
    "        self.name = 'gcn'\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.width = hidden_channels1\n",
    "        self.n_hidden_layers = 5\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels1)\n",
    "        self.conv2 = GCNConv(hidden_channels1, hidden_channels2)\n",
    "        self.conv3 = GCNConv(hidden_channels2, hidden_channels3)\n",
    "        self.conv4 = GCNConv(hidden_channels3, hidden_channels4)\n",
    "        self.conv5 = GCNConv(hidden_channels4, hidden_channels5)\n",
    "        self.conv6 = GCNConv(hidden_channels5, out_channels)\n",
    "        if sigmoid:\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        else:\n",
    "            self.sigmoid = None\n",
    "        if reLU:\n",
    "            self.reLU = torch.nn.ReLU()\n",
    "        else:\n",
    "            self.reLU = None\n",
    "\n",
    "    def forward(self, x, edge_index, weights=None):\n",
    "        if weights == None:\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv3(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv4(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv5(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv6(x, edge_index)\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv2(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv3(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv4(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv5(x, edge_index, edge_weight=weights)\n",
    "            x = x.relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv6(x, edge_index, edge_weight=weights)\n",
    "        if self.sigmoid != None:\n",
    "            x = self.sigmoid(x)\n",
    "        if self.reLU != None:\n",
    "            x = self.reLU(x)\n",
    "        return x\n",
    "\n",
    "def build(in_channels,out_channels,model,criterion_type,optimizer_type,scheduler_type = None):\n",
    "    \n",
    "    if isinstance(model, str):\n",
    "        hidden_channels1 = 256\n",
    "        hidden_channels2 = 128\n",
    "        hidden_channels3 = 64\n",
    "        hidden_channels4 = 32\n",
    "        hidden_channels5 = 8\n",
    "        if model == 'mlp':\n",
    "            if criterion_type in ['ce','bce','multimargin']:\n",
    "                model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,sigmoid = True)\n",
    "            elif criterion_type in ['mse','l2','l1']: \n",
    "                model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,reLU = True)\n",
    "            else:\n",
    "                model = MLP(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels)\n",
    "        elif model == 'gcn':\n",
    "            if criterion_type in ['ce','bce','multimargin']:\n",
    "                model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,sigmoid = True)\n",
    "            elif criterion_type in ['mse','l2','l1']: \n",
    "                model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels,reLU = True)\n",
    "            else:\n",
    "                model = GCN(in_channels,hidden_channels1,hidden_channels2,hidden_channels3,hidden_channels4,hidden_channels5,out_channels)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Model type not yet defined.\"\n",
    "            )\n",
    "    \n",
    "    if criterion_type == 'ce':\n",
    "        criterion = [torch.nn.CrossEntropyLoss()]\n",
    "    elif criterion_type == 'bce':\n",
    "        criterion = [torch.nn.BCELoss()]\n",
    "    elif criterion_type == 'bcelogits':\n",
    "        criterion = [torch.nn.BCEWithLogitsLoss()]\n",
    "    elif criterion_type in ['mse','l2']:\n",
    "        criterion = [torch.nn.MSELoss()]\n",
    "    elif criterion_type == 'l1':\n",
    "        criterion = [torch.nn.L1Loss()]\n",
    "    elif criterion_type == 'multimargin': # cuda crashed (similar to focal loss)\n",
    "        criterion = [torch.nn.MultiMarginLoss()]\n",
    "    elif criterion_type == 'mse-mse':\n",
    "        criterion1 = torch.nn.MSELoss()\n",
    "        criterion2 = torch.nn.MSELoss()\n",
    "        criterion = [criterion1,criterion2]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Criterion type not yet defined.\"\n",
    "        )\n",
    "    \n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, weight_decay=0.0001, alpha=0.99, eps=1e-8, momentum=0.9)\n",
    "    elif optimizer_type == 'adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=0.0001, lr_decay=0.0001)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Optimizer type not yet defined.\"\n",
    "        )\n",
    "    \n",
    "    if scheduler_type == None:\n",
    "        scheduler = None\n",
    "    else:\n",
    "        if scheduler_type == 'step':\n",
    "            scheduler = [torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)]\n",
    "        elif scheduler_type == 'reduce_on_plateau':\n",
    "            scheduler = [torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=False)]\n",
    "        elif scheduler_type == 'exponential':\n",
    "            scheduler = [torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)]\n",
    "        elif scheduler_type == 'cosine':\n",
    "            scheduler = [torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)]\n",
    "        elif scheduler_type == 'cyclic':\n",
    "            if optimizer_type in ['sgd','rmsprop']:\n",
    "                scheduler = [torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, cycle_momentum=True)]\n",
    "            else:\n",
    "                scheduler = [torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, cycle_momentum=False)]\n",
    "        elif scheduler_type == 'cyclic-cosine':\n",
    "            cycle_epochs = 5\n",
    "            if optimizer_type in ['sgd','rmsprop']:\n",
    "                scheduler_cyclic = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=cycle_epochs, cycle_momentum=True)\n",
    "            else:\n",
    "                scheduler_cyclic = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=cycle_epochs, cycle_momentum=False)\n",
    "            scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cycle_epochs * 2)\n",
    "            scheduler = [scheduler_cyclic,scheduler_cosine]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Scheduler type not yet defined.\"\n",
    "            )\n",
    "    \n",
    "    return model,criterion,optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(gpu_bool,model,criterion_type,samples_x,samples_edge_index=None,samples_weights=None):\n",
    "    y_pred = []\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if model.name == 'mlp':\n",
    "            for i in range(len(samples_x)):\n",
    "                if gpu_bool:\n",
    "                    x = samples_x[i].to('cuda:1')\n",
    "                else:\n",
    "                    x = samples_x[i]\n",
    "                out = model(x)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.append(pred.cpu())\n",
    "        elif samples_weights == None:\n",
    "            for i in range(len(samples_x)):\n",
    "                if gpu_bool:\n",
    "                    x = samples_x[i].to('cuda:1')\n",
    "                    edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                else:\n",
    "                    x = samples_x[i]\n",
    "                    edge_index = samples_edge_index[i]\n",
    "                out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.append(pred.cpu())\n",
    "        else:\n",
    "            for i in range(len(samples_x)):\n",
    "                if gpu_bool:\n",
    "                    x = samples_x[i].to('cuda:1')\n",
    "                    edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                    weights = samples_weights[i].to('cuda:1')\n",
    "                else:\n",
    "                    x = samples_x[i]\n",
    "                    edge_index = samples_edge_index[i]\n",
    "                    weights = samples_weights[i]\n",
    "                out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.append(pred.cpu())\n",
    "    model = model.to('cpu')\n",
    "    return np.array(y_pred)\n",
    "\n",
    "def predict_allBatches(model,criterion_type,samples):\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "    y_pred_train = predict(gpu_bool, model, criterion_type, samples[0], samples[6], samples[9])\n",
    "    y_pred_val = predict(gpu_bool, model, criterion_type, samples[1], samples[7], samples[10])\n",
    "    y_pred_test = predict(gpu_bool, model, criterion_type, samples[2], samples[8], samples[11])\n",
    "    return y_pred_train,y_pred_val,y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples_closestPoints_inner(n_samples,max_n_samples_per_graph,function,*args,**kwargs):\n",
    "    samples_x = []\n",
    "    samples_y1 = []\n",
    "    samples_y2 = []\n",
    "    samples_y3 = []\n",
    "    samples_y4 = []\n",
    "    samples_y5 = []\n",
    "    samples_y6 = []\n",
    "    samples_edge_index = []\n",
    "    samples_weights = None\n",
    "    not_disconnected_nodes = []\n",
    "    object,directed,weighted = function(*args,**kwargs)\n",
    "    if weighted:\n",
    "        samples_weights = []\n",
    "    if isinstance(object, np.ndarray):\n",
    "        matrix_type = object\n",
    "        object = matrix_to_graph(object)\n",
    "    else:\n",
    "        matrix_type = graph_to_matrix(object)\n",
    "    n = len(object.nodes())\n",
    "    i = 0\n",
    "    while i<n_samples:\n",
    "        #print(i)\n",
    "        components = list(nx.strongly_connected_components(object))\n",
    "        len_components = [len(c) for c in components]\n",
    "        to_remove = [c.pop() for c in components if len(c) < max(len_components)]\n",
    "        if len(to_remove) < len(object.nodes()):\n",
    "            matrix_type = np.delete(matrix_type, to_remove, axis=0)\n",
    "            matrix_type = np.delete(matrix_type, to_remove, axis=1)\n",
    "            object.remove_nodes_from(to_remove)\n",
    "            remaining = object.nodes()\n",
    "            object = nx.relabel_nodes(object, {node: index for index, node in enumerate(object.nodes())})\n",
    "            if weighted:\n",
    "                weights = list(nx.get_edge_attributes(object,'weight').values())\n",
    "                M = n*max(weights)*10\n",
    "            else:\n",
    "                M = n*10\n",
    "            n_new = len(object.nodes())\n",
    "            j = 0\n",
    "            #print('a')\n",
    "            while i<n_samples and j<max_n_samples_per_graph:\n",
    "                sample_sets = np.random.choice(object.nodes(),size=min(np.floor(np.sqrt(n)),n_new),replace=False)\n",
    "                \n",
    "                x_final = np.zeros((n_new,1))\n",
    "                x_final[u,0] = 1\n",
    "                y1_final = shortestDistance_allNodes_networkx(object,u)\n",
    "                y1_final = np.where(y1_final == float('inf'), M, y1_final)\n",
    "                if matrix_type.shape[0] > 50:\n",
    "                    y2_final = []\n",
    "                    y3_final = []\n",
    "                    y4_final = []\n",
    "                    y5_final = []\n",
    "                    y6_final = []\n",
    "                else:\n",
    "                    try:\n",
    "                        y2_final = shortestDistance_allNodes_Bourgain(matrix_type,u)\n",
    "                        y2_final = np.where(y2_final == float('inf'), M, y2_final)\n",
    "                    except:\n",
    "                        y2_final = []\n",
    "                    try:\n",
    "                        y3_final = shortestDistance_allNodes_Sarma(matrix_type,u,1)\n",
    "                        y3_final = np.where(y3_final == float('inf'), M, y3_final)\n",
    "                    except:\n",
    "                        y3_final = []\n",
    "                    try:\n",
    "                        y4_final = shortestDistance_allNodes_Sarma(matrix_type,u,2)\n",
    "                        y4_final = np.where(y4_final == float('inf'), M, y4_final)\n",
    "                    except:\n",
    "                        y4_final = []\n",
    "                    try:\n",
    "                        y5_final = shortestDistance_allNodes_Sarma(matrix_type,u,3)\n",
    "                        y5_final = np.where(y5_final == float('inf'), M, y5_final)\n",
    "                    except:\n",
    "                        y5_final = []\n",
    "                    try:\n",
    "                        y6_final = shortestDistance_allNodes_Sarma(matrix_type,u,4)\n",
    "                        y6_final = np.where(y6_final == float('inf'), M, y6_final)\n",
    "                    except:\n",
    "                        y6_final = []\n",
    "                samples_x.append(torch.tensor(x_final.astype(np.float32), requires_grad=True))\n",
    "                samples_y1.append(torch.tensor(y1_final).to(torch.float32))\n",
    "                samples_y2.append(y2_final)\n",
    "                samples_y3.append(y3_final)\n",
    "                samples_y4.append(y4_final)\n",
    "                samples_y5.append(y5_final)\n",
    "                samples_y6.append(y6_final)\n",
    "                samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                if weighted:\n",
    "                    samples_weights.append(torch.tensor(weights).to(torch.float32))\n",
    "                not_disconnected_nodes.append(remaining)\n",
    "                i += 1\n",
    "                j += 1\n",
    "        else:\n",
    "            object,directed,weighted = function(*args,**kwargs)\n",
    "            if isinstance(object, np.ndarray):\n",
    "                matrix_type = object\n",
    "                object = matrix_to_graph(object)\n",
    "            else:\n",
    "                matrix_type = graph_to_matrix(object)\n",
    "    return samples_x, [samples_y1, samples_y2, samples_y3, samples_y4, samples_y5, samples_y6], samples_edge_index, samples_weights,not_disconnected_nodes\n",
    "\n",
    "def generateSamples_closestPoints(n_train,n_val,n_test,max_n_samples_per_graph,function,*args,**kwargs):\n",
    "    x_train, y_train, edge_index_train, weights_train,not_disconnected_train = generateSamples_closestPoints_inner(n_train,max_n_samples_per_graph,function,*args,**kwargs)\n",
    "    x_val, y_val, edge_index_val, weights_val,not_disconnected_val = generateSamples_closestPoints_inner(n_val,max_n_samples_per_graph,function,*args,**kwargs)\n",
    "    x_test, y_test, edge_index_test, weights_test,not_disconnected_test = generateSamples_closestPoints_inner(n_test,max_n_samples_per_graph,function,*args,**kwargs)\n",
    "    return [x_train, x_val, x_test, y_train, y_val, y_test, edge_index_train,edge_index_val,edge_index_test,weights_train,weights_val,weights_test,not_disconnected_train,not_disconnected_val,not_disconnected_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inner(plot_title,title,y,y_pred,true_distances,x_samples = None,classification=True): ## target is nx1\n",
    "\n",
    "    if classification:\n",
    "\n",
    "        graph_size = true_distances.shape[1]\n",
    "\n",
    "        ## Calculate the proportion of points selected from the sample.\n",
    "        ratio1 = np.mean(y_pred,axis=1)\n",
    "\n",
    "        ## Check if any disconnected node is in actual sketch.\n",
    "        true_distances[true_distances == np.inf] = -1 ## convert inf into a large number for the sake of calculations\n",
    "        M = np.max(true_distances) * 10\n",
    "        true_distances[true_distances == -1] = M\n",
    "        y_actual = torch.stack(y).detach().numpy()\n",
    "        dump_actual = y_actual*true_distances\n",
    "        n_dump_actual_equal_M = np.sum((dump_actual == M).astype(int),axis=1)\n",
    "        ratio2 = n_dump_actual_equal_M/np.sum(y_actual,axis=1) ## nan means empty sketch\n",
    "        #ratio2[np.isnan(ratio2)] = -1\n",
    "\n",
    "        ## Check if any disconnected node is chosen.\n",
    "        dump = y_pred*true_distances\n",
    "        n_true_distances_equal_M = np.sum((true_distances == M).astype(int),axis=1)\n",
    "        n_dump_equal_M = np.sum((dump == M).astype(int),axis=1)\n",
    "        n_selected = np.sum(y_pred,axis=1)\n",
    "        n_selected[n_selected == 0] = 0.0001\n",
    "        ratio3 = n_dump_equal_M/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio3[np.isnan(ratio3)] = -1\n",
    "\n",
    "        if x_samples != None:\n",
    "\n",
    "            ## Seed node and neighbors -> 1. Others -> 0\n",
    "            seed_or_neighbors = np.zeros_like(true_distances)\n",
    "            _,seeds = np.where(true_distances == 0) ## number of seeds = number of samples because we selected one seed from each sample\n",
    "            seed_disconnected = np.zeros(seed_or_neighbors.shape[0])\n",
    "            for i in range(len(seeds)):\n",
    "                seed = seeds[i]\n",
    "                seed_or_neighbors[i] = x_samples[i].detach().numpy()[seed] ## x_samples[i] has to be adjacency matrix\n",
    "                seed_disconnected[i] = sum((seed_or_neighbors[i]>0).astype(int))>0\n",
    "                seed_or_neighbors[i,seed] = 1\n",
    "            seed_or_neighbors_selected = seed_or_neighbors*y_pred\n",
    "\n",
    "            ## Is seed node not disconnected from the remaining?\n",
    "            seed_disconnected = seed_disconnected.astype(int)\n",
    "\n",
    "            ## Have seed node and its neighbors been chosen?\n",
    "            ratio4 = np.sum(seed_or_neighbors_selected,axis=1)/np.sum(seed_or_neighbors,axis=1)\n",
    "\n",
    "            ## How many of the chosen nodes are seed node and its neighbors?\n",
    "            ratio5 = np.sum(seed_or_neighbors_selected,axis=1)/n_selected ## nan means no nodes have been selected, ratio1 = 0\n",
    "            #ratio5[np.isnan(ratio5)] = -1\n",
    "\n",
    "        ## Check if the chosen nodes are closer than random selection.\n",
    "        sum_distance_selected = np.sum(dump,axis=1)\n",
    "        term0 = np.mean(true_distances,axis=1)\n",
    "        term0[term0 == 0] = 0.0001\n",
    "        ratio6 = sum_distance_selected/n_selected/term0 ## nan means no nodes have been selected, ratio1 = 0\n",
    "        #ratio6[np.isnan(ratio6)] = -1\n",
    "\n",
    "        ## Same as ratio6 but excludes disconnected nodes with inf distance to the seed node.\n",
    "        n_selected[n_selected == 0.0001] = 0\n",
    "        term1 = n_selected-n_dump_equal_M\n",
    "        term1[term1 == 0] = 0.0001\n",
    "        term2 = np.sum(true_distances,axis=1)-n_true_distances_equal_M*M\n",
    "        term2[term2 == 0] = 0.0001\n",
    "        ratio7 = (sum_distance_selected-n_dump_equal_M*M)/term1/term2*(graph_size-n_true_distances_equal_M)\n",
    "        #ratio7[np.isnan(ratio7)] = -1\n",
    "        ## nan means no nodes have been selected (ratio1 = 0) or selected nodes are all disconnected from seed node (inf distance)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(ratio1)), ratio1, color = '#1f77b4', label = 'ratio 1: size of selected community to size of sample ratio',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio6, color = '#8c564b', label = 'ratio 2: mean distance of chosen nodes to seed node divided by mean distance of all nodes to seed node',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio7, color = '#e377c2', label = 'ratio 3: same as ratio 6 but excludes disconnected nodes',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio2, color = '#ff7f0e', label = 'ratio 4: proportion of disconnected nodes in actual sketch',alpha = 0.6)\n",
    "        plt.plot(range(len(ratio1)), ratio3, color = '#2ca02c', label = 'ratio 5: proportion of disconnected nodes in the chosen community',alpha = 0.6)\n",
    "        if x_samples != None:\n",
    "            plt.plot(range(len(ratio1)), ratio4, color = '#d62728', label = 'ratio 6: proportion of seed node and its neighbors that are selected',alpha = 0.6)\n",
    "            plt.plot(range(len(ratio1)), ratio5, color = '#9467bd', label = 'ratio 7: proportion of selected nodes that are seed node or its neighbors',alpha = 0.6)\n",
    "            plt.scatter(range(len(ratio1)), seed_disconnected, color = 'k', marker='x', label = 'is seed node not disconnected from the remaining nodes in graph? 1 = True, 0 = False',alpha = 0.6)\n",
    "        plt.xlabel(\"sample index\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(title)\n",
    "        plt.legend(loc = 'lower center',bbox_to_anchor=(0.48, -0.6))\n",
    "        plt.show()\n",
    "\n",
    "        true_distances[true_distances == M] = np.inf\n",
    "        if x_samples == None:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3)]\n",
    "        else:\n",
    "            return [np.mean(ratio1),np.mean(ratio6),np.mean(ratio7),np.mean(ratio2),np.mean(ratio3),np.mean(ratio4),np.mean(ratio5)]\n",
    "        \n",
    "    else:\n",
    "        y_actual = np.array(y[0])\n",
    "        n_nodes = y_actual.shape[0]*y_actual.shape[1]\n",
    "        diff_pred = (y_actual-y_pred).reshape(n_nodes)\n",
    "        values, base = np.histogram(diff_pred, bins=100)\n",
    "        cumulative = np.cumsum(values)\n",
    "        plt.plot(base[:-1], cumulative, label='GNN', alpha = 0.75)\n",
    "        lengths = [len(x) for x in y[1]]\n",
    "        if 0 not in lengths:\n",
    "            y_Bourgain = np.array(y[1])\n",
    "            diff_Bourgain = (y_actual-y_Bourgain).reshape(n_nodes)\n",
    "            values, base = np.histogram(diff_Bourgain, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Bourgain', alpha = 0.75)\n",
    "        lengths = [len(x) for x in y[2]]\n",
    "        if 0 not in lengths:\n",
    "            y_Sarma1 = np.array(y[2])\n",
    "            diff_Sarma1 = (y_actual-y_Sarma1).reshape(n_nodes)\n",
    "            values, base = np.histogram(diff_Sarma1, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Sarma, k=1', alpha = 0.75)\n",
    "        lengths = [len(x) for x in y[3]]\n",
    "        if 0 not in lengths:\n",
    "            y_Sarma2 = np.array(y[3])\n",
    "            diff_Sarma2 = (y_actual-y_Sarma2).reshape(n_nodes)\n",
    "            values, base = np.histogram(diff_Sarma2, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Sarma, k=2', alpha = 0.75)\n",
    "        lengths = [len(x) for x in y[4]]\n",
    "        if 0 not in lengths:\n",
    "            y_Sarma3 = np.array(y[4])\n",
    "            diff_Sarma3 = (y_actual-y_Sarma3).reshape(n_nodes)\n",
    "            values, base = np.histogram(diff_Sarma3, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Sarma, k=3', alpha = 0.75)\n",
    "        lengths = [len(x) for x in y[5]]\n",
    "        if 0 not in lengths:\n",
    "            y_Sarma4 = np.array(y[5])\n",
    "            diff_Sarma4 = (y_actual-y_Sarma4).reshape(n_nodes)\n",
    "            values, base = np.histogram(diff_Sarma4, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Sarma, k=4', alpha = 0.75)\n",
    "        plt.xlabel(\"y_actual - y_pred\")\n",
    "        plt.ylabel(\"cummulative frequency\")\n",
    "        plt.title(plot_title+title)\n",
    "        plt.legend()\n",
    "\n",
    "        if plot_title == 'shortest distance regression: ':\n",
    "            filenames = os.listdir(dir)\n",
    "            if '0.png' not in filenames:\n",
    "                plt.savefig(dir+'/0.png')\n",
    "            else:\n",
    "                filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "                plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "def evaluate(plot_title,model,criterion_type,samples,adjacency_matrix_available = False):\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        classification = False\n",
    "    else:\n",
    "        classification = True\n",
    "    y_pred_train,y_pred_val,y_pred_test = predict_allBatches(model,criterion_type,samples)\n",
    "    if adjacency_matrix_available:\n",
    "        evaluate_inner(plot_title,'training data',samples[3],y_pred_train,samples[12],samples[0],classification)\n",
    "        evaluate_inner(plot_title,'validation data',samples[4],y_pred_val,samples[13],samples[1],classification)\n",
    "        evaluate_inner(plot_title,'test data',samples[5],y_pred_test,samples[14],samples[2],classification)\n",
    "    else:\n",
    "        evaluate_inner(plot_title,'training data',samples[3],y_pred_train,samples[12],classification=classification)\n",
    "        evaluate_inner(plot_title,'validation data',samples[4],y_pred_val,samples[13],classification=classification)\n",
    "        evaluate_inner(plot_title,'test data',samples[5],y_pred_test,samples[14],classification=classification)\n",
    "    #return np.array(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gpu_bool,model,criterion,criterion_type,samples_x,samples_y,samples_edge_index = None,samples_weights = None):\n",
    "    t_loss = 0\n",
    "    total_samples = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if samples_edge_index == None:\n",
    "            for x,y in list(zip(samples_x,samples_y)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                out = model(x)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        elif samples_weights == None:\n",
    "            for x,y,edge_index in list(zip(samples_x,samples_y,samples_edge_index)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "        else:\n",
    "            for x,y,edge_index,weights in list(zip(samples_x,samples_y,samples_edge_index,samples_weights)):\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                    weights = weights.to('cuda:1')\n",
    "                out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                y_true.extend(y)\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss += criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss += criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss += criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "                total_samples += 1\n",
    "                if criterion_type in ['bce','ce','multimargin']:\n",
    "                    pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                    pred = out.squeeze()\n",
    "                else:\n",
    "                    pred = torch.round(out.squeeze())\n",
    "                y_pred.extend(pred)\n",
    "\n",
    "    t_loss = t_loss/total_samples\n",
    "    t_loss = t_loss.cpu()\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        return t_loss, None, None, None\n",
    "    else:\n",
    "        y_true = np.array([y.cpu() for y in y_true])\n",
    "        y_pred = np.array([y.cpu() for y in y_pred])\n",
    "        #print(np.array(y_true).shape)\n",
    "        #print(np.array(y_pred).shape)\n",
    "        t_accuracy = sum(y_true == y_pred)/len(y_true)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        #print(cm.ravel())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        t_sensitivity = tp / (tp + fn)\n",
    "        t_specificity = tn / (tn + fp)\n",
    "        return t_loss, t_accuracy, t_sensitivity, t_specificity\n",
    "\n",
    "def train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val,edge_index_train=None,edge_index_val=None,weights_train=None,weights_val=None):\n",
    "    model.train()\n",
    "    if edge_index_train == None:\n",
    "        for x,y in list(zip(x_train,y_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "            out = model(x)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    elif weights_train == None:\n",
    "        for x,y,edge_index in list(zip(x_train,y_train,edge_index_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "            out = model(x,edge_index)  # Perform a single forward pass\n",
    "            #print(out.shape)\n",
    "            #print(y.shape)\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "    else:\n",
    "        for x,y,edge_index,weights in list(zip(x_train,y_train,edge_index_train,weights_train)):\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            if gpu_bool:\n",
    "                x = x.to('cuda:1')\n",
    "                y = y.to('cuda:1')\n",
    "                edge_index = edge_index.to('cuda:1')\n",
    "                weights = weights.to('cuda:1')\n",
    "            out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "            if criterion_type in ['bce']:\n",
    "                t_loss = criterion[0](out, torch.stack((1-y, y)).T.to(torch.float32))\n",
    "            elif criterion_type == 'mse-mse':\n",
    "                t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "            elif criterion_type in ['ce','multimargin']:\n",
    "                t_loss = criterion[0](out, y) ## classification\n",
    "            else:\n",
    "                t_loss = criterion[0](out.squeeze(), y.to(torch.float32))\n",
    "            t_loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    train_loss, train_accuracy, train_sensitivity, train_specificity = test(gpu_bool,model,criterion,criterion_type,x_train,y_train,edge_index_train,weights_train)\n",
    "    v_loss, v_accuracy, v_sensitivity, v_specificity = test(gpu_bool,model,criterion,criterion_type,x_val,y_val,edge_index_val,weights_val)\n",
    "\n",
    "    if scheduler_type in ['step','exponential','cyclic','cosine']:\n",
    "        scheduler[0].step()\n",
    "    elif scheduler_type == 'reduce_on_plateau': \n",
    "        scheduler[0].step(v_loss)\n",
    "    elif scheduler_type == 'cyclic-cosine':\n",
    "        scheduler[0].step()\n",
    "        scheduler[1].step()\n",
    "    return train_loss,train_accuracy,train_sensitivity,train_specificity,v_loss,v_accuracy,v_sensitivity,v_specificity\n",
    "\n",
    "def run(samples,model,criterion_type,optimizer_type,scheduler_type,num_epochs=100,early_stopping_patience=None,visualize = False,save_model = False):\n",
    "\n",
    "    plot_title = 'shortest distance regression: '\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "\n",
    "    x_train = samples[0]\n",
    "    x_val = samples[1]\n",
    "    x_test = samples[2]\n",
    "    y_train = samples[3][0]\n",
    "    y_val = samples[4][0]\n",
    "    y_test = samples[5][0]\n",
    "    edge_index_train = samples[6]\n",
    "    edge_index_val = samples[7]\n",
    "    edge_index_test = samples[8]\n",
    "    weights_train = samples[9]\n",
    "    weights_val = samples[10]\n",
    "    weights_test = samples[11]\n",
    "    \n",
    "    out_channels = 1\n",
    "    if isinstance(model, str):\n",
    "        model_type = model\n",
    "        model,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model_type,criterion_type,optimizer_type,scheduler_type)\n",
    "    else:\n",
    "        model_type = model.name\n",
    "        _,criterion,optimizer,scheduler = build(x_train[0].shape[1], out_channels, model, criterion_type,optimizer_type,scheduler_type)\n",
    "    print(model)\n",
    "\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "        \n",
    "    if visualize:\n",
    "        model.eval()\n",
    "        if model_type == 'mlp':\n",
    "            out = model(x_test[0])\n",
    "        else:\n",
    "            out = model(x_test[0],edge_index_test[0])\n",
    "        visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_sen = []\n",
    "    train_spec = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_sen = []\n",
    "    val_spec = []\n",
    "\n",
    "    if early_stopping_patience != None:\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if model_type == 'mlp':\n",
    "            #print(y_train[0].shape)\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val)\n",
    "        else:\n",
    "            #print(y_train[0].shape)\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,optimizer_type,x_train,x_val,y_train,y_val,edge_index_train,edge_index_val,weights_train,weights_val)\n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(t_acc)\n",
    "        train_sen.append(t_sen)\n",
    "        train_spec.append(t_spec)\n",
    "        val_loss.append(v_loss)\n",
    "        val_acc.append(v_acc)\n",
    "        val_sen.append(v_sen)\n",
    "        val_spec.append(v_spec)\n",
    "        #print(t_loss)\n",
    "        #print(t_acc_or_error)\n",
    "        if epoch % 10 == 0:\n",
    "            if criterion_type in ['mse','l2','mse-mse']:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MSE): {t_loss:.4f}, Validation Loss (MSE): {v_loss:.4f}')\n",
    "            elif criterion_type == 'l1':\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MAE): {t_loss:.4f}, Validation Loss (MAE): {v_loss:.4f}')\n",
    "            else:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss: {t_loss:.4f}, Training Accuracy: {t_acc:.4f}, Training Sensitivity: {t_sen:.4f}, Training Specificity: {t_spec:.4f}, Validation Loss: {v_loss:.4f}, Validation Accuracy: {v_acc:.4f}, Validation Sensitivity: {v_sen:.4f}, Validation Specificity: {v_spec:.4f}')\n",
    "\n",
    "        if early_stopping_patience != None:\n",
    "            if v_loss < best_val_loss:\n",
    "                best_val_loss = v_loss\n",
    "                best_epoch = epoch\n",
    "                no_improvement_count = 0\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            if no_improvement_count >= early_stopping_patience:\n",
    "                model.load_state_dict(torch.load('best_model.pth'))\n",
    "                if gpu_bool:\n",
    "                    model = model.to('cuda:1')\n",
    "                break\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test)\n",
    "    else:\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test, edge_index_test, weights_test)\n",
    "    \n",
    "    if early_stopping_patience == None:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "    else:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "\n",
    "    x = range(1, len(train_loss)+1)\n",
    "    plt.plot(x, train_loss, color = '#1f77b4', label = 'training loss', alpha = 0.75)\n",
    "    plt.plot(x, val_loss, color = '#ff7f0e', label = 'validation loss', alpha = 0.75)\n",
    "    if criterion_type in ['ce','bce','bcelogits','multimargin']:\n",
    "        plt.plot(x, train_acc, color = '#2ca02c', label = 'training accuracy', alpha = 0.75)\n",
    "        plt.plot(x, val_acc, color = '#d62728', label = 'validation accuracy', alpha = 0.75)\n",
    "        plt.plot(x, train_sen, color = '#9467bd', label = 'training sensitivity', alpha = 0.75)\n",
    "        plt.plot(x, val_sen, color = '#8c564b', label = 'validation sensitivity', alpha = 0.75)\n",
    "        plt.plot(x, train_spec, color = '#e377c2', label = 'training specificity', alpha = 0.75)\n",
    "        plt.plot(x, val_spec, color = '#7f7f7f', label = 'validation specificity', alpha = 0.75)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"training results\")\n",
    "    plt.legend(loc = 'right',bbox_to_anchor=(1.45, 0.5))\n",
    "\n",
    "    filenames = os.listdir(dir)\n",
    "    if '0.png' not in filenames:\n",
    "        plt.savefig(dir+'/0.png')\n",
    "    else:\n",
    "        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if visualize:\n",
    "        model.eval()\n",
    "        if model_type == 'mlp':\n",
    "            out = model(x_test[0])\n",
    "        else:\n",
    "            out = model(x_test[0],edge_index_test[0])\n",
    "        visualizeTNSE(out, color=y_test[0])\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), 'trained_model_'+str(model.in_channels)+'_'+str(model.first_hidden_channels)+'_'+str(model.out_channels)+'_'+str(model.n_hidden_layers)+'.pth')\n",
    "    \n",
    "    evaluate(plot_title,model,criterion_type,samples,adjacency_matrix_available = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'dump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,10,0.5)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,30,0.5)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,90,0.5)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,270,0.5)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,810,0.5)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,10,0.25)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,30,0.25)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,90,0.25)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,270,0.25)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,810,0.25)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,2430,0.25)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,10,0.125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,30,0.125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,90,0.125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,270,0.125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,810,0.125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,2430,0.125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,10,0.0625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,30,0.0625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,90,0.0625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,270,0.0625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,810,0.0625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,2430,0.0625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,10,0.03125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,30,0.03125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,90,0.03125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,270,0.03125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,810,0.03125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,2430,0.03125)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,10,0.015625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,30,0.015625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,90,0.015625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,270,0.015625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,810,0.015625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,2430,0.015625)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generateSamples_closestPoints(250,50,50,1,ErdosRenyiGraph,2430,0.5)\n",
    "run(samples,'gcn','mse','adam',None,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shortestpath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
