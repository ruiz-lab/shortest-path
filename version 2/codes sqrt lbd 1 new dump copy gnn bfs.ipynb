{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "import os\n",
    "from Graphs import matrix_to_graph, graph_to_matrix, ErdosRenyiGraph, getLambda\n",
    "from ShortestDistanceAlgorithms import shortestDistance_allNodes_networkx, shortestDistance_allNodes_Bourgain, shortestDistance_allNodes_Sarma\n",
    "from Models import build\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateERSamples_inner(num_graphs,n,lbd):\n",
    "\n",
    "    #largest_component_sizes = []\n",
    "    #lbds = []\n",
    "\n",
    "    samples_x = []\n",
    "    samples_y_actual = []\n",
    "    samples_y_Bourgain = []\n",
    "    samples_y_Sarma1 = []\n",
    "    samples_y_Sarma2 = []\n",
    "    samples_y_Sarma3 = []\n",
    "    #samples_y_Sarma4 = []\n",
    "    samples_edge_index = []\n",
    "\n",
    "    #max_lbd = getLambda(n)\n",
    "    k = 0\n",
    "    n_rejected1 = 0\n",
    "    n_rejected2 = 0\n",
    "    while k < num_graphs:\n",
    "        try:\n",
    "            object,directed,weighted = ErdosRenyiGraph(n,lbd/n)\n",
    "            components = list(nx.strongly_connected_components(object))\n",
    "            largest_component = max(components, key=len)\n",
    "            n_nodes = len(largest_component)\n",
    "            r = int(np.floor(np.sqrt(n)))\n",
    "            if n_nodes >= max(r,10):\n",
    "                #largest_component_sizes.append(n_nodes)\n",
    "                #lbds.append(lbd)\n",
    "                object = object.subgraph(largest_component)\n",
    "                object = nx.relabel_nodes(object, {node: index for index, node in enumerate(object.nodes())})\n",
    "                matrix = graph_to_matrix(object)\n",
    "                seeds = np.random.choice(range(n_nodes),size=r,replace=False)\n",
    "                x = np.zeros((n_nodes,r))\n",
    "                y_actual = np.zeros((n_nodes,r))\n",
    "                y_Bourgain = np.zeros((n_nodes,r))\n",
    "                y_Sarma1 = np.zeros((n_nodes,r))\n",
    "                y_Sarma2 = np.zeros((n_nodes,r))\n",
    "                y_Sarma3 = np.zeros((n_nodes,r))\n",
    "                #y_Sarma4 = np.zeros((n_nodes,r))\n",
    "                M = n_nodes\n",
    "                for i in range(r):\n",
    "                    u = seeds[i]\n",
    "                    #support = [n for n in range(matrix.shape[0]) if np.count_nonzero(matrix[n]) >= 2 and n != u] # u is removed from the support for sampling seed sets\n",
    "                    #print(support)\n",
    "                    x[u,i] = 1\n",
    "                    y_actual[:,i] = shortestDistance_allNodes_networkx(object,u)\n",
    "                    if n <= 200:\n",
    "                        y_Bourgain[:,i] = shortestDistance_allNodes_Bourgain(matrix,u)\n",
    "                        y_Sarma1[:,i] = shortestDistance_allNodes_Sarma(matrix,u,1)\n",
    "                        y_Sarma2[:,i] = shortestDistance_allNodes_Sarma(matrix,u,2)\n",
    "                        y_Sarma3[:,i] = shortestDistance_allNodes_Sarma(matrix,u,3)\n",
    "                    #y_Sarma4[:,i] = shortestDistance_allNodes_Bourgain(matrix,u,4)\n",
    "                    #y_Sarma4 = np.where(y_Sarma4 == float('inf'), M, y_Sarma4)\n",
    "                if n <= 200:\n",
    "                    y_Bourgain = np.where(y_Bourgain == float('inf'), M, y_Bourgain)\n",
    "                    y_Sarma1 = np.where(y_Sarma1 == float('inf'), M, y_Sarma1)\n",
    "                    y_Sarma2 = np.where(y_Sarma2 == float('inf'), M, y_Sarma2)\n",
    "                    y_Sarma3 = np.where(y_Sarma3 == float('inf'), M, y_Sarma3)\n",
    "                samples_x.append(torch.tensor(x.astype(np.float32), requires_grad=True))\n",
    "                samples_y_actual.append(torch.tensor(y_actual).to(torch.float32))\n",
    "                samples_y_Bourgain.append(y_Bourgain)\n",
    "                samples_y_Sarma1.append(y_Sarma1)\n",
    "                samples_y_Sarma2.append(y_Sarma2)\n",
    "                samples_y_Sarma3.append(y_Sarma3)\n",
    "                samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                k += 1\n",
    "            else:\n",
    "                n_rejected2 += 1\n",
    "        except:\n",
    "            n_rejected1 += 1\n",
    "    \n",
    "    print('Number of graphs rejected because Bourgain\\'s and Sarma\\'s algorithms yield errors: ',n_rejected1)\n",
    "    print('Number of graphs rejected because the largest component has insufficient size: ',n_rejected2)\n",
    "\n",
    "    return samples_x, [samples_y_actual, samples_y_Bourgain, samples_y_Sarma1, samples_y_Sarma2, samples_y_Sarma3], samples_edge_index, None ## None for sample weights\n",
    "\n",
    "def generateERSamples(n_train,n_val,n_test,n,lbd):\n",
    "    print('Generating training data...')\n",
    "    train = generateERSamples_inner(n_train,n,lbd)\n",
    "    print('Generating validation data...')\n",
    "    val = generateERSamples_inner(n_val,n,lbd)\n",
    "    print('Generating test data...')\n",
    "    test = generateERSamples_inner(n_test,n,lbd)\n",
    "    return [train, val, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(gpu_bool,model,criterion_type,samples_x,samples_edge_index=None,samples_weights=None):\n",
    "    y_pred = []\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if model.out_channels == 1:\n",
    "            if model.name == 'mlp':\n",
    "                for i in range(len(samples_x)):\n",
    "                    if gpu_bool:\n",
    "                        x = samples_x[i].to('cuda:1')\n",
    "                    else:\n",
    "                        x = samples_x[i]\n",
    "                    pred_all = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        out = model(x[:,j].reshape(len(x[:,j]),1))  # Perform a single forward pass.\n",
    "                        if criterion_type in ['bce','ce','multimargin']:\n",
    "                            pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                        elif criterion_type in ['mse','l2','l1']:\n",
    "                            pred = out.squeeze()\n",
    "                        else:\n",
    "                            pred = torch.round(out.squeeze())\n",
    "                        pred_all.append(pred.cpu())\n",
    "                    y_pred.append(np.array(pred_all).T)\n",
    "            elif samples_weights == None:\n",
    "                for i in range(len(samples_x)):\n",
    "                    if gpu_bool:\n",
    "                        x = samples_x[i].to('cuda:1')\n",
    "                        edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                    else:\n",
    "                        x = samples_x[i]\n",
    "                        edge_index = samples_edge_index[i]\n",
    "                    pred_all = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        out = model(x[:,j].reshape(len(x[:,j]),1),edge_index)  # Perform a single forward pass.\n",
    "                        if criterion_type in ['bce','ce','multimargin']:\n",
    "                            pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                        elif criterion_type in ['mse','l2','l1']:\n",
    "                            pred = out.squeeze()\n",
    "                        else:\n",
    "                            pred = torch.round(out.squeeze())\n",
    "                        pred_all.append(pred.cpu())\n",
    "                    y_pred.append(np.array(pred_all).T)\n",
    "            else:\n",
    "                for i in range(len(samples_x)):\n",
    "                    if gpu_bool:\n",
    "                        x = samples_x[i].to('cuda:1')\n",
    "                        edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                        weights = samples_weights[i].to('cuda:1')\n",
    "                    else:\n",
    "                        x = samples_x[i]\n",
    "                        edge_index = samples_edge_index[i]\n",
    "                        weights = samples_weights[i]\n",
    "                    pred_all = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        out = model(x[:,j].reshape(len(x[:,j]),1),edge_index,weights)  # Perform a single forward pass.\n",
    "                        if criterion_type in ['bce','ce','multimargin']:\n",
    "                            pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                        elif criterion_type in ['mse','l2','l1']:\n",
    "                            pred = out.squeeze()\n",
    "                        else:\n",
    "                            pred = torch.round(out.squeeze())\n",
    "                        pred_all.append(pred.cpu())\n",
    "                    y_pred.append(np.array(pred_all).T)\n",
    "        else:\n",
    "            if model.name == 'mlp':\n",
    "                for i in range(len(samples_x)):\n",
    "                    if gpu_bool:\n",
    "                        x = samples_x[i].to('cuda:1')\n",
    "                    else:\n",
    "                        x = samples_x[i]\n",
    "                    out = model(x)  # Perform a single forward pass.\n",
    "                    if criterion_type in ['bce','ce','multimargin']:\n",
    "                        pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                    elif criterion_type in ['mse','l2','l1']:\n",
    "                        pred = out.squeeze()\n",
    "                    else:\n",
    "                        pred = torch.round(out.squeeze())\n",
    "                    y_pred.append(pred.cpu())\n",
    "            elif samples_weights == None:\n",
    "                for i in range(len(samples_x)):\n",
    "                    if gpu_bool:\n",
    "                        x = samples_x[i].to('cuda:1')\n",
    "                        edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                    else:\n",
    "                        x = samples_x[i]\n",
    "                        edge_index = samples_edge_index[i]\n",
    "                    out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                    if criterion_type in ['bce','ce','multimargin']:\n",
    "                        pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                    elif criterion_type in ['mse','l2','l1']:\n",
    "                        pred = out.squeeze()\n",
    "                    else:\n",
    "                        pred = torch.round(out.squeeze())\n",
    "                    y_pred.append(pred.cpu())\n",
    "            else:\n",
    "                for i in range(len(samples_x)):\n",
    "                    if gpu_bool:\n",
    "                        x = samples_x[i].to('cuda:1')\n",
    "                        edge_index = samples_edge_index[i].to('cuda:1')\n",
    "                        weights = samples_weights[i].to('cuda:1')\n",
    "                    else:\n",
    "                        x = samples_x[i]\n",
    "                        edge_index = samples_edge_index[i]\n",
    "                        weights = samples_weights[i]\n",
    "                    out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                    if criterion_type in ['bce','ce','multimargin']:\n",
    "                        pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                    elif criterion_type in ['mse','l2','l1']:\n",
    "                        pred = out.squeeze()\n",
    "                    else:\n",
    "                        pred = torch.round(out.squeeze())\n",
    "                    y_pred.append(pred.cpu())\n",
    "    model = model.to('cpu')\n",
    "    return y_pred\n",
    "\n",
    "def predict_allBatches(model,criterion_type,samples):\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "    y_pred_train = predict(gpu_bool, model, criterion_type, samples[0][0], samples[0][2], samples[0][3])\n",
    "    y_pred_val = predict(gpu_bool, model, criterion_type, samples[1][0], samples[1][2], samples[1][3])\n",
    "    y_pred_test = predict(gpu_bool, model, criterion_type, samples[2][0], samples[2][2], samples[2][3])\n",
    "    return y_pred_train,y_pred_val,y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gpu_bool,model,criterion,criterion_type,samples_x,samples_y,samples_edge_index = None,samples_weights = None):\n",
    "    t_loss = 0\n",
    "    total_samples = 0\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if model.out_channels == 1:\n",
    "            if samples_edge_index == None:\n",
    "                for x,y in list(zip(samples_x,samples_y)):\n",
    "                    if gpu_bool:\n",
    "                        x = x.to('cuda:1')\n",
    "                        y = y.to('cuda:1')\n",
    "                    pred_all = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        out = model(x[:,j].reshape(len(x[:,j]),1))  # Perform a single forward pass.\n",
    "                        t_loss += criterion[0](out.squeeze(), y[:,j])\n",
    "                        total_samples += 1\n",
    "                        if criterion_type in ['bce','ce','multimargin']:\n",
    "                            pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                        elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                            pred = out.squeeze()\n",
    "                        else:\n",
    "                            pred = torch.round(out.squeeze())\n",
    "                        pred_all.append(pred.cpu())\n",
    "                    y_pred.append(np.array(pred_all).T)\n",
    "            elif samples_weights == None:\n",
    "                for x,y,edge_index in list(zip(samples_x,samples_y,samples_edge_index)):\n",
    "                    if gpu_bool:\n",
    "                        x = x.to('cuda:1')\n",
    "                        y = y.to('cuda:1')\n",
    "                        edge_index = edge_index.to('cuda:1')\n",
    "                    pred_all = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        out = model(x[:,j].reshape(len(x[:,j]),1),edge_index)  # Perform a single forward pass.\n",
    "                        t_loss += criterion[0](out.squeeze(), y[:,j])\n",
    "                        total_samples += 1\n",
    "                        if criterion_type in ['bce','ce','multimargin']:\n",
    "                            pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                        elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                            pred = out.squeeze()\n",
    "                        else:\n",
    "                            pred = torch.round(out.squeeze())\n",
    "                        pred_all.append(pred.cpu())\n",
    "                    y_pred.append(np.array(pred_all).T)\n",
    "            else:\n",
    "                for x,y,edge_index,weights in list(zip(samples_x,samples_y,samples_edge_index,samples_weights)):\n",
    "                    if gpu_bool:\n",
    "                        x = x.to('cuda:1')\n",
    "                        y = y.to('cuda:1')\n",
    "                        edge_index = edge_index.to('cuda:1')\n",
    "                        weights = weights.to('cuda:1')\n",
    "                    pred_all = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        out = model(x[:,j].reshape(len(x[:,j]),1),edge_index,weights)  # Perform a single forward pass.\n",
    "                        t_loss += criterion[0](out.squeeze(), y[:,j])\n",
    "                        total_samples += 1\n",
    "                        if criterion_type in ['bce','ce','multimargin']:\n",
    "                            pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                        elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                            pred = out.squeeze()\n",
    "                        else:\n",
    "                            pred = torch.round(out.squeeze())\n",
    "                        pred_all.append(pred.cpu())\n",
    "                    y_pred.append(np.array(pred_all).T)\n",
    "        else:\n",
    "            if samples_edge_index == None:\n",
    "                for x,y in list(zip(samples_x,samples_y)):\n",
    "                    if gpu_bool:\n",
    "                        x = x.to('cuda:1')\n",
    "                        y = y.to('cuda:1')\n",
    "                    out = model(x)  # Perform a single forward pass.\n",
    "                    if criterion_type in ['bce']:\n",
    "                        t_loss += criterion[0](out, torch.stack((1-y, y)).T)\n",
    "                    elif criterion_type == 'mse-mse':\n",
    "                        t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                    elif criterion_type in ['ce','multimargin']:\n",
    "                        t_loss += criterion[0](out, y) ## classification\n",
    "                    else:\n",
    "                        t_loss += criterion[0](out.squeeze(), y)\n",
    "                    total_samples += 1\n",
    "                    if criterion_type in ['bce','ce','multimargin']:\n",
    "                        pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                    elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                        pred = out.squeeze()\n",
    "                    else:\n",
    "                        pred = torch.round(out.squeeze())\n",
    "                    y_pred.append(pred.cpu())\n",
    "            elif samples_weights == None:\n",
    "                for x,y,edge_index in list(zip(samples_x,samples_y,samples_edge_index)):\n",
    "                    if gpu_bool:\n",
    "                        x = x.to('cuda:1')\n",
    "                        y = y.to('cuda:1')\n",
    "                        edge_index = edge_index.to('cuda:1')\n",
    "                    out = model(x,edge_index)  # Perform a single forward pass.\n",
    "                    if criterion_type in ['bce']:\n",
    "                        t_loss += criterion[0](out, torch.stack((1-y, y)).T)\n",
    "                    elif criterion_type == 'mse-mse':\n",
    "                        t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                    elif criterion_type in ['ce','multimargin']:\n",
    "                        t_loss += criterion[0](out, y) ## classification\n",
    "                    else:\n",
    "                        t_loss += criterion[0](out.squeeze(), y)\n",
    "                    total_samples += 1\n",
    "                    if criterion_type in ['bce','ce','multimargin']:\n",
    "                        pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                    elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                        pred = out.squeeze()\n",
    "                    else:\n",
    "                        pred = torch.round(out.squeeze())\n",
    "                    y_pred.append(pred.cpu())\n",
    "            else:\n",
    "                for x,y,edge_index,weights in list(zip(samples_x,samples_y,samples_edge_index,samples_weights)):\n",
    "                    if gpu_bool:\n",
    "                        x = x.to('cuda:1')\n",
    "                        y = y.to('cuda:1')\n",
    "                        edge_index = edge_index.to('cuda:1')\n",
    "                        weights = weights.to('cuda:1')\n",
    "                    out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                    if criterion_type in ['bce']:\n",
    "                        t_loss += criterion[0](out, torch.stack((1-y, y)).T)\n",
    "                    elif criterion_type == 'mse-mse':\n",
    "                        t_loss += 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                    elif criterion_type in ['ce','multimargin']:\n",
    "                        t_loss += criterion[0](out, y) ## classification\n",
    "                    else:\n",
    "                        t_loss += criterion[0](out.squeeze(), y)\n",
    "                    total_samples += 1\n",
    "                    if criterion_type in ['bce','ce','multimargin']:\n",
    "                        pred = out.argmax(dim=1) #  Use the class with highest probability.\n",
    "                    elif criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "                        pred = out.squeeze()\n",
    "                    else:\n",
    "                        pred = torch.round(out.squeeze())\n",
    "                    y_pred.append(pred.cpu())\n",
    "\n",
    "    t_loss = t_loss.cpu()/total_samples\n",
    "    if criterion_type in ['mse','l2','l1','mse-mse']:\n",
    "        return t_loss, None, None, None\n",
    "    else:\n",
    "        y_true = np.array([y.cpu() for y in samples_y])\n",
    "        y_pred = np.array(y_pred)\n",
    "        t_accuracy = sum(y_true == y_pred)/len(y_true)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        t_sensitivity = tp / (tp + fn)\n",
    "        t_specificity = tn / (tn + fp)\n",
    "        return t_loss, t_accuracy, t_sensitivity, t_specificity\n",
    "\n",
    "def train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val,edge_index_train=None,edge_index_val=None,weights_train=None,weights_val=None):\n",
    "    model.train()\n",
    "    if model.out_channels == 1:\n",
    "        if edge_index_train == None:\n",
    "            for x,y in list(zip(x_train,y_train)):\n",
    "                optimizer.zero_grad()  # Clear gradients.\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                for j in range(x.shape[1]):\n",
    "                    out = model(x[:,j].reshape(len(x[:,j]),1))  # Perform a single forward pass.\n",
    "                    t_loss = criterion[0](out.squeeze(), y[:,j])\n",
    "                    t_loss.backward()  # Derive gradients\n",
    "                    optimizer.step()  # Update parameters based on gradients.\n",
    "        elif weights_train == None:\n",
    "            for x,y,edge_index in list(zip(x_train,y_train,edge_index_train)):\n",
    "                optimizer.zero_grad()  # Clear gradients.\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                for j in range(x.shape[1]):\n",
    "                    out = model(x[:,j].reshape(len(x[:,j]),1),edge_index)  # Perform a single forward pass\n",
    "                    t_loss = criterion[0](out.squeeze(), y[:,j])\n",
    "                    t_loss.backward()  # Derive gradients\n",
    "                    optimizer.step()  # Update parameters based on gradients.\n",
    "        else:\n",
    "            for x,y,edge_index,weights in list(zip(x_train,y_train,edge_index_train,weights_train)):\n",
    "                optimizer.zero_grad()  # Clear gradients.\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                    weights = weights.to('cuda:1')\n",
    "                for j in range(x.shape[1]):\n",
    "                    out = model(x[:,j].reshape(len(x[:,j]),1),edge_index,weights)  # Perform a single forward pass.\n",
    "                    t_loss = criterion[0](out.squeeze(), y[:,j])\n",
    "                    t_loss.backward()  # Derive gradients\n",
    "                    optimizer.step()  # Update parameters based on gradients.\n",
    "    else:\n",
    "        if edge_index_train == None:\n",
    "            for x,y in list(zip(x_train,y_train)):\n",
    "                optimizer.zero_grad()  # Clear gradients.\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                out = model(x)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss = criterion[0](out, torch.stack((1-y, y)).T)\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss = criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss = criterion[0](out.squeeze(), y)\n",
    "                t_loss.backward()  # Derive gradients\n",
    "                optimizer.step()  # Update parameters based on gradients.\n",
    "        elif weights_train == None:\n",
    "            for x,y,edge_index in list(zip(x_train,y_train,edge_index_train)):\n",
    "                optimizer.zero_grad()  # Clear gradients.\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                out = model(x,edge_index)  # Perform a single forward pass\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss = criterion[0](out, torch.stack((1-y, y)).T)\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss = criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss = criterion[0](out.squeeze(), y)\n",
    "                t_loss.backward()  # Derive gradients\n",
    "                optimizer.step()  # Update parameters based on gradients.\n",
    "        else:\n",
    "            for x,y,edge_index,weights in list(zip(x_train,y_train,edge_index_train,weights_train)):\n",
    "                optimizer.zero_grad()  # Clear gradients.\n",
    "                if gpu_bool:\n",
    "                    x = x.to('cuda:1')\n",
    "                    y = y.to('cuda:1')\n",
    "                    edge_index = edge_index.to('cuda:1')\n",
    "                    weights = weights.to('cuda:1')\n",
    "                out = model(x,edge_index,weights)  # Perform a single forward pass.\n",
    "                if criterion_type in ['bce']:\n",
    "                    t_loss = criterion[0](out, torch.stack((1-y, y)).T)\n",
    "                elif criterion_type == 'mse-mse':\n",
    "                    t_loss = 100*criterion[0](out[:, ::2], y[:, ::2]) + criterion[1](out[:, 1::2], y[:, 1::2])\n",
    "                elif criterion_type in ['ce','multimargin']:\n",
    "                    t_loss = criterion[0](out, y) ## classification\n",
    "                else:\n",
    "                    t_loss = criterion[0](out.squeeze(), y)\n",
    "                t_loss.backward()  # Derive gradients\n",
    "                optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    train_loss, train_accuracy, train_sensitivity, train_specificity = test(gpu_bool,model,criterion,criterion_type,x_train,y_train,edge_index_train,weights_train)\n",
    "    v_loss, v_accuracy, v_sensitivity, v_specificity = test(gpu_bool,model,criterion,criterion_type,x_val,y_val,edge_index_val,weights_val)\n",
    "\n",
    "    if scheduler_type in ['step','exponential','cyclic','cosine']:\n",
    "        scheduler[0].step()\n",
    "    elif scheduler_type == 'reduce_on_plateau': \n",
    "        scheduler[0].step(v_loss)\n",
    "    elif scheduler_type == 'cyclic-cosine':\n",
    "        scheduler[0].step()\n",
    "        scheduler[1].step()\n",
    "    return train_loss,train_accuracy,train_sensitivity,train_specificity,v_loss,v_accuracy,v_sensitivity,v_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inner(n,data_name,y,y_pred1,y_pred2):\n",
    "\n",
    "    y_pred1 = np.array(list(chain(*[np.ravel(arr) for arr in y_pred1])))\n",
    "    y_pred2 = np.array(list(chain(*[np.ravel(arr) for arr in y_pred2])))\n",
    "    y_actual = np.array(list(chain(*[np.ravel(arr) for arr in y[0]])))\n",
    "    diff_pred1 = y_actual - y_pred1\n",
    "    diff_pred2 = y_actual - y_pred2\n",
    "\n",
    "    values, base = np.histogram(diff_pred1, bins=100)\n",
    "    cumulative = np.cumsum(values)\n",
    "    plt.plot(base[:-1], cumulative, label='GNN, out_channels = floor(sqrt(n))', alpha = 0.75)\n",
    "    \n",
    "    values, base = np.histogram(diff_pred2, bins=100)\n",
    "    cumulative = np.cumsum(values)\n",
    "    plt.plot(base[:-1], cumulative, label='GNN, out_channels = 1', alpha = 0.75)\n",
    "\n",
    "    if n <= 200:\n",
    "\n",
    "        y_Bourgain = np.array(list(chain(*[np.ravel(arr) for arr in y[1]])))\n",
    "        y_Sarma1 = np.array(list(chain(*[np.ravel(arr) for arr in y[2]])))\n",
    "        y_Sarma2 = np.array(list(chain(*[np.ravel(arr) for arr in y[3]])))\n",
    "        y_Sarma3 = np.array(list(chain(*[np.ravel(arr) for arr in y[4]])))\n",
    "        diff_Bourgain = y_actual-y_Bourgain\n",
    "        diff_Sarma1 = y_actual-y_Sarma1\n",
    "        diff_Sarma2 = y_actual-y_Sarma2\n",
    "        diff_Sarma3 = y_actual-y_Sarma3\n",
    "\n",
    "        values, base = np.histogram(diff_Bourgain, bins=100)\n",
    "        cumulative = np.cumsum(values)\n",
    "        plt.plot(base[:-1], cumulative, label='Bourgain', alpha = 0.75)\n",
    "\n",
    "        values, base = np.histogram(diff_Sarma1, bins=100)\n",
    "        cumulative = np.cumsum(values)\n",
    "        plt.plot(base[:-1], cumulative, label='Sarma, k = 1', alpha = 0.75)\n",
    "\n",
    "        values, base = np.histogram(diff_Sarma2, bins=100)\n",
    "        cumulative = np.cumsum(values)\n",
    "        plt.plot(base[:-1], cumulative, label='Sarma, k = 2', alpha = 0.75)\n",
    "\n",
    "        values, base = np.histogram(diff_Sarma3, bins=100)\n",
    "        cumulative = np.cumsum(values)\n",
    "        plt.plot(base[:-1], cumulative, label='Sarma, k = 3', alpha = 0.75)\n",
    "\n",
    "    plt.xlabel(\"Actual Distance - Predicted Distance\")\n",
    "    plt.ylabel(\"Cummulative Frequency\")\n",
    "    plt.title(data_name+\", n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "    plt.legend()\n",
    "    filenames = os.listdir(dir)\n",
    "    if '0.png' not in filenames:\n",
    "        plt.savefig(dir+'/0.png')\n",
    "    else:\n",
    "        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(y_actual, y_pred1, label='GNN, out_channels = floor(sqrt(n))', alpha = 0.1)\n",
    "    plt.scatter(y_actual, y_pred2, label='GNN, out_channels = 1', alpha = 0.1)\n",
    "    plt.xlabel(\"Actual Distance\")\n",
    "    plt.ylabel('Predicted Distance')\n",
    "    plt.title(data_name+\", n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "    plt.legend()\n",
    "    filenames = os.listdir(dir)\n",
    "    if '0.png' not in filenames:\n",
    "        plt.savefig(dir+'/0.png')\n",
    "    else:\n",
    "        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(y_actual, edgecolor='white', alpha=0.4)\n",
    "    #sns.kdeplot(y_actual, fill=True, alpha=0.2)\n",
    "    plt.xlabel(\"Actual Distance\")\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(data_name+\", n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "    filenames = os.listdir(dir)\n",
    "    if '0.png' not in filenames:\n",
    "        plt.savefig(dir+'/0.png')\n",
    "    else:\n",
    "        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "    plt.show()\n",
    "\n",
    "    return diff_pred1\n",
    "    \n",
    "def evaluate(n,model1,model2,criterion_type,samples):\n",
    "    y_pred_train1,y_pred_val1,y_pred_test1 = predict_allBatches(model1,criterion_type,samples)\n",
    "    y_pred_train2,y_pred_val2,y_pred_test2 = predict_allBatches(model2,criterion_type,samples) \n",
    "    diff_pred_train = evaluate_inner(n,'Training Data',samples[0][1],y_pred_train1,y_pred_train2)\n",
    "    diff_pred_val = evaluate_inner(n,'Validation Data',samples[1][1],y_pred_val1,y_pred_val2)\n",
    "    diff_pred_test = evaluate_inner(n,'Test Data',samples[2][1],y_pred_test1,y_pred_test2)\n",
    "\n",
    "    sns.kdeplot(diff_pred_train, fill=True, label=\"Training Data\", alpha=0.2)\n",
    "    sns.kdeplot(diff_pred_val, fill=True, label=\"Validation Data\", alpha=0.2)\n",
    "    sns.kdeplot(diff_pred_test, fill=True, label=\"Test Data\", alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"Actual Distance - Predicted Distance\")\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(\"n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "    plt.legend()\n",
    "    filenames = os.listdir(dir)\n",
    "    if '0.png' not in filenames:\n",
    "        plt.savefig(dir+'/0.png')\n",
    "    else:\n",
    "        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(samples,model,criterion_type,optimizer_type,scheduler_type,num_epochs=100,early_stopping_patience=None,save_model = False):\n",
    "\n",
    "    title = 'out_channels = floor(sqrt(n))'\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "\n",
    "    x_train = samples[0][0]\n",
    "    x_val = samples[1][0]\n",
    "    x_test = samples[2][0]\n",
    "    y_train = samples[0][1][0]\n",
    "    y_val = samples[1][1][0]\n",
    "    y_test = samples[2][1][0]\n",
    "    edge_index_train = samples[0][2]\n",
    "    edge_index_val = samples[1][2]\n",
    "    edge_index_test = samples[2][2]\n",
    "    weights_train = samples[0][3]\n",
    "    weights_val = samples[1][3]\n",
    "    weights_test = samples[2][3]\n",
    "\n",
    "    in_channels = x_train[0].shape[1]\n",
    "    out_channels = y_train[0].shape[1]\n",
    "    if isinstance(model, str):\n",
    "        model_type = model\n",
    "        model,criterion,optimizer,scheduler = build(in_channels, out_channels, model_type,criterion_type,optimizer_type,scheduler_type)\n",
    "    else:\n",
    "        model_type = model.name\n",
    "        _,criterion,optimizer,scheduler = build(in_channels, out_channels, model, criterion_type,optimizer_type,scheduler_type)\n",
    "    print(model)\n",
    "\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_sen = []\n",
    "    train_spec = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_sen = []\n",
    "    val_spec = []\n",
    "\n",
    "    if early_stopping_patience != None:\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if model_type == 'mlp':\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val)\n",
    "        else:\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,optimizer_type,x_train,x_val,y_train,y_val,edge_index_train,edge_index_val,weights_train,weights_val)\n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(t_acc)\n",
    "        train_sen.append(t_sen)\n",
    "        train_spec.append(t_spec)\n",
    "        val_loss.append(v_loss)\n",
    "        val_acc.append(v_acc)\n",
    "        val_sen.append(v_sen)\n",
    "        val_spec.append(v_spec)\n",
    "        if epoch % 10 == 0:\n",
    "            if criterion_type in ['mse','l2','mse-mse']:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MSE): {t_loss:.4f}, Validation Loss (MSE): {v_loss:.4f}')\n",
    "            elif criterion_type == 'l1':\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MAE): {t_loss:.4f}, Validation Loss (MAE): {v_loss:.4f}')\n",
    "            else:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss: {t_loss:.4f}, Training Accuracy: {t_acc:.4f}, Training Sensitivity: {t_sen:.4f}, Training Specificity: {t_spec:.4f}, Validation Loss: {v_loss:.4f}, Validation Accuracy: {v_acc:.4f}, Validation Sensitivity: {v_sen:.4f}, Validation Specificity: {v_spec:.4f}')\n",
    "\n",
    "        if early_stopping_patience != None:\n",
    "            if v_loss < best_val_loss:\n",
    "                best_val_loss = v_loss\n",
    "                best_epoch = epoch\n",
    "                no_improvement_count = 0\n",
    "                torch.save(model.state_dict(), 'best_model_sqrt1.pth')\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            if no_improvement_count >= early_stopping_patience:\n",
    "                model.load_state_dict(torch.load('best_model_sqrt1.pth'))\n",
    "                if gpu_bool:\n",
    "                    model = model.to('cuda:1')\n",
    "                break\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test)\n",
    "    else:\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test, edge_index_test, weights_test)\n",
    "    \n",
    "    if early_stopping_patience == None:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "    else:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "\n",
    "    x = range(1, len(train_loss)+1)\n",
    "    plt.plot(x, train_loss, color = '#1f77b4', label = 'Training Loss', alpha = 0.75)\n",
    "    plt.plot(x, val_loss, color = '#ff7f0e', label = 'Validation Loss', alpha = 0.75)\n",
    "    if criterion_type in ['ce','bce','bcelogits','multimargin']:\n",
    "        plt.plot(x, train_acc, color = '#2ca02c', label = 'Training Accuracy', alpha = 0.75)\n",
    "        plt.plot(x, val_acc, color = '#d62728', label = 'Validation Accuracy', alpha = 0.75)\n",
    "        plt.plot(x, train_sen, color = '#9467bd', label = 'Training Sensitivity', alpha = 0.75)\n",
    "        plt.plot(x, val_sen, color = '#8c564b', label = 'Validation Sensitivity', alpha = 0.75)\n",
    "        plt.plot(x, train_spec, color = '#e377c2', label = 'Training Specificity', alpha = 0.75)\n",
    "        plt.plot(x, val_spec, color = '#7f7f7f', label = 'Validation Specificity', alpha = 0.75)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"Training Results: \"+title)\n",
    "    plt.legend()\n",
    "    filenames = os.listdir(dir)\n",
    "    if '0.png' not in filenames:\n",
    "        plt.savefig(dir+'/0.png')\n",
    "    else:\n",
    "        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "    plt.show()\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), 'trained_model_'+str(model.in_channels)+'_'+str(model.first_hidden_channels)+'_'+str(model.out_channels)+'_'+str(model.n_hidden_layers)+'.pth')\n",
    "    \n",
    "    return model\n",
    "    #evaluate(title,model,criterion_type,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_out1(samples,model,criterion_type,optimizer_type,scheduler_type,num_epochs=100,early_stopping_patience=None,save_model = False):\n",
    "\n",
    "    title = 'out_channels = 1'\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "\n",
    "    x_train = samples[0][0]\n",
    "    x_val = samples[1][0]\n",
    "    x_test = samples[2][0]\n",
    "    y_train = samples[0][1][0]\n",
    "    y_val = samples[1][1][0]\n",
    "    y_test = samples[2][1][0]\n",
    "    edge_index_train = samples[0][2]\n",
    "    edge_index_val = samples[1][2]\n",
    "    edge_index_test = samples[2][2]\n",
    "    weights_train = samples[0][3]\n",
    "    weights_val = samples[1][3]\n",
    "    weights_test = samples[2][3]\n",
    "\n",
    "    in_channels = 1\n",
    "    out_channels = 1\n",
    "    if isinstance(model, str):\n",
    "        model_type = model\n",
    "        model,criterion,optimizer,scheduler = build(in_channels, out_channels, model_type,criterion_type,optimizer_type,scheduler_type)\n",
    "    else:\n",
    "        model_type = model.name\n",
    "        _,criterion,optimizer,scheduler = build(in_channels, out_channels, model, criterion_type,optimizer_type,scheduler_type)\n",
    "    print(model)\n",
    "\n",
    "    if gpu_bool:\n",
    "        model = model.to('cuda:1')\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_sen = []\n",
    "    train_spec = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_sen = []\n",
    "    val_spec = []\n",
    "\n",
    "    if early_stopping_patience != None:\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if model_type == 'mlp':\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,scheduler_type,x_train,x_val,y_train,y_val)\n",
    "        else:\n",
    "            t_loss,t_acc,t_sen,t_spec, v_loss, v_acc, v_sen, v_spec = train(gpu_bool,model,criterion,optimizer,scheduler,criterion_type,optimizer_type,x_train,x_val,y_train,y_val,edge_index_train,edge_index_val,weights_train,weights_val)\n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(t_acc)\n",
    "        train_sen.append(t_sen)\n",
    "        train_spec.append(t_spec)\n",
    "        val_loss.append(v_loss)\n",
    "        val_acc.append(v_acc)\n",
    "        val_sen.append(v_sen)\n",
    "        val_spec.append(v_spec)\n",
    "        if epoch % 10 == 0:\n",
    "            if criterion_type in ['mse','l2','mse-mse']:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MSE): {t_loss:.4f}, Validation Loss (MSE): {v_loss:.4f}')\n",
    "            elif criterion_type == 'l1':\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss (MAE): {t_loss:.4f}, Validation Loss (MAE): {v_loss:.4f}')\n",
    "            else:\n",
    "                print(f'Epoch: {epoch:03d}, Training Loss: {t_loss:.4f}, Training Accuracy: {t_acc:.4f}, Training Sensitivity: {t_sen:.4f}, Training Specificity: {t_spec:.4f}, Validation Loss: {v_loss:.4f}, Validation Accuracy: {v_acc:.4f}, Validation Sensitivity: {v_sen:.4f}, Validation Specificity: {v_spec:.4f}')\n",
    "\n",
    "        if early_stopping_patience != None:\n",
    "            if v_loss < best_val_loss:\n",
    "                best_val_loss = v_loss\n",
    "                best_epoch = epoch\n",
    "                no_improvement_count = 0\n",
    "                torch.save(model.state_dict(), 'best_model_sqrt1.pth')\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            if no_improvement_count >= early_stopping_patience:\n",
    "                model.load_state_dict(torch.load('best_model_sqrt1.pth'))\n",
    "                if gpu_bool:\n",
    "                    model = model.to('cuda:1')\n",
    "                break\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test)\n",
    "    else:\n",
    "        test_loss, test_acc, test_sen, test_spec = test(gpu_bool,model,criterion,criterion_type,x_test, y_test, edge_index_test, weights_test)\n",
    "    \n",
    "    if early_stopping_patience == None:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "    else:\n",
    "        if criterion_type in ['mse','l2','mse-mse']:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MSE): {test_loss:03f}')\n",
    "        elif criterion_type == 'l1':\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss (MAE): {test_loss:03f}')\n",
    "        else:\n",
    "            print(f'Best Epoch: {best_epoch:03d}, Test Loss: {test_loss:03f}, Test Accuracy: {test_acc:03f}, Test Sensitivity: {test_sen:03f}, Test Specificity: {test_spec:03f}')\n",
    "\n",
    "    x = range(1, len(train_loss)+1)\n",
    "    plt.plot(x, train_loss, color = '#1f77b4', label = 'Training Loss', alpha = 0.75)\n",
    "    plt.plot(x, val_loss, color = '#ff7f0e', label = 'Validation Loss', alpha = 0.75)\n",
    "    if criterion_type in ['ce','bce','bcelogits','multimargin']:\n",
    "        plt.plot(x, train_acc, color = '#2ca02c', label = 'Training Accuracy', alpha = 0.75)\n",
    "        plt.plot(x, val_acc, color = '#d62728', label = 'Validation Accuracy', alpha = 0.75)\n",
    "        plt.plot(x, train_sen, color = '#9467bd', label = 'Training Sensitivity', alpha = 0.75)\n",
    "        plt.plot(x, val_sen, color = '#8c564b', label = 'Validation Sensitivity', alpha = 0.75)\n",
    "        plt.plot(x, train_spec, color = '#e377c2', label = 'Training Specificity', alpha = 0.75)\n",
    "        plt.plot(x, val_spec, color = '#7f7f7f', label = 'Validation Specificity', alpha = 0.75)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"Training Results: \"+title)\n",
    "    plt.legend()\n",
    "    filenames = os.listdir(dir)\n",
    "    if '0.png' not in filenames:\n",
    "        plt.savefig(dir+'/0.png')\n",
    "    else:\n",
    "        filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "        plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "    plt.show()\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), 'trained_model_'+str(model.in_channels)+'_'+str(model.first_hidden_channels)+'_'+str(model.out_channels)+'_'+str(model.n_hidden_layers)+'.pth')\n",
    "    \n",
    "    return model\n",
    "    #evaluate(title,model,criterion_type,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_ER_all_distances(n,lbd,print_summary = False):\n",
    "\n",
    "    samples_x = []\n",
    "    samples_y_actual = []\n",
    "    samples_y_Bourgain = []\n",
    "    samples_y_Sarma1 = []\n",
    "    samples_y_Sarma2 = []\n",
    "    samples_y_Sarma3 = []\n",
    "    samples_edge_index = []\n",
    "\n",
    "    n_rejected1 = 0\n",
    "    n_rejected2 = 0\n",
    "    while True:\n",
    "        try:\n",
    "            object,directed,weighted = ErdosRenyiGraph(n,lbd/n)\n",
    "            components = list(nx.strongly_connected_components(object))\n",
    "            largest_component = max(components, key=len)\n",
    "            n_nodes = len(largest_component)\n",
    "            if n_nodes >= 10:\n",
    "                object = object.subgraph(largest_component)\n",
    "                object = nx.relabel_nodes(object, {node: index for index, node in enumerate(object.nodes())})\n",
    "                matrix = graph_to_matrix(object)\n",
    "                y_actual_all = np.zeros((n_nodes,n_nodes))\n",
    "                y_Bourgain_all = np.zeros((n_nodes,n_nodes))\n",
    "                y_Sarma1_all = np.zeros((n_nodes,n_nodes))\n",
    "                y_Sarma2_all = np.zeros((n_nodes,n_nodes))\n",
    "                y_Sarma3_all = np.zeros((n_nodes,n_nodes))\n",
    "                M = n_nodes\n",
    "                for i in range(n_nodes):\n",
    "                    y_actual_all[:,i] = shortestDistance_allNodes_networkx(object,i)\n",
    "                    if n <= 200:\n",
    "                        y_Bourgain_all[:,i] = shortestDistance_allNodes_Bourgain(matrix,i)\n",
    "                        y_Sarma1_all[:,i] = shortestDistance_allNodes_Sarma(matrix,i,1)\n",
    "                        y_Sarma2_all[:,i] = shortestDistance_allNodes_Sarma(matrix,i,2)\n",
    "                        y_Sarma3_all[:,i] = shortestDistance_allNodes_Sarma(matrix,i,3)\n",
    "                samples_x.append(torch.tensor(np.eye(n_nodes).astype(np.float32), requires_grad=True))\n",
    "                samples_y_actual.append(torch.tensor(y_actual_all).to(torch.float32))\n",
    "                samples_edge_index.append(torch.tensor(np.array(list(object.edges())).T).to(torch.int64))\n",
    "                if n <= 200:\n",
    "                    y_Bourgain_all = np.where(y_Bourgain_all == float('inf'), M, y_Bourgain_all)\n",
    "                    y_Sarma1_all = np.where(y_Sarma1_all == float('inf'), M, y_Sarma1_all)\n",
    "                    y_Sarma2_all = np.where(y_Sarma2_all == float('inf'), M, y_Sarma2_all)\n",
    "                    y_Sarma3_all = np.where(y_Sarma3_all == float('inf'), M, y_Sarma3_all)\n",
    "                    samples_y_Bourgain.append(y_Bourgain_all)\n",
    "                    samples_y_Sarma1.append(y_Sarma1_all)\n",
    "                    samples_y_Sarma2.append(y_Sarma2_all)\n",
    "                    samples_y_Sarma3.append(y_Sarma3_all)\n",
    "                break\n",
    "            else:\n",
    "                n_rejected2 += 1\n",
    "        except:\n",
    "            n_rejected1 += 1\n",
    "    \n",
    "    if print_summary:\n",
    "        print('Number of graphs rejected because Bourgain\\'s and Sarma\\'s algorithms yield errors: ',n_rejected1)\n",
    "        print('Number of graphs rejected because the largest component has insufficient size: ',n_rejected2)\n",
    "\n",
    "    return samples_x, [samples_y_actual, samples_y_Bourgain, samples_y_Sarma1, samples_y_Sarma2, samples_y_Sarma3], samples_edge_index, None\n",
    "\n",
    "def evaluate_one_graph_all_distances(n,model,criterion_type,samples,display_results = False):\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "    if model.out_channels == 1:\n",
    "        y_pred = predict(gpu_bool, model, criterion_type, samples[0], samples[2], samples[3])\n",
    "        samples_y_actual = samples[1][0]\n",
    "        samples_y_Bourgain = samples[1][1]\n",
    "        samples_y_Sarma1 = samples[1][2]\n",
    "        samples_y_Sarma2 = samples[1][3]\n",
    "        samples_y_Sarma3 = samples[1][4]\n",
    "        title = 'GNN, out_channels = 1'\n",
    "    else:\n",
    "        samples_x = []\n",
    "        samples_y_actual = []\n",
    "        samples_y_Bourgain = []\n",
    "        samples_y_Sarma1 = []\n",
    "        samples_y_Sarma2 = []\n",
    "        samples_y_Sarma3 = []\n",
    "        samples_edge_index = []\n",
    "        y_actual_all = samples[1][0][0]\n",
    "        y_Bourgain_all = samples[1][1][0]\n",
    "        y_Sarma1_all = samples[1][2][0]\n",
    "        y_Sarma2_all = samples[1][3][0]\n",
    "        y_Sarma3_all = samples[1][4][0]\n",
    "        edge_index = samples[2][0]\n",
    "        n_nodes = y_actual_all.shape[0]\n",
    "        r = model.out_channels\n",
    "        if n_nodes % r != 0:\n",
    "            n_extra = int(np.ceil(n_nodes/r))*r - n_nodes\n",
    "            extra_seeds = np.random.choice(range(n_nodes),size=n_extra,replace=True)\n",
    "            shuffled_nodes = np.random.permutation(list(range(n_nodes))+list(extra_seeds))\n",
    "        else:\n",
    "            shuffled_nodes = np.random.permutation(range(n_nodes))\n",
    "        list_of_seeds = [shuffled_nodes[i:i + r] for i in range(0, len(shuffled_nodes), r)]\n",
    "        for seeds in list_of_seeds:\n",
    "            x = np.zeros((n_nodes,r))\n",
    "            y_actual = np.zeros((n_nodes,r))\n",
    "            y_Bourgain = np.zeros((n_nodes,r))\n",
    "            y_Sarma1 = np.zeros((n_nodes,r))\n",
    "            y_Sarma2 = np.zeros((n_nodes,r))\n",
    "            y_Sarma3 = np.zeros((n_nodes,r))\n",
    "            for i in range(r):\n",
    "                u = seeds[i]\n",
    "                x[u,i] = 1\n",
    "                y_actual[:,i] = y_actual_all[:,u]\n",
    "                y_Bourgain[:,i] = y_Bourgain_all[:,u]\n",
    "                y_Sarma1[:,i] = y_Sarma1_all[:,u]\n",
    "                y_Sarma2[:,i] = y_Sarma2_all[:,u]\n",
    "                y_Sarma3[:,i] = y_Sarma3_all[:,u]\n",
    "            samples_x.append(torch.tensor(x.astype(np.float32), requires_grad=True))\n",
    "            samples_y_actual.append(torch.tensor(y_actual).to(torch.float32))\n",
    "            samples_y_Bourgain.append(y_Bourgain)\n",
    "            samples_y_Sarma1.append(y_Sarma1)\n",
    "            samples_y_Sarma2.append(y_Sarma2)\n",
    "            samples_y_Sarma3.append(y_Sarma3)\n",
    "            samples_edge_index.append(edge_index)\n",
    "        if samples[3] == None:\n",
    "            samples_weights = None\n",
    "        else:\n",
    "            samples_weights = [samples[3][0] for i in range(len(list_of_seeds))]\n",
    "        y_pred = predict(gpu_bool, model, criterion_type, samples_x, samples_edge_index, samples_weights)\n",
    "        title = 'GNN, out_channels = floor(sqrt(n))'\n",
    "   \n",
    "    y_pred = np.array(list(chain(*[np.ravel(arr) for arr in y_pred])))\n",
    "    y_actual = np.array(list(chain(*[np.ravel(arr) for arr in samples_y_actual])))\n",
    "    if n <= 200:\n",
    "        y_Bourgain = np.array(list(chain(*[np.ravel(arr) for arr in samples_y_Bourgain])))\n",
    "        y_Sarma1 = np.array(list(chain(*[np.ravel(arr) for arr in samples_y_Sarma1])))\n",
    "        y_Sarma2 = np.array(list(chain(*[np.ravel(arr) for arr in samples_y_Sarma2])))\n",
    "        y_Sarma3 = np.array(list(chain(*[np.ravel(arr) for arr in samples_y_Sarma3])))\n",
    "\n",
    "    mseGNN = mean_squared_error(y_actual, y_pred)\n",
    "    mseBourgain = mean_squared_error(y_actual, y_Bourgain)\n",
    "    mseSarma1 = mean_squared_error(y_actual, y_Sarma1)\n",
    "    mseSarma2 = mean_squared_error(y_actual, y_Sarma2)\n",
    "    mseSarma3 = mean_squared_error(y_actual, y_Sarma3)\n",
    "    \n",
    "    if display_results:\n",
    "\n",
    "        diff_pred = y_actual - y_pred\n",
    "        values, base = np.histogram(diff_pred, bins=100)\n",
    "        cumulative = np.cumsum(values)\n",
    "        plt.plot(base[:-1], cumulative, label=title, alpha = 0.75)\n",
    "\n",
    "        if n <= 200:\n",
    "\n",
    "            diff_Bourgain = y_actual-y_Bourgain\n",
    "            values, base = np.histogram(diff_Bourgain, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Bourgain', alpha = 0.75)\n",
    "\n",
    "            diff_Sarma1 = y_actual-y_Sarma1\n",
    "            values, base = np.histogram(diff_Sarma1, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Sarma, k = 1', alpha = 0.75)\n",
    "\n",
    "            diff_Sarma2 = y_actual-y_Sarma2\n",
    "            values, base = np.histogram(diff_Sarma2, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Sarma, k = 2', alpha = 0.75)\n",
    "\n",
    "            diff_Sarma3 = y_actual-y_Sarma3\n",
    "            values, base = np.histogram(diff_Sarma3, bins=100)\n",
    "            cumulative = np.cumsum(values)\n",
    "            plt.plot(base[:-1], cumulative, label='Sarma, k = 3', alpha = 0.75)\n",
    "            \n",
    "        plt.xlabel(\"Actual Distance - Predicted Distance\")\n",
    "        plt.ylabel(\"Cummulative Frequency\")\n",
    "        plt.title(\"All Distances: n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "        plt.legend()\n",
    "        filenames = os.listdir(dir)\n",
    "        if '0.png' not in filenames:\n",
    "            plt.savefig(dir+'/0.png')\n",
    "        else:\n",
    "            filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "            plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(y_actual, y_pred, label=title, alpha = 0.1)\n",
    "        plt.xlabel(\"Actual Distance\")\n",
    "        plt.ylabel('Predicted Distance')\n",
    "        plt.title(\"All Distances: n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "        filenames = os.listdir(dir)\n",
    "        if '0.png' not in filenames:\n",
    "            plt.savefig(dir+'/0.png')\n",
    "        else:\n",
    "            filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "            plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "        plt.show()\n",
    "\n",
    "        plt.hist(y_actual, edgecolor='white', alpha=0.4)\n",
    "        #sns.kdeplot(y_actual, fill=True, alpha=0.2)\n",
    "        plt.xlabel(\"Actual Distance\")\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(\"All Distances: n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "        filenames = os.listdir(dir)\n",
    "        if '0.png' not in filenames:\n",
    "            plt.savefig(dir+'/0.png')\n",
    "        else:\n",
    "            filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "            plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "        plt.show()\n",
    "\n",
    "        #plt.hist(diff_pred, edgecolor='white', alpha=0.4)\n",
    "        sns.kdeplot(diff_pred, fill=True, alpha=0.2)\n",
    "        plt.xlabel(\"Actual Distance - Predicted Distance\")\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(\"All Distances: n = \"+str(n)+\", lambda = \"+str(lbd))\n",
    "        filenames = os.listdir(dir)\n",
    "        if '0.png' not in filenames:\n",
    "            plt.savefig(dir+'/0.png')\n",
    "        else:\n",
    "            filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "            plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')   \n",
    "        plt.show()\n",
    "\n",
    "        print(title+', MSE:',mseGNN)\n",
    "        print('Bourgain, MSE:',mseBourgain)\n",
    "        print('Sarma, k = 1, MSE:',mseSarma1)\n",
    "        print('Sarma, k = 2, MSE:',mseSarma2)\n",
    "        print('Sarma, k = 3, MSE:',mseSarma3)\n",
    "\n",
    "    return [mseGNN,mseBourgain,mseSarma1,mseSarma2,mseSarma3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'outputs/lbd 1 new dump copy'\n",
    "lbd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse1 = []\n",
    "mse2 = []\n",
    "graph_sizes = 2**np.array(range(12))*10\n",
    "for n in graph_sizes:\n",
    "    print(n)\n",
    "    samples = generateERSamples(200,50,50,n,lbd)\n",
    "    model1 = run(samples,'gcn','mse','adam','cyclic-cosine',100,20)\n",
    "    model2 = run_out1(samples,'gcn','mse','adam','cyclic-cosine',100,20)\n",
    "    evaluate(n,model1,model2,'mse',samples)\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    one_sample = generate_one_ER_all_distances(n,lbd,True)\n",
    "    list1.append(evaluate_one_graph_all_distances(n,model1,'mse',one_sample,True))\n",
    "    list2.append(evaluate_one_graph_all_distances(n,model2,'mse',one_sample,True))\n",
    "    for k in range(100):\n",
    "        one_sample = generate_one_ER_all_distances(n,lbd)\n",
    "        list1.append(evaluate_one_graph_all_distances(n,model1,'mse',one_sample))\n",
    "        list2.append(evaluate_one_graph_all_distances(n,model2,'mse',one_sample))\n",
    "    mse1.append(np.mean(np.array(list1), axis=0))\n",
    "    mse2.append(np.mean(np.array(list2), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse1 = np.array(mse1)\n",
    "plt.plot(range(12),mse1[:,0], label = 'GNN, out_channels = floor(sqrt(n))')\n",
    "plt.plot(range(12),mse1[:,1], label = 'Bourgain')\n",
    "plt.plot(range(12),mse1[:,2], label = 'Sarma, k = 1')\n",
    "plt.plot(range(12),mse1[:,3], label = 'Sarma, k = 2')\n",
    "plt.plot(range(12),mse1[:,4], label = 'Sarma, k = 3')\n",
    "custom_ticks = range(12)\n",
    "custom_labels = graph_sizes\n",
    "plt.xticks(custom_ticks, custom_labels,rotation=45)\n",
    "plt.xlabel(\"Graph Size\")\n",
    "plt.ylabel('MSE')\n",
    "plt.title(\"Mean All-Distance MSE: lambda = \"+str(lbd))\n",
    "plt.legend()\n",
    "filenames = os.listdir(dir)\n",
    "if '0.png' not in filenames:\n",
    "    plt.savefig(dir+'/0.png')\n",
    "else:\n",
    "    filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "    plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')\n",
    "plt.show()\n",
    "\n",
    "mse2 = np.array(mse2)\n",
    "plt.plot(range(12),mse1[:,0], label = 'GNN, out_channels = 1')\n",
    "plt.plot(range(12),mse1[:,1], label = 'Bourgain')\n",
    "plt.plot(range(12),mse1[:,2], label = 'Sarma, k = 1')\n",
    "plt.plot(range(12),mse1[:,3], label = 'Sarma, k = 2')\n",
    "plt.plot(range(12),mse1[:,4], label = 'Sarma, k = 3')\n",
    "custom_ticks = range(12)\n",
    "custom_labels = graph_sizes\n",
    "plt.xticks(custom_ticks, custom_labels,rotation=45)\n",
    "plt.xlabel(\"Graph Size\")\n",
    "plt.ylabel('MSE')\n",
    "plt.title(\"Mean All-Distance MSE: lambda = \"+str(lbd))\n",
    "plt.legend()\n",
    "filenames = os.listdir(dir)\n",
    "if '0.png' not in filenames:\n",
    "    plt.savefig(dir+'/0.png')\n",
    "else:\n",
    "    filenames = sorted([int(f[:-4]) for f in filenames])\n",
    "    plt.savefig(dir+'/'+str(filenames[-1]+1)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allShortestDistances_directed(i,j,d_ki,d_ij,d_jk): ## assume i,j in all shortest paths\n",
    "    n = len(d_ki)\n",
    "    distances =  np.zeros((n,n))\n",
    "    for u in range(n):\n",
    "        for v in range(n):\n",
    "            if u != v:\n",
    "                distances[u,v] = d_ki[u]+d_ij+d_jk[v]\n",
    "    distances[i,j] = min(distances[i,j],d_ij)\n",
    "    return distances\n",
    "\n",
    "def allShortestDistances_undirected(i,j,d_ki,d_jk):\n",
    "    if i == j:\n",
    "        d_ij = 0\n",
    "    else:\n",
    "        d_ij = min(d_ki[j],d_jk[i])\n",
    "    n = len(d_ki)\n",
    "    distances =  np.zeros((n,n))\n",
    "    for u in range(n):\n",
    "        for v in range(u+1,n):\n",
    "            distances[u,v] = min(d_ki[u]+d_ki[v],d_jk[u]+d_jk[v],d_ki[u]+d_ij+d_jk[v],d_ki[v]+d_ij+d_jk[u])\n",
    "            distances[v,u] = distances[u,v]\n",
    "    distances[i,j] = min(distances[i,j],d_ij)\n",
    "    distances[j,i] = distances[i,j]\n",
    "    return distances\n",
    "\n",
    "def ShortestPath(G,distances,u,v,epsilon):\n",
    "    S = [u,v]\n",
    "    for k in range(distances.shape[0]):\n",
    "        if np.abs(distances[u,k]+distances[k,v]-distances[u,v]) <= epsilon:\n",
    "            S.append(k)\n",
    "    S.sort()\n",
    "    if not isinstance(G, np.ndarray):\n",
    "        subgraph = G.subgraph(S)\n",
    "        if nx.has_path(subgraph, u, v):\n",
    "            path = nx.shortest_path(subgraph, u, v, weight=\"weight\")\n",
    "            distance = nx.shortest_path_length(subgraph, u, v, weight=\"weight\")\n",
    "            return path, distance\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        subgraph = G[S]\n",
    "        subgraph = subgraph[:,S]\n",
    "        subgraph = matrix_to_graph(subgraph)\n",
    "        u_masked = S.index(u)\n",
    "        v_masked = S.index(v)\n",
    "        if nx.has_path(subgraph, u_masked, v_masked):\n",
    "            path = nx.shortest_path(subgraph, u_masked, v_masked, weight=\"weight\")\n",
    "            distance = nx.shortest_path_length(subgraph, u_masked, v_masked, weight=\"weight\")\n",
    "            return [S[i] for i in path], distance\n",
    "        else:\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sarma claimed that there is at least one common node in SKETCH[u] and SKETCH[v] for undirected connected graphs. This is FALSE!!\n",
    "import numpy as np\n",
    "def checkSarmaAssumption(n,k):\n",
    "    r = int(np.floor(np.log(n))) ## which base? use e instead of 10 this time since it returns a higher r, lowering the possibility of having no common nodes.\n",
    "    seed_sizes = 2**np.array(list(range(r+1))*k)\n",
    "    max_nodes_to_considered = (np.sum(seed_sizes)+1)*2\n",
    "    boolean = n>max_nodes_to_considered\n",
    "    return boolean\n",
    "k1 = []\n",
    "k2 = []\n",
    "k3 = []\n",
    "for n in range(1,100000):\n",
    "    k1.append(checkSarmaAssumption(n,1))\n",
    "    k2.append(checkSarmaAssumption(n,2))\n",
    "    k3.append(checkSarmaAssumption(n,3))\n",
    "print(np.sum(k1))\n",
    "print(np.sum(k2))\n",
    "print(np.sum(k3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offlineSample_GNN(model,criterion_type,G,u,weighted = False):\n",
    "\n",
    "    n_nodes = len(G.nodes())\n",
    "    if n_nodes <= 1:\n",
    "        return None, set()\n",
    "    r = np.floor(np.log(n_nodes))\n",
    "    sample_sets = [np.random.choice(G.nodes(),size=2**i,replace=False) for i in range(r+1)]\n",
    "\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "    out = model.out_channels\n",
    "    x = np.zeros((n_nodes,out))\n",
    "    x[u,0] = 1\n",
    "    if out > 1:\n",
    "        extra_seeds = np.random.choice(G.nodes(),size=out-1,replace=True)\n",
    "        for i in range(out-1):\n",
    "            x[extra_seeds[i],i+1] = 1\n",
    "    samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "    samples_edge_index = [torch.tensor(np.array(list(G.edges())).T).to(torch.int64)]\n",
    "    if weighted:\n",
    "        samples_weights = [torch.tensor(list(nx.get_edge_attributes(G,'weight').values())).to(torch.float32)]\n",
    "    else:\n",
    "        samples_weights = None\n",
    "    y_pred = predict(gpu_bool, model, criterion_type, samples_x, samples_edge_index, samples_weights)[0][:,0]\n",
    "    distances = [[y_pred[i] for i in S] for S in sample_sets]\n",
    "    index = [d.index(min(d)) for d in distances]\n",
    "    closest_points = [(sample_sets[i][index[i]],distances[i][index[i]]) for i in range(r+1)]\n",
    "    return closest_points,set(np.concatenate(sample_sets))\n",
    "\n",
    "def offlineSketch_GNN(model,criterion_type,G,u,k,weighted=False):\n",
    "    closest_points,sample_sets = offlineSample_GNN(model,criterion_type,G,u,weighted)\n",
    "    for i in range(k):\n",
    "        closest_points_new,sample_sets_new = offlineSample_GNN(model,criterion_type,G,u,weighted)\n",
    "        closest_points = closest_points.union(closest_points_new)\n",
    "        sample_sets = sample_sets.union(sample_sets_new)\n",
    "    return np.array(list(closest_points)),np.array(list(sample_sets))\n",
    "\n",
    "def onlineShortestPath_Sarma_GNN(model1,model2,criterion_type,G,u,v,k,weighted=False): ## upper bound\n",
    "    ## if undirected, model1 = model2\n",
    "    ## else, model1 calculates distances from u to each k and model2 calculates distances from eack k to v\n",
    "    sketch_u,_ = offlineSketch_GNN(model1,criterion_type,G,u,k,weighted)\n",
    "    sketch_v,_ = offlineSketch_GNN(model2,criterion_type,G,v,k,weighted)\n",
    "    if sketch_u.shape[0] != 0 and sketch_v.shape[0] != 0:\n",
    "        common_nodes = [w for w in sketch_u[:,0] if w in sketch_v[:,0]]\n",
    "        while None in common_nodes:\n",
    "            common_nodes.remove(None)\n",
    "        min_dist = float('inf')\n",
    "        for w in common_nodes:\n",
    "            dist = sketch_u[sketch_u[:, 0] == w][0,1] + sketch_v[sketch_v[:, 0] == w][0,1]\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        return min_dist\n",
    "    else:\n",
    "        return float('inf')\n",
    "\n",
    "def onlineShortestPath_Bourgain_GNN(model1,model2,criterion_type,G,u,v,weighted=False): ## lower bound\n",
    "\n",
    "    n_nodes = len(G.nodes())\n",
    "    if n_nodes <= 1:\n",
    "        return None, set()\n",
    "    r = np.floor(np.log(n_nodes))\n",
    "    sample_sets = [np.random.choice(G.nodes(),size=2**i,replace=False) for i in range(r+1)]\n",
    "\n",
    "    gpu_bool = torch.cuda.is_available()\n",
    "    samples_edge_index = [torch.tensor(np.array(list(G.edges())).T).to(torch.int64)]\n",
    "    if weighted:\n",
    "        samples_weights = [torch.tensor(list(nx.get_edge_attributes(G,'weight').values())).to(torch.float32)]\n",
    "    else:\n",
    "        samples_weights = None\n",
    "    if model1 == model2: # set model1 = model2 if undirected\n",
    "        out = model1.out_channels\n",
    "        x = np.zeros((n_nodes,out))\n",
    "        x[u,0] = 1\n",
    "        if out > 1:\n",
    "            x[v,1] = 1\n",
    "            extra_seeds = np.random.choice(G.nodes(),size=out-2,replace=True)\n",
    "            for i in range(out-2):\n",
    "                x[extra_seeds[i],i+2] = 1\n",
    "            samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "            y_pred = predict(gpu_bool, model1, criterion_type, samples_x, samples_edge_index, samples_weights)[0]\n",
    "            y_pred_u = y_pred[:,0]\n",
    "            y_pred_v = y_pred[:,1]\n",
    "        if out == 1:\n",
    "            samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "            y_pred_u = predict(gpu_bool, model1, criterion_type, samples_x, samples_edge_index, samples_weights)[0][:,0]\n",
    "            x = np.zeros((n_nodes,out))\n",
    "            x[v,0] = 1\n",
    "            samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "            y_pred_v = predict(gpu_bool, model1, criterion_type, samples_x, samples_edge_index, samples_weights)[0][:,0]\n",
    "        d_u_S = [[y_pred_u[i] for i in S] for S in sample_sets]\n",
    "        d_v_S = [[y_pred_v[i] for i in S] for S in sample_sets]\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_u_S,d_v_S))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_u_S = np.array([value for index, value in enumerate(d_u_S) if index not in to_remove])\n",
    "        d_v_S = np.array([value for index, value in enumerate(d_v_S) if index not in to_remove])\n",
    "        return np.max(np.abs(d_u_S-d_v_S))\n",
    "    else:\n",
    "        out = model1.out_channels\n",
    "        x = np.zeros((n_nodes,out))\n",
    "        x[u,0] = 1\n",
    "        if out > 1:\n",
    "            extra_seeds = np.random.choice(G.nodes(),size=out-1,replace=True)\n",
    "            for i in range(out-1):\n",
    "                x[extra_seeds[i],i+1] = 1\n",
    "        samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "        y_pred_u_k = predict(gpu_bool, model1, criterion_type, samples_x, samples_edge_index, samples_weights)[0][:,0]\n",
    "        x = np.zeros((n_nodes,out))\n",
    "        x[v,0] = 1\n",
    "        if out > 1:\n",
    "            extra_seeds = np.random.choice(G.nodes(),size=out-1,replace=True)\n",
    "            for i in range(out-1):\n",
    "                x[extra_seeds[i],i+1] = 1\n",
    "        samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "        y_pred_v_k = predict(gpu_bool, model1, criterion_type, samples_x, samples_edge_index, samples_weights)[0][:,0]\n",
    "        out = model2.out_channels\n",
    "        x = np.zeros((n_nodes,out))\n",
    "        x[u,0] = 1\n",
    "        if out > 1:\n",
    "            extra_seeds = np.random.choice(G.nodes(),size=out-1,replace=True)\n",
    "            for i in range(out-1):\n",
    "                x[extra_seeds[i],i+1] = 1\n",
    "        samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "        y_pred_k_u = predict(gpu_bool, model2, criterion_type, samples_x, samples_edge_index, samples_weights)[0][:,0]\n",
    "        x = np.zeros((n_nodes,out))\n",
    "        x[v,0] = 1\n",
    "        if out > 1:\n",
    "            extra_seeds = np.random.choice(G.nodes(),size=out-1,replace=True)\n",
    "            for i in range(out-1):\n",
    "                x[extra_seeds[i],i+1] = 1\n",
    "        samples_x = [torch.tensor(x.astype(np.float32), requires_grad=True)]\n",
    "        y_pred_k_v = predict(gpu_bool, model2, criterion_type, samples_x, samples_edge_index, samples_weights)[0][:,0]\n",
    "        d_u_S = [[y_pred_u_k[i] for i in S] for S in sample_sets]\n",
    "        d_v_S = [[y_pred_v_k[i] for i in S] for S in sample_sets]\n",
    "        d_S_u = [[y_pred_k_u[i] for i in S] for S in sample_sets]\n",
    "        d_S_v = [[y_pred_k_v[i] for i in S] for S in sample_sets]\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_u_S,d_v_S))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_u_S = np.array([value for index, value in enumerate(d_u_S) if index not in to_remove])\n",
    "        d_v_S = np.array([value for index, value in enumerate(d_v_S) if index not in to_remove])\n",
    "        to_remove = [idx for idx,val in enumerate(list(zip(d_S_u,d_S_v))) if val[0] == float('inf') or val[1] == float('inf')]\n",
    "        d_S_u = np.array([value for index, value in enumerate(d_S_u) if index not in to_remove])\n",
    "        d_S_v = np.array([value for index, value in enumerate(d_S_v) if index not in to_remove])\n",
    "        return max([0,np.max(d_S_v-d_S_u),np.max(d_u_S-d_v_S)])\n",
    "\n",
    "def shortestDistance_allNodes_Sarma_GNN(model1,model2,criterion_type,G,u,k,weighted=False):\n",
    "    ## if undirected, model1 = model2\n",
    "    ## else, model1 calculates distances from u to each k and model2 calculates distances from eack k to v\n",
    "    distances = np.zeros(G.shape[0])\n",
    "    for v in range(G.shape[0]):\n",
    "        if u != v:\n",
    "            distances[v] = onlineShortestPath_Sarma_GNN(model1,model2,criterion_type,G,u,v,k,weighted)\n",
    "    return distances\n",
    "\n",
    "def shortestDistance_allNodes_Bourgain_GNN(model1,model2,criterion_type,G,u,weighted=False):\n",
    "    ## if undirected, model1 = model2\n",
    "    ## else, model1 calculates distances from u to each k and model2 calculates distances from eack k to v\n",
    "    distances = np.zeros(G.shape[0])\n",
    "    for v in range(G.shape[0]):\n",
    "        if u != v:\n",
    "            distances[v] = onlineShortestPath_Bourgain_GNN(model1,model2,criterion_type,G,u,v,weighted)\n",
    "    return distances\n",
    "\n",
    "def shortestDistance_allNodes_networkx(G,u):\n",
    "    if isinstance(G, np.ndarray):\n",
    "        G = matrix_to_graph(G)\n",
    "    n_nodes = len(G.nodes())\n",
    "    distances = np.zeros(n_nodes)\n",
    "    for v in range(n_nodes):\n",
    "        if u != v:\n",
    "            if nx.has_path(G, u, v):\n",
    "                distances[v] = nx.shortest_path_length(G, u, v, weight=\"weight\")\n",
    "            else:\n",
    "                distances[v] = float('inf')       \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
